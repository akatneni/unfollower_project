{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ec1640d-123d-4c76-ba7b-6b2cc1cd5d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import negative_sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "060860fa-6c79-4c94-9131-6455dd4ba228",
   "metadata": {},
   "outputs": [],
   "source": [
    "follower = {}\n",
    "friend = {}\n",
    "e_tweet = {}\n",
    "m_tweet = {}\n",
    "r_tweet = {}\n",
    "\n",
    "def open_files():\n",
    "    with open('./Unfollower/15weeks_follower_dict.pkl', 'rb') as f:\n",
    "        follower = pickle.load(f)\n",
    "\n",
    "    with open('./Unfollower/15weeks_friend_dict.pkl', 'rb') as f:\n",
    "        friend = pickle.load(f)\n",
    "\n",
    "    with open('./Unfollower/e_tweet_dict.pkl', 'rb') as f:\n",
    "        e_tweet = pickle.load(f)\n",
    "\n",
    "    with open('./Unfollower/m_tweet_dict.pkl', 'rb') as f:\n",
    "        m_tweet = pickle.load(f)\n",
    "\n",
    "    with open('./Unfollower/r_tweet_dict.pkl', 'rb') as f:\n",
    "        r_tweet = pickle.load(f)\n",
    "        \n",
    "    return (follower,friend,e_tweet,m_tweet,r_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "970b653a-7657-4c3c-afe6-f68c82e5b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = (0,9)\n",
    "test_range = (10,14)\n",
    "\n",
    "\n",
    "# make a networkx graph in order to find the clustering coefficient of the nodes\n",
    "def getTrainClustering(follower,friend,edge_attr,edge_to_in):\n",
    "    G = nx.Graph()\n",
    "    for key in follower:\n",
    "        if len(follower[key][train_range[1]]) == 2:\n",
    "            for f in follower[key][train_range[1]][1]:\n",
    "                G.add_edge(key,f)\n",
    "\n",
    "    for key in friend:\n",
    "        if len(friend[key][train_range[1]]) == 2:\n",
    "            for f in friend[key][train_range[1]][1]:\n",
    "                G.add_edge(f,key)\n",
    "              \n",
    "    print(\"Getting train common neighbors\")\n",
    "    #i = 0\n",
    "    for key in edge_to_in:\n",
    "        if edge_attr[edge_to_in[key]][0] == 0.0 and key[0] in G and key[1] in G:\n",
    "            neighbors = sum(1 for _ in nx.common_neighbors(G,key[0],key[1]))\n",
    "            edge_attr[edge_to_in[key]][0] = neighbors\n",
    "            # if i < 100:\n",
    "            #     print(f'{ edge_attr[edge_to_in[key]][0]} {neighbors} {edge_to_in[key]}')\n",
    "            # i += 1\n",
    "            if (key[1],key[0]) in edge_to_in:\n",
    "                edge_attr[edge_to_in[(key[1],key[0])]][0] = neighbors\n",
    "    print(\"Getting train clustering\")  \n",
    "    cluster_coeffs = nx.clustering(G)\n",
    "    return (cluster_coeffs,edge_attr)\n",
    "\n",
    "\n",
    "    # make another for the test set\n",
    "def getTestClustering(follower,friend,edge_attr,edge_to_in):\n",
    "    G2 = nx.Graph()\n",
    "    for key in follower:\n",
    "        if len(follower[key][test_range[1]]) == 2:\n",
    "            for f in follower[key][test_range[1]][1]:\n",
    "                G2.add_edge(key,f)\n",
    "\n",
    "    for key in friend:\n",
    "        if len(friend[key][test_range[1]]) == 2:\n",
    "            for f in friend[key][test_range[1]][1]:\n",
    "                G2.add_edge(f,key)\n",
    "    \n",
    "    print(\"Getting test common neighbors\")\n",
    "    for key in edge_to_in:\n",
    "        if edge_attr[edge_to_in[key]][0] == 0.0 and key[0] in G2 and key[1] in G2:\n",
    "            neighbors = sum(1 for _ in nx.common_neighbors(G2,key[0],key[1]))\n",
    "            edge_attr[edge_to_in[key]][0] = neighbors\n",
    "            if (key[1],key[0]) in edge_to_in:\n",
    "                edge_attr[edge_to_in[(key[1],key[0])]][0] = neighbors       \n",
    "    print(\"Getting test clustering\")     \n",
    "    cluster_coeffs = nx.clustering(G2)\n",
    "    return (cluster_coeffs,edge_attr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba110697-01a1-4921-9c8a-3d994ba9af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_edges(follower,friend,id_to_in):\n",
    "    nodes1 = []\n",
    "    nodes2 = []\n",
    "\n",
    "    # mapping of edge tuple to index in edge_index tensor\n",
    "    edge_to_in = {}\n",
    "\n",
    "    # Create edge lists for the train Data object\n",
    "    for key in follower:\n",
    "        if len(follower[key][train_range[1]]) == 2:\n",
    "            for f in follower[key][train_range[1]][1]:\n",
    "                edge_to_in[(key,f)] = len(nodes1)\n",
    "                nodes1.append(id_to_in[key])\n",
    "                nodes2.append(id_to_in[f])\n",
    "\n",
    "    for key in friend:\n",
    "        if len(friend[key][train_range[1]]) == 2:\n",
    "            for f in friend[key][train_range[1]][1]:\n",
    "                edge_to_in[(f,key)] = len(nodes1)\n",
    "                nodes1.append(id_to_in[f])\n",
    "                nodes2.append(id_to_in[key])\n",
    "                \n",
    "    edge_label = [0.0]*len(nodes1)\n",
    "    \n",
    "    # find edges that have been removed\n",
    "    for key in follower:\n",
    "        if len(follower[key][train_range[0]]) == 2:\n",
    "            for f in follower[key][train_range[0]][1]:\n",
    "                if (key,f) not in edge_to_in:\n",
    "                    edge_label.append(1.0)\n",
    "                    edge_to_in[(key,f)] = len(nodes1)\n",
    "                    nodes1.append(id_to_in[key])\n",
    "                    nodes2.append(id_to_in[f])\n",
    "                    \n",
    "    for key in friend:\n",
    "        if len(friend[key][train_range[0]]) == 2:\n",
    "            for f in friend[key][train_range[0]][1]:\n",
    "                if (f,key) not in edge_to_in:\n",
    "                    edge_label.append(1.0)\n",
    "                    edge_to_in[(f,key)] = len(nodes1)\n",
    "                    nodes1.append(id_to_in[f])\n",
    "                    nodes2.append(id_to_in[key])\n",
    "                    \n",
    "    return (nodes1,nodes2,edge_label,edge_to_in)\n",
    "\n",
    "def get_test_edges(follower,friend,id_to_in):\n",
    "    nodes3 = []\n",
    "    nodes4 = []\n",
    "    edge_to_in2 = {}\n",
    "\n",
    "\n",
    "    # Create edge lists for the test Data object\n",
    "    for key in follower:\n",
    "        if len(follower[key][test_range[1]]) == 2:\n",
    "            for f in follower[key][test_range[1]][1]:\n",
    "                edge_to_in2[(key,f)] = len(nodes3)\n",
    "                nodes3.append(id_to_in[key])\n",
    "                nodes4.append(id_to_in[f])\n",
    "\n",
    "    for key in friend:\n",
    "        if len(friend[key][test_range[1]]) == 2:\n",
    "            for f in friend[key][test_range[1]][1]:\n",
    "                edge_to_in2[(f,key)] = len(nodes3)\n",
    "                nodes3.append(id_to_in[f])\n",
    "                nodes4.append(id_to_in[key])\n",
    "                \n",
    "    edge_label = [0.0]*len(nodes3)\n",
    "    \n",
    "    # find edges that have been removed\n",
    "    for key in follower:\n",
    "        if len(follower[key][test_range[0]]) == 2:\n",
    "            for f in follower[key][test_range[0]][1]:\n",
    "                if (key,f) not in edge_to_in2:\n",
    "                    edge_label.append(1.0)\n",
    "                    edge_to_in2[(key,f)] = len(nodes3)\n",
    "                    nodes3.append(id_to_in[key])\n",
    "                    nodes4.append(id_to_in[f])\n",
    "                    \n",
    "    for key in friend:\n",
    "        if len(friend[key][test_range[0]]) == 2:\n",
    "            for f in friend[key][test_range[0]][1]:\n",
    "                if (f,key) not in edge_to_in2:\n",
    "                    edge_label.append(1.0)\n",
    "                    edge_to_in2[(f,key)] = len(nodes3)\n",
    "                    nodes3.append(id_to_in[f])\n",
    "                    nodes4.append(id_to_in[key])\n",
    "                \n",
    "    return (nodes3,nodes4,edge_label,edge_to_in2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4686b5b5-93fe-412b-9f50-8882d0fe857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# put clustering coefficients in feature array\n",
    "\n",
    "def add_clustering(x,n,in_to_id,cluster_coeffs):\n",
    "    for i in range(n):\n",
    "        key = str(in_to_id[i])\n",
    "        if key in cluster_coeffs:\n",
    "            x[i][0] = cluster_coeffs[key]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f1fa78d-0efa-42e2-958e-cdb3a90f5ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add number of tweets to feature list x\n",
    "\n",
    "def add_tweets_train(x,n,in_to_id,e_tweet):\n",
    "    for i in range(n):\n",
    "        key = str(in_to_id[i])\n",
    "        x[i][1] = 0\n",
    "        for j in range(train_range[1]):\n",
    "            if key in e_tweet['train'][j+1]:\n",
    "                x[i][1] += len(e_tweet['train'][j+1][key])\n",
    "                \n",
    "def add_tweets_test(x2,n,in_to_id,e_tweet):\n",
    "    for i in range(n):\n",
    "        key = str(in_to_id[i])\n",
    "        x2[i][1] = 0\n",
    "        for j in range(test_range[0],test_range[1]):\n",
    "            if key in e_tweet['test'][j+1]:\n",
    "                x2[i][1] += len(e_tweet['test'][j+1][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "737df115-3409-4b9b-b608-882cde0df175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_edge_tweets_train(edge_attr,n,edge_to_in,nodes1,nodes2,m_tweet,r_tweet):\n",
    "    for key in edge_to_in:\n",
    "        for j in range(train_range[1]):\n",
    "            if key[0] in m_tweet['train'][j+1] and key[1] in m_tweet['train'][j+1][key[0]]:\n",
    "                edge_attr[edge_to_in[key]][1] += len(m_tweet['train'][j+1][key[0]][key[1]])\n",
    "            if key[0] in r_tweet['train'][j+1] and key[1] in r_tweet['train'][j+1][key[0]]:\n",
    "                edge_attr[edge_to_in[key]][2] += len(r_tweet['train'][j+1][key[0]][key[1]])\n",
    "                \n",
    "def add_edge_tweets_test(edge_attr,n,edge_to_in,nodes1,nodes2,m_tweet,r_tweet):\n",
    "    for key in edge_to_in:\n",
    "        for j in range(test_range[0],test_range[1]):\n",
    "            if key[0] in m_tweet['test'][j+1] and key[1] in m_tweet['test'][j+1][key[0]]:\n",
    "                edge_attr[edge_to_in[key]][1] += len(m_tweet['test'][j+1][key[0]][key[1]])\n",
    "            if key[0] in r_tweet['test'][j+1] and key[1] in r_tweet['test'][j+1][key[0]]:\n",
    "                edge_attr[edge_to_in[key]][2] += len(r_tweet['test'][j+1][key[0]][key[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34834938-bb28-4ae7-9ed9-cdc67f61ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data():\n",
    "    print(\"loading files\")\n",
    "    follower,friend,e_tweet,m_tweet,r_tweet = open_files()\n",
    "    # make a mapping of the person's id to their index in the node list\n",
    "    # The index will be the index of their x values when making the Data object\n",
    "    id_to_in = {}\n",
    "    in_to_id = list(follower.keys())\n",
    "    i = 0\n",
    "    for key in follower:\n",
    "        id_to_in[key] = i\n",
    "        i += 1\n",
    "        \n",
    "    print(\"getting edges\")\n",
    "    # get edges\n",
    "    nodes1,nodes2,edge_label_train,edge_to_in_train = get_train_edges(follower,friend,id_to_in)\n",
    "    nodes3,nodes4,edge_label_test,edge_to_in_test = get_test_edges(follower,friend,id_to_in)\n",
    "    \n",
    "    # make edge feature arrays\n",
    "    num_edges_train = len(nodes1)\n",
    "    num_edges_test = len(nodes3)\n",
    "    edge_attr1 = [[0 for i in range(3)] for j in range(num_edges_train)]\n",
    "    edge_attr2 = [[0 for i in range(3)] for j in range(num_edges_test)]\n",
    "    \n",
    "    # calculate the clustering coefficients and common neighbors\n",
    "    cluster_coeffs,edge_attr1 = getTrainClustering(follower,friend,edge_attr1,edge_to_in_train)\n",
    "    cluster_coeffs2,edge_attr2 = getTestClustering(follower,friend,edge_attr2,edge_to_in_test)\n",
    "    print(\"exited nx graph calculations\")\n",
    "    \n",
    "    # add retweets and mentions\n",
    "    add_edge_tweets_train(edge_attr1,num_edges_train,edge_to_in_train,nodes1,nodes2,m_tweet,r_tweet)\n",
    "    add_edge_tweets_train(edge_attr2,num_edges_test,edge_to_in_test,nodes3,nodes4,m_tweet,r_tweet)\n",
    "    \n",
    "    \n",
    "    # make node feature arrays\n",
    "    n = len(list(id_to_in.keys()))\n",
    "    x = [[0 for i in range(2)] for j in range(n)]\n",
    "    x2 = [[0 for i in range(2)] for j in range(n)]\n",
    "    \n",
    "    \n",
    "    print(\"adding clustering to feature array\")\n",
    "    # add clustering coefficients to the feature arrays\n",
    "    add_clustering(x,n,in_to_id,cluster_coeffs)\n",
    "    add_clustering(x2,n,in_to_id,cluster_coeffs2)\n",
    "    \n",
    "    \n",
    "    print(\"adding number of tweets\")\n",
    "    # add number of tweets per user\n",
    "    add_tweets_train(x,n,in_to_id,e_tweet)\n",
    "    add_tweets_test(x2,n,in_to_id,e_tweet)\n",
    "    \n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    x2 = torch.tensor(x2, dtype=torch.float)\n",
    "    edge_index = torch.tensor([nodes1,nodes2], dtype=torch.long)\n",
    "    edge_index2 = torch.tensor([nodes3,nodes4], dtype=torch.long)\n",
    "    n_edges = len(nodes1)\n",
    "    n_edges2 = len(nodes3)\n",
    "    edge_label = torch.tensor(edge_label_train, dtype=torch.float)\n",
    "    edge_label2 = torch.tensor(edge_label_test, dtype=torch.float)\n",
    "    edge_attr1 = torch.tensor(edge_attr1, dtype=torch.float)\n",
    "    edge_attr2 = torch.tensor(edge_attr2, dtype=torch.float)\n",
    "    \n",
    "    train_data = Data(x=x, edge_index=edge_index, edge_label_index=edge_index, edge_label=edge_label,edge_attr=edge_attr1)\n",
    "    test_data = Data(x=x2, edge_index=edge_index2, edge_label_index=edge_index2, edge_label=edge_label2,edge_attr=edge_attr2)\n",
    "    \n",
    "    print(\"Done\")\n",
    "    return (train_data,test_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdb37ef2-084c-4019-bb8f-7b11a8344f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading files\n",
      "getting edges\n",
      "Getting train common neighbors\n",
      "Getting train clustering\n",
      "Getting test common neighbors\n",
      "Getting test clustering\n",
      "exited nx graph calculations\n",
      "adding clustering to feature array\n",
      "adding number of tweets\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_data,test_data = make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7251791a-31f0-4514-a9e4-c24ee04d806f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "5715947\n",
      "5682979\n"
     ]
    }
   ],
   "source": [
    "l = train_data.edge_label.tolist()\n",
    "i = -1\n",
    "print(l[-1])\n",
    "print(len(l))\n",
    "for j in range(len(l)):\n",
    "    if i == -1 and l[j] == 1:\n",
    "        i = j\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078e1fb-dee9-4e3f-81d6-ea3c977ce52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10000000149011612, 0.0], [1.0, 0.0], [0.4346197247505188, 5.0], [0.11166881769895554, 9.0], [0.0, 0.0], [0.0, 0.0], [0.2554112672805786, 7.0], [0.0, 0.0], [0.20346319675445557, 0.0], [0.4346764385700226, 0.0], [0.0, 14.0], [0.0679602101445198, 21.0], [0.20705881714820862, 0.0], [0.08176100999116898, 4.0], [1.0, 0.0], [0.4000000059604645, 0.0], [0.1428571492433548, 0.0], [0.12946158647537231, 1.0], [0.0, 0.0], [0.07621333748102188, 1.0]]\n",
      "Epoch: 001, Loss: 179.9043, Train: 0.5449, Test: 0.5687\n",
      "Epoch: 002, Loss: 163.1438, Train: 0.5492, Test: 0.5694\n",
      "Epoch: 003, Loss: 148.5969, Train: 0.5507, Test: 0.5709\n",
      "Epoch: 004, Loss: 135.9909, Train: 0.5525, Test: 0.5743\n",
      "Epoch: 005, Loss: 124.5348, Train: 0.5534, Test: 0.5773\n",
      "Epoch: 006, Loss: 114.4591, Train: 0.5544, Test: 0.5800\n",
      "Epoch: 007, Loss: 105.4352, Train: 0.5554, Test: 0.5812\n",
      "Epoch: 008, Loss: 97.2333, Train: 0.5558, Test: 0.5827\n",
      "Epoch: 009, Loss: 89.8420, Train: 0.5561, Test: 0.5840\n",
      "Epoch: 010, Loss: 83.0518, Train: 0.5560, Test: 0.5852\n",
      "Epoch: 011, Loss: 76.7357, Train: 0.5558, Test: 0.5859\n",
      "Epoch: 012, Loss: 71.0787, Train: 0.5555, Test: 0.5863\n",
      "Epoch: 013, Loss: 65.7696, Train: 0.5552, Test: 0.5872\n",
      "Epoch: 014, Loss: 60.8990, Train: 0.5548, Test: 0.5880\n",
      "Epoch: 015, Loss: 56.4452, Train: 0.5540, Test: 0.5887\n",
      "Epoch: 016, Loss: 52.2242, Train: 0.5530, Test: 0.5893\n",
      "Epoch: 017, Loss: 48.4005, Train: 0.5519, Test: 0.5891\n",
      "Epoch: 018, Loss: 44.8878, Train: 0.5490, Test: 0.5903\n",
      "Epoch: 019, Loss: 41.5169, Train: 0.5455, Test: 0.5916\n",
      "Epoch: 020, Loss: 38.3938, Train: 0.5429, Test: 0.5923\n",
      "Epoch: 021, Loss: 35.5023, Train: 0.5410, Test: 0.5935\n",
      "Epoch: 022, Loss: 32.8133, Train: 0.5393, Test: 0.5940\n",
      "Epoch: 023, Loss: 30.2845, Train: 0.5363, Test: 0.5950\n",
      "Epoch: 024, Loss: 27.8871, Train: 0.5346, Test: 0.5954\n",
      "Epoch: 025, Loss: 25.6851, Train: 0.5310, Test: 0.5951\n",
      "Epoch: 026, Loss: 23.6127, Train: 0.5262, Test: 0.5945\n",
      "Epoch: 027, Loss: 21.7003, Train: 0.5221, Test: 0.5928\n",
      "Epoch: 028, Loss: 19.9364, Train: 0.5159, Test: 0.5917\n",
      "Epoch: 029, Loss: 18.2835, Train: 0.5084, Test: 0.5891\n",
      "Epoch: 030, Loss: 16.7015, Train: 0.4964, Test: 0.5810\n",
      "Epoch: 031, Loss: 15.2908, Train: 0.4842, Test: 0.5651\n",
      "Epoch: 032, Loss: 13.9873, Train: 0.4736, Test: 0.5399\n",
      "Epoch: 033, Loss: 12.7808, Train: 0.4653, Test: 0.5031\n",
      "Epoch: 034, Loss: 11.6546, Train: 0.4644, Test: 0.4712\n",
      "Epoch: 035, Loss: 10.6214, Train: 0.4645, Test: 0.4491\n",
      "Epoch: 036, Loss: 9.6854, Train: 0.4640, Test: 0.4342\n",
      "Epoch: 037, Loss: 8.8093, Train: 0.4647, Test: 0.4276\n",
      "Epoch: 038, Loss: 8.0230, Train: 0.4646, Test: 0.4221\n",
      "Epoch: 039, Loss: 7.2879, Train: 0.4639, Test: 0.4171\n",
      "Epoch: 040, Loss: 6.6197, Train: 0.4631, Test: 0.4150\n",
      "Epoch: 041, Loss: 6.0210, Train: 0.4626, Test: 0.4131\n",
      "Epoch: 042, Loss: 5.4714, Train: 0.4608, Test: 0.4104\n",
      "Epoch: 043, Loss: 4.9659, Train: 0.4601, Test: 0.4070\n",
      "Epoch: 044, Loss: 4.5109, Train: 0.4582, Test: 0.4043\n",
      "Epoch: 045, Loss: 4.1000, Train: 0.4562, Test: 0.4011\n",
      "Epoch: 046, Loss: 3.7295, Train: 0.4550, Test: 0.3982\n",
      "Epoch: 047, Loss: 3.3904, Train: 0.4541, Test: 0.3961\n",
      "Epoch: 048, Loss: 3.0841, Train: 0.4530, Test: 0.3957\n",
      "Epoch: 049, Loss: 2.8078, Train: 0.4519, Test: 0.3939\n",
      "Epoch: 050, Loss: 2.5628, Train: 0.4500, Test: 0.3923\n",
      "Epoch: 051, Loss: 2.3385, Train: 0.4476, Test: 0.3916\n",
      "Epoch: 052, Loss: 2.1352, Train: 0.4464, Test: 0.3919\n",
      "Epoch: 053, Loss: 1.9540, Train: 0.4447, Test: 0.3925\n",
      "Epoch: 054, Loss: 1.7924, Train: 0.4437, Test: 0.3916\n",
      "Epoch: 055, Loss: 1.6496, Train: 0.4423, Test: 0.3912\n",
      "Epoch: 056, Loss: 1.5206, Train: 0.4407, Test: 0.3909\n",
      "Epoch: 057, Loss: 1.4104, Train: 0.4396, Test: 0.3909\n",
      "Epoch: 058, Loss: 1.3146, Train: 0.4381, Test: 0.3911\n",
      "Epoch: 059, Loss: 1.2343, Train: 0.4367, Test: 0.3916\n",
      "Epoch: 060, Loss: 1.1664, Train: 0.4350, Test: 0.3921\n",
      "Epoch: 061, Loss: 1.1106, Train: 0.4327, Test: 0.3923\n",
      "Epoch: 062, Loss: 1.0627, Train: 0.4298, Test: 0.3939\n",
      "Epoch: 063, Loss: 1.0215, Train: 0.4277, Test: 0.3955\n",
      "Epoch: 064, Loss: 0.9853, Train: 0.4256, Test: 0.3973\n",
      "Epoch: 065, Loss: 0.9517, Train: 0.4245, Test: 0.4008\n",
      "Epoch: 066, Loss: 0.9204, Train: 0.4220, Test: 0.4049\n",
      "Epoch: 067, Loss: 0.8908, Train: 0.4234, Test: 0.4093\n",
      "Epoch: 068, Loss: 0.8642, Train: 0.4260, Test: 0.4136\n",
      "Epoch: 069, Loss: 0.8426, Train: 0.4282, Test: 0.4181\n",
      "Epoch: 070, Loss: 0.8280, Train: 0.4304, Test: 0.4220\n",
      "Epoch: 071, Loss: 0.8193, Train: 0.4313, Test: 0.4258\n",
      "Epoch: 072, Loss: 0.8148, Train: 0.4312, Test: 0.4294\n",
      "Epoch: 073, Loss: 0.8123, Train: 0.4314, Test: 0.4334\n",
      "Epoch: 074, Loss: 0.8103, Train: 0.4309, Test: 0.4379\n",
      "Epoch: 075, Loss: 0.8087, Train: 0.4297, Test: 0.4431\n",
      "Epoch: 076, Loss: 0.8069, Train: 0.4284, Test: 0.4493\n",
      "Epoch: 077, Loss: 0.8050, Train: 0.4270, Test: 0.4565\n",
      "Epoch: 078, Loss: 0.8031, Train: 0.4248, Test: 0.4639\n",
      "Epoch: 079, Loss: 0.8012, Train: 0.4220, Test: 0.4709\n",
      "Epoch: 080, Loss: 0.7992, Train: 0.4202, Test: 0.4780\n",
      "Epoch: 081, Loss: 0.7971, Train: 0.4195, Test: 0.4854\n",
      "Epoch: 082, Loss: 0.7950, Train: 0.4193, Test: 0.4932\n",
      "Epoch: 083, Loss: 0.7929, Train: 0.4193, Test: 0.5008\n",
      "Epoch: 084, Loss: 0.7906, Train: 0.4198, Test: 0.5088\n",
      "Epoch: 085, Loss: 0.7883, Train: 0.4213, Test: 0.5177\n",
      "Epoch: 086, Loss: 0.7858, Train: 0.4240, Test: 0.5275\n",
      "Epoch: 087, Loss: 0.7833, Train: 0.4279, Test: 0.5377\n",
      "Epoch: 088, Loss: 0.7808, Train: 0.4323, Test: 0.5473\n",
      "Epoch: 089, Loss: 0.7782, Train: 0.4371, Test: 0.5560\n",
      "Epoch: 090, Loss: 0.7755, Train: 0.4417, Test: 0.5636\n",
      "Epoch: 091, Loss: 0.7729, Train: 0.4458, Test: 0.5703\n",
      "Epoch: 092, Loss: 0.7701, Train: 0.4490, Test: 0.5762\n",
      "Epoch: 093, Loss: 0.7674, Train: 0.4517, Test: 0.5812\n",
      "Epoch: 094, Loss: 0.7646, Train: 0.4538, Test: 0.5855\n",
      "Epoch: 095, Loss: 0.7618, Train: 0.4555, Test: 0.5890\n",
      "Epoch: 096, Loss: 0.7591, Train: 0.4568, Test: 0.5919\n",
      "Epoch: 097, Loss: 0.7563, Train: 0.4578, Test: 0.5943\n",
      "Epoch: 098, Loss: 0.7536, Train: 0.4585, Test: 0.5963\n",
      "Epoch: 099, Loss: 0.7509, Train: 0.4589, Test: 0.5977\n",
      "Epoch: 100, Loss: 0.7483, Train: 0.4590, Test: 0.5984\n",
      "Epoch: 101, Loss: 0.7457, Train: 0.4591, Test: 0.5983\n",
      "Epoch: 102, Loss: 0.7432, Train: 0.4597, Test: 0.5979\n",
      "Epoch: 103, Loss: 0.7407, Train: 0.4609, Test: 0.5976\n",
      "Epoch: 104, Loss: 0.7383, Train: 0.4623, Test: 0.5976\n",
      "Epoch: 105, Loss: 0.7361, Train: 0.4639, Test: 0.5975\n",
      "Epoch: 106, Loss: 0.7339, Train: 0.4655, Test: 0.5974\n",
      "Epoch: 107, Loss: 0.7318, Train: 0.4674, Test: 0.5971\n",
      "Epoch: 108, Loss: 0.7298, Train: 0.4695, Test: 0.5968\n",
      "Epoch: 109, Loss: 0.7279, Train: 0.4718, Test: 0.5965\n",
      "Epoch: 110, Loss: 0.7261, Train: 0.4744, Test: 0.5961\n",
      "Epoch: 111, Loss: 0.7245, Train: 0.4772, Test: 0.5955\n",
      "Epoch: 112, Loss: 0.7229, Train: 0.4803, Test: 0.5947\n",
      "Epoch: 113, Loss: 0.7213, Train: 0.4835, Test: 0.5937\n",
      "Epoch: 114, Loss: 0.7199, Train: 0.4868, Test: 0.5926\n",
      "Epoch: 115, Loss: 0.7186, Train: 0.4901, Test: 0.5913\n",
      "Epoch: 116, Loss: 0.7173, Train: 0.4935, Test: 0.5899\n",
      "Epoch: 117, Loss: 0.7162, Train: 0.4968, Test: 0.5885\n",
      "Epoch: 118, Loss: 0.7150, Train: 0.5000, Test: 0.5871\n",
      "Epoch: 119, Loss: 0.7140, Train: 0.5029, Test: 0.5856\n",
      "Epoch: 120, Loss: 0.7130, Train: 0.5057, Test: 0.5842\n",
      "Epoch: 121, Loss: 0.7121, Train: 0.5081, Test: 0.5828\n",
      "Epoch: 122, Loss: 0.7112, Train: 0.5102, Test: 0.5815\n",
      "Epoch: 123, Loss: 0.7104, Train: 0.5121, Test: 0.5802\n",
      "Epoch: 124, Loss: 0.7096, Train: 0.5136, Test: 0.5790\n",
      "Epoch: 125, Loss: 0.7088, Train: 0.5150, Test: 0.5780\n",
      "Epoch: 126, Loss: 0.7081, Train: 0.5160, Test: 0.5771\n",
      "Epoch: 127, Loss: 0.7075, Train: 0.5169, Test: 0.5763\n",
      "Epoch: 128, Loss: 0.7069, Train: 0.5176, Test: 0.5756\n",
      "Epoch: 129, Loss: 0.7063, Train: 0.5182, Test: 0.5752\n",
      "Epoch: 130, Loss: 0.7057, Train: 0.5186, Test: 0.5748\n",
      "Epoch: 131, Loss: 0.7052, Train: 0.5188, Test: 0.5746\n",
      "Epoch: 132, Loss: 0.7047, Train: 0.5190, Test: 0.5744\n",
      "Epoch: 133, Loss: 0.7042, Train: 0.5190, Test: 0.5744\n",
      "Epoch: 134, Loss: 0.7038, Train: 0.5189, Test: 0.5745\n",
      "Epoch: 135, Loss: 0.7034, Train: 0.5188, Test: 0.5747\n",
      "Epoch: 136, Loss: 0.7030, Train: 0.5186, Test: 0.5749\n",
      "Epoch: 137, Loss: 0.7027, Train: 0.5183, Test: 0.5752\n",
      "Epoch: 138, Loss: 0.7023, Train: 0.5180, Test: 0.5755\n",
      "Epoch: 139, Loss: 0.7020, Train: 0.5176, Test: 0.5758\n",
      "Epoch: 140, Loss: 0.7018, Train: 0.5172, Test: 0.5762\n",
      "Epoch: 141, Loss: 0.7015, Train: 0.5167, Test: 0.5765\n",
      "Epoch: 142, Loss: 0.7013, Train: 0.5163, Test: 0.5769\n",
      "Epoch: 143, Loss: 0.7010, Train: 0.5158, Test: 0.5773\n",
      "Epoch: 144, Loss: 0.7009, Train: 0.5154, Test: 0.5776\n",
      "Epoch: 145, Loss: 0.7007, Train: 0.5149, Test: 0.5779\n",
      "Epoch: 146, Loss: 0.7005, Train: 0.5144, Test: 0.5782\n",
      "Epoch: 147, Loss: 0.7004, Train: 0.5140, Test: 0.5785\n",
      "Epoch: 148, Loss: 0.7002, Train: 0.5136, Test: 0.5788\n",
      "Epoch: 149, Loss: 0.7001, Train: 0.5132, Test: 0.5790\n",
      "Epoch: 150, Loss: 0.7000, Train: 0.5128, Test: 0.5792\n",
      "Epoch: 151, Loss: 0.6999, Train: 0.5125, Test: 0.5794\n",
      "Epoch: 152, Loss: 0.6998, Train: 0.5122, Test: 0.5795\n",
      "Epoch: 153, Loss: 0.6998, Train: 0.5119, Test: 0.5797\n",
      "Epoch: 154, Loss: 0.6997, Train: 0.5116, Test: 0.5798\n",
      "Epoch: 155, Loss: 0.6996, Train: 0.5114, Test: 0.5799\n",
      "Epoch: 156, Loss: 0.6996, Train: 0.5112, Test: 0.5799\n",
      "Epoch: 157, Loss: 0.6996, Train: 0.5110, Test: 0.5800\n",
      "Epoch: 158, Loss: 0.6995, Train: 0.5109, Test: 0.5800\n",
      "Epoch: 159, Loss: 0.6995, Train: 0.5108, Test: 0.5800\n",
      "Epoch: 160, Loss: 0.6994, Train: 0.5107, Test: 0.5800\n",
      "Epoch: 161, Loss: 0.6994, Train: 0.5107, Test: 0.5799\n",
      "Epoch: 162, Loss: 0.6994, Train: 0.5106, Test: 0.5799\n",
      "Epoch: 163, Loss: 0.6993, Train: 0.5106, Test: 0.5799\n",
      "Epoch: 164, Loss: 0.6993, Train: 0.5106, Test: 0.5798\n",
      "Epoch: 165, Loss: 0.6993, Train: 0.5106, Test: 0.5797\n",
      "Epoch: 166, Loss: 0.6993, Train: 0.5106, Test: 0.5797\n",
      "Epoch: 167, Loss: 0.6993, Train: 0.5106, Test: 0.5796\n",
      "Epoch: 168, Loss: 0.6992, Train: 0.5107, Test: 0.5795\n",
      "Epoch: 169, Loss: 0.6992, Train: 0.5107, Test: 0.5795\n",
      "Epoch: 170, Loss: 0.6992, Train: 0.5107, Test: 0.5794\n",
      "Epoch: 171, Loss: 0.6992, Train: 0.5108, Test: 0.5793\n",
      "Epoch: 172, Loss: 0.6992, Train: 0.5108, Test: 0.5792\n",
      "Epoch: 173, Loss: 0.6991, Train: 0.5108, Test: 0.5792\n",
      "Epoch: 174, Loss: 0.6991, Train: 0.5109, Test: 0.5791\n",
      "Epoch: 175, Loss: 0.6991, Train: 0.5109, Test: 0.5791\n",
      "Epoch: 176, Loss: 0.6991, Train: 0.5109, Test: 0.5790\n",
      "Epoch: 177, Loss: 0.6991, Train: 0.5109, Test: 0.5790\n",
      "Epoch: 178, Loss: 0.6991, Train: 0.5109, Test: 0.5790\n",
      "Epoch: 179, Loss: 0.6991, Train: 0.5109, Test: 0.5789\n",
      "Epoch: 180, Loss: 0.6990, Train: 0.5109, Test: 0.5789\n",
      "Epoch: 181, Loss: 0.6990, Train: 0.5109, Test: 0.5789\n",
      "Epoch: 182, Loss: 0.6990, Train: 0.5108, Test: 0.5789\n",
      "Epoch: 183, Loss: 0.6990, Train: 0.5108, Test: 0.5789\n",
      "Epoch: 184, Loss: 0.6990, Train: 0.5107, Test: 0.5789\n",
      "Epoch: 185, Loss: 0.6989, Train: 0.5107, Test: 0.5789\n",
      "Epoch: 186, Loss: 0.6989, Train: 0.5106, Test: 0.5790\n",
      "Epoch: 187, Loss: 0.6989, Train: 0.5106, Test: 0.5790\n",
      "Epoch: 188, Loss: 0.6989, Train: 0.5105, Test: 0.5790\n",
      "Epoch: 189, Loss: 0.6989, Train: 0.5104, Test: 0.5791\n",
      "Epoch: 190, Loss: 0.6989, Train: 0.5103, Test: 0.5791\n",
      "Epoch: 191, Loss: 0.6988, Train: 0.5102, Test: 0.5792\n",
      "Epoch: 192, Loss: 0.6988, Train: 0.5101, Test: 0.5793\n",
      "Epoch: 193, Loss: 0.6988, Train: 0.5100, Test: 0.5793\n",
      "Epoch: 194, Loss: 0.6988, Train: 0.5099, Test: 0.5794\n",
      "Epoch: 195, Loss: 0.6988, Train: 0.5098, Test: 0.5794\n",
      "Epoch: 196, Loss: 0.6987, Train: 0.5097, Test: 0.5795\n",
      "Epoch: 197, Loss: 0.6987, Train: 0.5096, Test: 0.5796\n",
      "Epoch: 198, Loss: 0.6987, Train: 0.5094, Test: 0.5796\n",
      "Epoch: 199, Loss: 0.6987, Train: 0.5093, Test: 0.5797\n",
      "Epoch: 200, Loss: 0.6987, Train: 0.5092, Test: 0.5798\n",
      "Epoch: 201, Loss: 0.6987, Train: 0.5091, Test: 0.5798\n",
      "Epoch: 202, Loss: 0.6986, Train: 0.5090, Test: 0.5799\n",
      "Epoch: 203, Loss: 0.6986, Train: 0.5089, Test: 0.5800\n",
      "Epoch: 204, Loss: 0.6986, Train: 0.5088, Test: 0.5800\n",
      "Epoch: 205, Loss: 0.6986, Train: 0.5086, Test: 0.5801\n",
      "Epoch: 206, Loss: 0.6986, Train: 0.5085, Test: 0.5802\n",
      "Epoch: 207, Loss: 0.6985, Train: 0.5084, Test: 0.5802\n",
      "Epoch: 208, Loss: 0.6985, Train: 0.5083, Test: 0.5803\n",
      "Epoch: 209, Loss: 0.6985, Train: 0.5082, Test: 0.5803\n",
      "Epoch: 210, Loss: 0.6985, Train: 0.5081, Test: 0.5804\n",
      "Epoch: 211, Loss: 0.6985, Train: 0.5080, Test: 0.5804\n",
      "Epoch: 212, Loss: 0.6985, Train: 0.5079, Test: 0.5805\n",
      "Epoch: 213, Loss: 0.6984, Train: 0.5078, Test: 0.5805\n",
      "Epoch: 214, Loss: 0.6984, Train: 0.5077, Test: 0.5806\n",
      "Epoch: 215, Loss: 0.6984, Train: 0.5076, Test: 0.5806\n",
      "Epoch: 216, Loss: 0.6984, Train: 0.5076, Test: 0.5807\n",
      "Epoch: 217, Loss: 0.6984, Train: 0.5075, Test: 0.5807\n",
      "Epoch: 218, Loss: 0.6984, Train: 0.5074, Test: 0.5807\n",
      "Epoch: 219, Loss: 0.6984, Train: 0.5073, Test: 0.5808\n",
      "Epoch: 220, Loss: 0.6983, Train: 0.5072, Test: 0.5808\n",
      "Epoch: 221, Loss: 0.6983, Train: 0.5071, Test: 0.5809\n",
      "Epoch: 222, Loss: 0.6983, Train: 0.5070, Test: 0.5809\n",
      "Epoch: 223, Loss: 0.6983, Train: 0.5070, Test: 0.5809\n",
      "Epoch: 224, Loss: 0.6983, Train: 0.5069, Test: 0.5810\n",
      "Epoch: 225, Loss: 0.6983, Train: 0.5068, Test: 0.5810\n",
      "Epoch: 226, Loss: 0.6983, Train: 0.5067, Test: 0.5810\n",
      "Epoch: 227, Loss: 0.6982, Train: 0.5067, Test: 0.5810\n",
      "Epoch: 228, Loss: 0.6982, Train: 0.5066, Test: 0.5811\n",
      "Epoch: 229, Loss: 0.6982, Train: 0.5065, Test: 0.5811\n",
      "Epoch: 230, Loss: 0.6982, Train: 0.5064, Test: 0.5811\n",
      "Epoch: 231, Loss: 0.6982, Train: 0.5064, Test: 0.5812\n",
      "Epoch: 232, Loss: 0.6982, Train: 0.5063, Test: 0.5812\n",
      "Epoch: 233, Loss: 0.6981, Train: 0.5062, Test: 0.5812\n",
      "Epoch: 234, Loss: 0.6981, Train: 0.5061, Test: 0.5813\n",
      "Epoch: 235, Loss: 0.6981, Train: 0.5060, Test: 0.5813\n",
      "Epoch: 236, Loss: 0.6981, Train: 0.5060, Test: 0.5813\n",
      "Epoch: 237, Loss: 0.6981, Train: 0.5059, Test: 0.5813\n",
      "Epoch: 238, Loss: 0.6981, Train: 0.5058, Test: 0.5814\n",
      "Epoch: 239, Loss: 0.6981, Train: 0.5057, Test: 0.5814\n",
      "Epoch: 240, Loss: 0.6980, Train: 0.5057, Test: 0.5814\n",
      "Epoch: 241, Loss: 0.6980, Train: 0.5056, Test: 0.5815\n",
      "Epoch: 242, Loss: 0.6980, Train: 0.5055, Test: 0.5815\n",
      "Epoch: 243, Loss: 0.6980, Train: 0.5054, Test: 0.5815\n",
      "Epoch: 244, Loss: 0.6980, Train: 0.5054, Test: 0.5815\n",
      "Epoch: 245, Loss: 0.6980, Train: 0.5053, Test: 0.5816\n",
      "Epoch: 246, Loss: 0.6979, Train: 0.5052, Test: 0.5816\n",
      "Epoch: 247, Loss: 0.6979, Train: 0.5051, Test: 0.5816\n",
      "Epoch: 248, Loss: 0.6979, Train: 0.5051, Test: 0.5816\n",
      "Epoch: 249, Loss: 0.6979, Train: 0.5050, Test: 0.5817\n",
      "Epoch: 250, Loss: 0.6979, Train: 0.5049, Test: 0.5817\n",
      "Epoch: 251, Loss: 0.6979, Train: 0.5048, Test: 0.5817\n",
      "Epoch: 252, Loss: 0.6979, Train: 0.5048, Test: 0.5818\n",
      "Epoch: 253, Loss: 0.6978, Train: 0.5047, Test: 0.5818\n",
      "Epoch: 254, Loss: 0.6978, Train: 0.5046, Test: 0.5818\n",
      "Epoch: 255, Loss: 0.6978, Train: 0.5045, Test: 0.5818\n",
      "Epoch: 256, Loss: 0.6978, Train: 0.5045, Test: 0.5819\n",
      "Epoch: 257, Loss: 0.6978, Train: 0.5044, Test: 0.5819\n",
      "Epoch: 258, Loss: 0.6978, Train: 0.5043, Test: 0.5819\n",
      "Epoch: 259, Loss: 0.6978, Train: 0.5042, Test: 0.5819\n",
      "Epoch: 260, Loss: 0.6978, Train: 0.5042, Test: 0.5820\n",
      "Epoch: 261, Loss: 0.6978, Train: 0.5041, Test: 0.5820\n",
      "Epoch: 262, Loss: 0.6977, Train: 0.5040, Test: 0.5820\n",
      "Epoch: 263, Loss: 0.6977, Train: 0.5039, Test: 0.5820\n",
      "Epoch: 264, Loss: 0.6977, Train: 0.5039, Test: 0.5821\n",
      "Epoch: 265, Loss: 0.6977, Train: 0.5038, Test: 0.5821\n",
      "Epoch: 266, Loss: 0.6977, Train: 0.5037, Test: 0.5821\n",
      "Epoch: 267, Loss: 0.6977, Train: 0.5036, Test: 0.5821\n",
      "Epoch: 268, Loss: 0.6977, Train: 0.5036, Test: 0.5822\n",
      "Epoch: 269, Loss: 0.6976, Train: 0.5035, Test: 0.5822\n",
      "Epoch: 270, Loss: 0.6976, Train: 0.5034, Test: 0.5822\n",
      "Epoch: 271, Loss: 0.6976, Train: 0.5033, Test: 0.5823\n",
      "Epoch: 272, Loss: 0.6976, Train: 0.5033, Test: 0.5823\n",
      "Epoch: 273, Loss: 0.6976, Train: 0.5032, Test: 0.5823\n",
      "Epoch: 274, Loss: 0.6976, Train: 0.5031, Test: 0.5823\n",
      "Epoch: 275, Loss: 0.6976, Train: 0.5030, Test: 0.5824\n",
      "Epoch: 276, Loss: 0.6975, Train: 0.5030, Test: 0.5824\n",
      "Epoch: 277, Loss: 0.6975, Train: 0.5029, Test: 0.5824\n",
      "Epoch: 278, Loss: 0.6975, Train: 0.5028, Test: 0.5824\n",
      "Epoch: 279, Loss: 0.6975, Train: 0.5028, Test: 0.5825\n",
      "Epoch: 280, Loss: 0.6975, Train: 0.5027, Test: 0.5825\n",
      "Epoch: 281, Loss: 0.6975, Train: 0.5026, Test: 0.5825\n",
      "Epoch: 282, Loss: 0.6975, Train: 0.5025, Test: 0.5825\n",
      "Epoch: 283, Loss: 0.6975, Train: 0.5025, Test: 0.5826\n",
      "Epoch: 284, Loss: 0.6974, Train: 0.5024, Test: 0.5826\n",
      "Epoch: 285, Loss: 0.6974, Train: 0.5023, Test: 0.5826\n",
      "Epoch: 286, Loss: 0.6974, Train: 0.5023, Test: 0.5826\n",
      "Epoch: 287, Loss: 0.6974, Train: 0.5022, Test: 0.5827\n",
      "Epoch: 288, Loss: 0.6974, Train: 0.5021, Test: 0.5827\n",
      "Epoch: 289, Loss: 0.6974, Train: 0.5021, Test: 0.5827\n",
      "Epoch: 290, Loss: 0.6974, Train: 0.5020, Test: 0.5827\n",
      "Epoch: 291, Loss: 0.6974, Train: 0.5019, Test: 0.5828\n",
      "Epoch: 292, Loss: 0.6974, Train: 0.5019, Test: 0.5828\n",
      "Epoch: 293, Loss: 0.6973, Train: 0.5018, Test: 0.5828\n",
      "Epoch: 294, Loss: 0.6973, Train: 0.5017, Test: 0.5828\n",
      "Epoch: 295, Loss: 0.6973, Train: 0.5016, Test: 0.5829\n",
      "Epoch: 296, Loss: 0.6973, Train: 0.5016, Test: 0.5829\n",
      "Epoch: 297, Loss: 0.6973, Train: 0.5015, Test: 0.5829\n",
      "Epoch: 298, Loss: 0.6973, Train: 0.5014, Test: 0.5829\n",
      "Epoch: 299, Loss: 0.6973, Train: 0.5014, Test: 0.5830\n",
      "Epoch: 300, Loss: 0.6973, Train: 0.5013, Test: 0.5830\n",
      "Epoch: 301, Loss: 0.6972, Train: 0.5012, Test: 0.5830\n",
      "Epoch: 302, Loss: 0.6972, Train: 0.5012, Test: 0.5830\n",
      "Epoch: 303, Loss: 0.6972, Train: 0.5011, Test: 0.5831\n",
      "Epoch: 304, Loss: 0.6972, Train: 0.5010, Test: 0.5831\n",
      "Epoch: 305, Loss: 0.6972, Train: 0.5009, Test: 0.5831\n",
      "Epoch: 306, Loss: 0.6972, Train: 0.5009, Test: 0.5831\n",
      "Epoch: 307, Loss: 0.6972, Train: 0.5008, Test: 0.5832\n",
      "Epoch: 308, Loss: 0.6972, Train: 0.5007, Test: 0.5832\n",
      "Epoch: 309, Loss: 0.6971, Train: 0.5007, Test: 0.5832\n",
      "Epoch: 310, Loss: 0.6971, Train: 0.5006, Test: 0.5832\n",
      "Epoch: 311, Loss: 0.6971, Train: 0.5005, Test: 0.5833\n",
      "Epoch: 312, Loss: 0.6971, Train: 0.5005, Test: 0.5833\n",
      "Epoch: 313, Loss: 0.6971, Train: 0.5004, Test: 0.5833\n",
      "Epoch: 314, Loss: 0.6971, Train: 0.5003, Test: 0.5833\n",
      "Epoch: 315, Loss: 0.6971, Train: 0.5002, Test: 0.5834\n",
      "Epoch: 316, Loss: 0.6971, Train: 0.5002, Test: 0.5834\n",
      "Epoch: 317, Loss: 0.6971, Train: 0.5001, Test: 0.5834\n",
      "Epoch: 318, Loss: 0.6971, Train: 0.5000, Test: 0.5834\n",
      "Epoch: 319, Loss: 0.6970, Train: 0.5000, Test: 0.5835\n",
      "Epoch: 320, Loss: 0.6970, Train: 0.4999, Test: 0.5835\n",
      "Epoch: 321, Loss: 0.6970, Train: 0.4998, Test: 0.5835\n",
      "Epoch: 322, Loss: 0.6970, Train: 0.4998, Test: 0.5835\n",
      "Epoch: 323, Loss: 0.6970, Train: 0.4997, Test: 0.5836\n",
      "Epoch: 324, Loss: 0.6970, Train: 0.4996, Test: 0.5836\n",
      "Epoch: 325, Loss: 0.6970, Train: 0.4996, Test: 0.5836\n",
      "Epoch: 326, Loss: 0.6970, Train: 0.4995, Test: 0.5836\n",
      "Epoch: 327, Loss: 0.6969, Train: 0.4994, Test: 0.5836\n",
      "Epoch: 328, Loss: 0.6970, Train: 0.4994, Test: 0.5837\n",
      "Epoch: 329, Loss: 0.6969, Train: 0.4993, Test: 0.5837\n",
      "Epoch: 330, Loss: 0.6969, Train: 0.4992, Test: 0.5837\n",
      "Epoch: 331, Loss: 0.6969, Train: 0.4992, Test: 0.5837\n",
      "Epoch: 332, Loss: 0.6969, Train: 0.4991, Test: 0.5838\n",
      "Epoch: 333, Loss: 0.6969, Train: 0.4990, Test: 0.5838\n",
      "Epoch: 334, Loss: 0.6969, Train: 0.4990, Test: 0.5838\n",
      "Epoch: 335, Loss: 0.6969, Train: 0.4989, Test: 0.5838\n",
      "Epoch: 336, Loss: 0.6969, Train: 0.4989, Test: 0.5838\n",
      "Epoch: 337, Loss: 0.6969, Train: 0.4988, Test: 0.5839\n",
      "Epoch: 338, Loss: 0.6968, Train: 0.4987, Test: 0.5839\n",
      "Epoch: 339, Loss: 0.6968, Train: 0.4987, Test: 0.5839\n",
      "Epoch: 340, Loss: 0.6968, Train: 0.4986, Test: 0.5839\n",
      "Epoch: 341, Loss: 0.6968, Train: 0.4985, Test: 0.5839\n",
      "Epoch: 342, Loss: 0.6968, Train: 0.4985, Test: 0.5840\n",
      "Epoch: 343, Loss: 0.6968, Train: 0.4984, Test: 0.5840\n",
      "Epoch: 344, Loss: 0.6968, Train: 0.4983, Test: 0.5840\n",
      "Epoch: 345, Loss: 0.6968, Train: 0.4983, Test: 0.5840\n",
      "Epoch: 346, Loss: 0.6968, Train: 0.4982, Test: 0.5841\n",
      "Epoch: 347, Loss: 0.6968, Train: 0.4981, Test: 0.5841\n",
      "Epoch: 348, Loss: 0.6967, Train: 0.4981, Test: 0.5841\n",
      "Epoch: 349, Loss: 0.6967, Train: 0.4980, Test: 0.5841\n",
      "Epoch: 350, Loss: 0.6967, Train: 0.4979, Test: 0.5841\n",
      "Epoch: 351, Loss: 0.6967, Train: 0.4979, Test: 0.5842\n",
      "Epoch: 352, Loss: 0.6967, Train: 0.4978, Test: 0.5842\n",
      "Epoch: 353, Loss: 0.6967, Train: 0.4977, Test: 0.5842\n",
      "Epoch: 354, Loss: 0.6967, Train: 0.4977, Test: 0.5842\n",
      "Epoch: 355, Loss: 0.6967, Train: 0.4976, Test: 0.5842\n",
      "Epoch: 356, Loss: 0.6967, Train: 0.4975, Test: 0.5843\n",
      "Epoch: 357, Loss: 0.6967, Train: 0.4975, Test: 0.5843\n",
      "Epoch: 358, Loss: 0.6967, Train: 0.4974, Test: 0.5843\n",
      "Epoch: 359, Loss: 0.6966, Train: 0.4973, Test: 0.5843\n",
      "Epoch: 360, Loss: 0.6966, Train: 0.4973, Test: 0.5843\n",
      "Epoch: 361, Loss: 0.6966, Train: 0.4972, Test: 0.5844\n",
      "Epoch: 362, Loss: 0.6966, Train: 0.4972, Test: 0.5844\n",
      "Epoch: 363, Loss: 0.6966, Train: 0.4971, Test: 0.5844\n",
      "Epoch: 364, Loss: 0.6966, Train: 0.4970, Test: 0.5844\n",
      "Epoch: 365, Loss: 0.6966, Train: 0.4970, Test: 0.5844\n",
      "Epoch: 366, Loss: 0.6966, Train: 0.4969, Test: 0.5845\n",
      "Epoch: 367, Loss: 0.6966, Train: 0.4968, Test: 0.5845\n",
      "Epoch: 368, Loss: 0.6966, Train: 0.4968, Test: 0.5845\n",
      "Epoch: 369, Loss: 0.6966, Train: 0.4967, Test: 0.5845\n",
      "Epoch: 370, Loss: 0.6965, Train: 0.4967, Test: 0.5845\n",
      "Epoch: 371, Loss: 0.6965, Train: 0.4966, Test: 0.5845\n",
      "Epoch: 372, Loss: 0.6965, Train: 0.4965, Test: 0.5846\n",
      "Epoch: 373, Loss: 0.6965, Train: 0.4965, Test: 0.5846\n",
      "Epoch: 374, Loss: 0.6965, Train: 0.4964, Test: 0.5846\n",
      "Epoch: 375, Loss: 0.6965, Train: 0.4964, Test: 0.5846\n",
      "Epoch: 376, Loss: 0.6965, Train: 0.4963, Test: 0.5846\n",
      "Epoch: 377, Loss: 0.6965, Train: 0.4962, Test: 0.5847\n",
      "Epoch: 378, Loss: 0.6965, Train: 0.4962, Test: 0.5847\n",
      "Epoch: 379, Loss: 0.6965, Train: 0.4961, Test: 0.5847\n",
      "Epoch: 380, Loss: 0.6965, Train: 0.4960, Test: 0.5847\n",
      "Epoch: 381, Loss: 0.6964, Train: 0.4960, Test: 0.5847\n",
      "Epoch: 382, Loss: 0.6964, Train: 0.4959, Test: 0.5848\n",
      "Epoch: 383, Loss: 0.6964, Train: 0.4959, Test: 0.5848\n",
      "Epoch: 384, Loss: 0.6964, Train: 0.4958, Test: 0.5848\n",
      "Epoch: 385, Loss: 0.6964, Train: 0.4957, Test: 0.5848\n",
      "Epoch: 386, Loss: 0.6964, Train: 0.4957, Test: 0.5848\n",
      "Epoch: 387, Loss: 0.6964, Train: 0.4956, Test: 0.5849\n",
      "Epoch: 388, Loss: 0.6964, Train: 0.4955, Test: 0.5849\n",
      "Epoch: 389, Loss: 0.6964, Train: 0.4955, Test: 0.5849\n",
      "Epoch: 390, Loss: 0.6964, Train: 0.4954, Test: 0.5849\n",
      "Epoch: 391, Loss: 0.6964, Train: 0.4954, Test: 0.5849\n",
      "Epoch: 392, Loss: 0.6964, Train: 0.4953, Test: 0.5849\n",
      "Epoch: 393, Loss: 0.6964, Train: 0.4952, Test: 0.5850\n",
      "Epoch: 394, Loss: 0.6964, Train: 0.4952, Test: 0.5850\n",
      "Epoch: 395, Loss: 0.6963, Train: 0.4951, Test: 0.5850\n",
      "Epoch: 396, Loss: 0.6963, Train: 0.4951, Test: 0.5850\n",
      "Epoch: 397, Loss: 0.6963, Train: 0.4950, Test: 0.5850\n",
      "Epoch: 398, Loss: 0.6963, Train: 0.4950, Test: 0.5850\n",
      "Epoch: 399, Loss: 0.6963, Train: 0.4949, Test: 0.5851\n",
      "Epoch: 400, Loss: 0.6963, Train: 0.4948, Test: 0.5851\n",
      "Epoch: 401, Loss: 0.6963, Train: 0.4948, Test: 0.5851\n",
      "Epoch: 402, Loss: 0.6963, Train: 0.4947, Test: 0.5851\n",
      "Epoch: 403, Loss: 0.6963, Train: 0.4947, Test: 0.5851\n",
      "Epoch: 404, Loss: 0.6963, Train: 0.4946, Test: 0.5851\n",
      "Epoch: 405, Loss: 0.6963, Train: 0.4946, Test: 0.5852\n",
      "Epoch: 406, Loss: 0.6963, Train: 0.4945, Test: 0.5852\n",
      "Epoch: 407, Loss: 0.6962, Train: 0.4945, Test: 0.5852\n",
      "Epoch: 408, Loss: 0.6962, Train: 0.4944, Test: 0.5852\n",
      "Epoch: 409, Loss: 0.6962, Train: 0.4943, Test: 0.5852\n",
      "Epoch: 410, Loss: 0.6962, Train: 0.4943, Test: 0.5852\n",
      "Epoch: 411, Loss: 0.6962, Train: 0.4942, Test: 0.5853\n",
      "Epoch: 412, Loss: 0.6962, Train: 0.4942, Test: 0.5853\n",
      "Epoch: 413, Loss: 0.6962, Train: 0.4941, Test: 0.5853\n",
      "Epoch: 414, Loss: 0.6962, Train: 0.4941, Test: 0.5853\n",
      "Epoch: 415, Loss: 0.6962, Train: 0.4940, Test: 0.5853\n",
      "Epoch: 416, Loss: 0.6962, Train: 0.4940, Test: 0.5853\n",
      "Epoch: 417, Loss: 0.6962, Train: 0.4939, Test: 0.5853\n",
      "Epoch: 418, Loss: 0.6962, Train: 0.4939, Test: 0.5854\n",
      "Epoch: 419, Loss: 0.6961, Train: 0.4938, Test: 0.5854\n",
      "Epoch: 420, Loss: 0.6962, Train: 0.4938, Test: 0.5854\n",
      "Epoch: 421, Loss: 0.6961, Train: 0.4937, Test: 0.5854\n",
      "Epoch: 422, Loss: 0.6961, Train: 0.4937, Test: 0.5854\n",
      "Epoch: 423, Loss: 0.6961, Train: 0.4936, Test: 0.5854\n",
      "Epoch: 424, Loss: 0.6961, Train: 0.4936, Test: 0.5855\n",
      "Epoch: 425, Loss: 0.6961, Train: 0.4935, Test: 0.5855\n",
      "Epoch: 426, Loss: 0.6961, Train: 0.4934, Test: 0.5855\n",
      "Epoch: 427, Loss: 0.6961, Train: 0.4934, Test: 0.5855\n",
      "Epoch: 428, Loss: 0.6961, Train: 0.4933, Test: 0.5855\n",
      "Epoch: 429, Loss: 0.6961, Train: 0.4933, Test: 0.5855\n",
      "Epoch: 430, Loss: 0.6961, Train: 0.4932, Test: 0.5855\n",
      "Epoch: 431, Loss: 0.6961, Train: 0.4932, Test: 0.5856\n",
      "Epoch: 432, Loss: 0.6961, Train: 0.4931, Test: 0.5856\n",
      "Epoch: 433, Loss: 0.6960, Train: 0.4931, Test: 0.5856\n",
      "Epoch: 434, Loss: 0.6960, Train: 0.4930, Test: 0.5856\n",
      "Epoch: 435, Loss: 0.6960, Train: 0.4930, Test: 0.5856\n",
      "Epoch: 436, Loss: 0.6960, Train: 0.4929, Test: 0.5856\n",
      "Epoch: 437, Loss: 0.6960, Train: 0.4929, Test: 0.5856\n",
      "Epoch: 438, Loss: 0.6960, Train: 0.4928, Test: 0.5857\n",
      "Epoch: 439, Loss: 0.6960, Train: 0.4928, Test: 0.5857\n",
      "Epoch: 440, Loss: 0.6960, Train: 0.4927, Test: 0.5857\n",
      "Epoch: 441, Loss: 0.6960, Train: 0.4927, Test: 0.5857\n",
      "Epoch: 442, Loss: 0.6960, Train: 0.4927, Test: 0.5857\n",
      "Epoch: 443, Loss: 0.6960, Train: 0.4926, Test: 0.5857\n",
      "Epoch: 444, Loss: 0.6960, Train: 0.4926, Test: 0.5857\n",
      "Epoch: 445, Loss: 0.6960, Train: 0.4925, Test: 0.5857\n",
      "Epoch: 446, Loss: 0.6960, Train: 0.4925, Test: 0.5858\n",
      "Epoch: 447, Loss: 0.6960, Train: 0.4924, Test: 0.5858\n",
      "Epoch: 448, Loss: 0.6959, Train: 0.4924, Test: 0.5858\n",
      "Epoch: 449, Loss: 0.6959, Train: 0.4923, Test: 0.5858\n",
      "Epoch: 450, Loss: 0.6959, Train: 0.4923, Test: 0.5858\n",
      "Epoch: 451, Loss: 0.6959, Train: 0.4922, Test: 0.5858\n",
      "Epoch: 452, Loss: 0.6959, Train: 0.4922, Test: 0.5858\n",
      "Epoch: 453, Loss: 0.6959, Train: 0.4921, Test: 0.5858\n",
      "Epoch: 454, Loss: 0.6959, Train: 0.4921, Test: 0.5859\n",
      "Epoch: 455, Loss: 0.6959, Train: 0.4920, Test: 0.5859\n",
      "Epoch: 456, Loss: 0.6959, Train: 0.4920, Test: 0.5859\n",
      "Epoch: 457, Loss: 0.6959, Train: 0.4920, Test: 0.5859\n",
      "Epoch: 458, Loss: 0.6959, Train: 0.4919, Test: 0.5859\n",
      "Epoch: 459, Loss: 0.6959, Train: 0.4919, Test: 0.5859\n",
      "Epoch: 460, Loss: 0.6959, Train: 0.4918, Test: 0.5859\n",
      "Epoch: 461, Loss: 0.6959, Train: 0.4918, Test: 0.5859\n",
      "Epoch: 462, Loss: 0.6959, Train: 0.4917, Test: 0.5860\n",
      "Epoch: 463, Loss: 0.6959, Train: 0.4917, Test: 0.5860\n",
      "Epoch: 464, Loss: 0.6959, Train: 0.4916, Test: 0.5860\n",
      "Epoch: 465, Loss: 0.6958, Train: 0.4916, Test: 0.5860\n",
      "Epoch: 466, Loss: 0.6958, Train: 0.4915, Test: 0.5860\n",
      "Epoch: 467, Loss: 0.6958, Train: 0.4915, Test: 0.5860\n",
      "Epoch: 468, Loss: 0.6958, Train: 0.4914, Test: 0.5860\n",
      "Epoch: 469, Loss: 0.6958, Train: 0.4914, Test: 0.5860\n",
      "Epoch: 470, Loss: 0.6958, Train: 0.4914, Test: 0.5861\n",
      "Epoch: 471, Loss: 0.6958, Train: 0.4913, Test: 0.5861\n",
      "Epoch: 472, Loss: 0.6958, Train: 0.4913, Test: 0.5861\n",
      "Epoch: 473, Loss: 0.6958, Train: 0.4912, Test: 0.5861\n",
      "Epoch: 474, Loss: 0.6958, Train: 0.4912, Test: 0.5861\n",
      "Epoch: 475, Loss: 0.6958, Train: 0.4911, Test: 0.5861\n",
      "Epoch: 476, Loss: 0.6958, Train: 0.4911, Test: 0.5861\n",
      "Epoch: 477, Loss: 0.6958, Train: 0.4911, Test: 0.5861\n",
      "Epoch: 478, Loss: 0.6958, Train: 0.4910, Test: 0.5861\n",
      "Epoch: 479, Loss: 0.6958, Train: 0.4910, Test: 0.5861\n",
      "Epoch: 480, Loss: 0.6958, Train: 0.4909, Test: 0.5862\n",
      "Epoch: 481, Loss: 0.6957, Train: 0.4909, Test: 0.5862\n",
      "Epoch: 482, Loss: 0.6957, Train: 0.4909, Test: 0.5862\n",
      "Epoch: 483, Loss: 0.6957, Train: 0.4908, Test: 0.5862\n",
      "Epoch: 484, Loss: 0.6957, Train: 0.4908, Test: 0.5862\n",
      "Epoch: 485, Loss: 0.6957, Train: 0.4907, Test: 0.5862\n",
      "Epoch: 486, Loss: 0.6957, Train: 0.4907, Test: 0.5862\n",
      "Epoch: 487, Loss: 0.6957, Train: 0.4907, Test: 0.5862\n",
      "Epoch: 488, Loss: 0.6957, Train: 0.4906, Test: 0.5862\n",
      "Epoch: 489, Loss: 0.6957, Train: 0.4906, Test: 0.5862\n",
      "Epoch: 490, Loss: 0.6957, Train: 0.4905, Test: 0.5863\n",
      "Epoch: 491, Loss: 0.6957, Train: 0.4905, Test: 0.5863\n",
      "Epoch: 492, Loss: 0.6957, Train: 0.4904, Test: 0.5863\n",
      "Epoch: 493, Loss: 0.6957, Train: 0.4904, Test: 0.5863\n",
      "Epoch: 494, Loss: 0.6957, Train: 0.4904, Test: 0.5863\n",
      "Epoch: 495, Loss: 0.6957, Train: 0.4903, Test: 0.5863\n",
      "Epoch: 496, Loss: 0.6957, Train: 0.4903, Test: 0.5863\n",
      "Epoch: 497, Loss: 0.6957, Train: 0.4902, Test: 0.5863\n",
      "Epoch: 498, Loss: 0.6957, Train: 0.4902, Test: 0.5863\n",
      "Epoch: 499, Loss: 0.6956, Train: 0.4902, Test: 0.5863\n",
      "Epoch: 500, Loss: 0.6956, Train: 0.4901, Test: 0.5863\n",
      "Best Test: 0.5984\n"
     ]
    }
   ],
   "source": [
    "print(train_data.x.tolist()[:20])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data = train_data.to(device)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # heads = 8\n",
    "        # droupout?, concat?\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "num_features = 2\n",
    "hidden_features = 8\n",
    "model = Net(num_features, hidden_features, num_features).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=.005)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "losses = []\n",
    "best_test_auc = 0\n",
    "for epoch in range(1, 501):\n",
    "    loss = train()\n",
    "    #val_auc = test(val_data)\n",
    "    train_auc = test(train_data)\n",
    "    test_auc = test(test_data)\n",
    "    if test_auc > best_test_auc:\n",
    "        best_test_auc = test_auc\n",
    "    # print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "    #       f'Test: {test_auc:.4f}')\n",
    "    losses.append(loss)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_auc:.4f}, Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Best Test: {best_test_auc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64584fae-de30-418f-a7f8-f9b637fd724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 427.9030, Train: 0.4990, Test: 0.5312\n",
      "Epoch: 002, Loss: 390.4045, Train: 0.5002, Test: 0.5333\n",
      "Epoch: 003, Loss: 361.9710, Train: 0.5008, Test: 0.5345\n",
      "Epoch: 004, Loss: 331.7034, Train: 0.5014, Test: 0.5365\n",
      "Epoch: 005, Loss: 309.8726, Train: 0.5025, Test: 0.5385\n",
      "Epoch: 006, Loss: 283.4200, Train: 0.5032, Test: 0.5405\n",
      "Epoch: 007, Loss: 239.7411, Train: 0.5019, Test: 0.5412\n",
      "Epoch: 008, Loss: 204.8929, Train: 0.5023, Test: 0.5426\n",
      "Epoch: 009, Loss: 184.6708, Train: 0.5029, Test: 0.5442\n",
      "Epoch: 010, Loss: 164.5259, Train: 0.5017, Test: 0.5450\n",
      "Epoch: 011, Loss: 154.6784, Train: 0.5028, Test: 0.5448\n",
      "Epoch: 012, Loss: 145.3862, Train: 0.5011, Test: 0.5450\n",
      "Epoch: 013, Loss: 136.8086, Train: 0.5009, Test: 0.5449\n",
      "Epoch: 014, Loss: 129.5165, Train: 0.5013, Test: 0.5453\n",
      "Epoch: 015, Loss: 121.9577, Train: 0.5003, Test: 0.5470\n",
      "Epoch: 016, Loss: 104.2763, Train: 0.5006, Test: 0.5484\n",
      "Epoch: 017, Loss: 74.0545, Train: 0.5001, Test: 0.5491\n",
      "Epoch: 018, Loss: 68.0525, Train: 0.4995, Test: 0.5485\n",
      "Epoch: 019, Loss: 64.1043, Train: 0.4996, Test: 0.5492\n",
      "Epoch: 020, Loss: 60.9146, Train: 0.5002, Test: 0.5508\n",
      "Epoch: 021, Loss: 57.4505, Train: 0.5008, Test: 0.5512\n",
      "Epoch: 022, Loss: 53.3503, Train: 0.5016, Test: 0.5521\n",
      "Epoch: 023, Loss: 48.7405, Train: 0.5012, Test: 0.5522\n",
      "Epoch: 024, Loss: 44.1622, Train: 0.5024, Test: 0.5522\n",
      "Epoch: 025, Loss: 40.7268, Train: 0.5041, Test: 0.5514\n",
      "Epoch: 026, Loss: 38.8948, Train: 0.5053, Test: 0.5521\n",
      "Epoch: 027, Loss: 37.4906, Train: 0.5063, Test: 0.5518\n",
      "Epoch: 028, Loss: 36.1900, Train: 0.5069, Test: 0.5514\n",
      "Epoch: 029, Loss: 34.9887, Train: 0.5076, Test: 0.5503\n",
      "Epoch: 030, Loss: 33.8140, Train: 0.5083, Test: 0.5501\n",
      "Epoch: 031, Loss: 32.7020, Train: 0.5086, Test: 0.5495\n",
      "Epoch: 032, Loss: 31.6345, Train: 0.5092, Test: 0.5496\n",
      "Epoch: 033, Loss: 30.6317, Train: 0.5102, Test: 0.5503\n",
      "Epoch: 034, Loss: 29.6324, Train: 0.5114, Test: 0.5512\n",
      "Epoch: 035, Loss: 28.6758, Train: 0.5122, Test: 0.5519\n",
      "Epoch: 036, Loss: 27.7804, Train: 0.5132, Test: 0.5526\n",
      "Epoch: 037, Loss: 26.9286, Train: 0.5138, Test: 0.5537\n",
      "Epoch: 038, Loss: 26.1455, Train: 0.5145, Test: 0.5544\n",
      "Epoch: 039, Loss: 25.3984, Train: 0.5149, Test: 0.5550\n",
      "Epoch: 040, Loss: 24.6837, Train: 0.5155, Test: 0.5556\n",
      "Epoch: 041, Loss: 23.9957, Train: 0.5158, Test: 0.5563\n",
      "Epoch: 042, Loss: 23.3278, Train: 0.5163, Test: 0.5568\n",
      "Epoch: 043, Loss: 22.6826, Train: 0.5165, Test: 0.5574\n",
      "Epoch: 044, Loss: 22.0623, Train: 0.5167, Test: 0.5579\n",
      "Epoch: 045, Loss: 21.4582, Train: 0.5170, Test: 0.5583\n",
      "Epoch: 046, Loss: 20.8630, Train: 0.5172, Test: 0.5585\n",
      "Epoch: 047, Loss: 20.2747, Train: 0.5175, Test: 0.5587\n",
      "Epoch: 048, Loss: 19.6989, Train: 0.5177, Test: 0.5589\n",
      "Epoch: 049, Loss: 19.1353, Train: 0.5175, Test: 0.5591\n",
      "Epoch: 050, Loss: 18.5519, Train: 0.5177, Test: 0.5593\n",
      "Epoch: 051, Loss: 17.9866, Train: 0.5179, Test: 0.5597\n",
      "Epoch: 052, Loss: 17.4107, Train: 0.5182, Test: 0.5600\n",
      "Epoch: 053, Loss: 16.8027, Train: 0.5186, Test: 0.5603\n",
      "Epoch: 054, Loss: 16.1988, Train: 0.5189, Test: 0.5604\n",
      "Epoch: 055, Loss: 15.5469, Train: 0.5192, Test: 0.5605\n",
      "Epoch: 056, Loss: 14.8780, Train: 0.5196, Test: 0.5607\n",
      "Epoch: 057, Loss: 14.1519, Train: 0.5201, Test: 0.5608\n",
      "Epoch: 058, Loss: 13.3800, Train: 0.5207, Test: 0.5608\n",
      "Epoch: 059, Loss: 12.5815, Train: 0.5217, Test: 0.5610\n",
      "Epoch: 060, Loss: 11.7814, Train: 0.5236, Test: 0.5611\n",
      "Epoch: 061, Loss: 10.9983, Train: 0.5247, Test: 0.5612\n",
      "Epoch: 062, Loss: 10.4772, Train: 0.5252, Test: 0.5613\n",
      "Epoch: 063, Loss: 10.0678, Train: 0.5259, Test: 0.5612\n",
      "Epoch: 064, Loss: 9.7275, Train: 0.5268, Test: 0.5612\n",
      "Epoch: 065, Loss: 9.4347, Train: 0.5275, Test: 0.5613\n",
      "Epoch: 066, Loss: 9.2006, Train: 0.5279, Test: 0.5614\n",
      "Epoch: 067, Loss: 8.9997, Train: 0.5282, Test: 0.5615\n",
      "Epoch: 068, Loss: 8.8010, Train: 0.5283, Test: 0.5615\n",
      "Epoch: 069, Loss: 8.6195, Train: 0.5283, Test: 0.5616\n",
      "Epoch: 070, Loss: 8.4524, Train: 0.5281, Test: 0.5616\n",
      "Epoch: 071, Loss: 8.2800, Train: 0.5281, Test: 0.5617\n",
      "Epoch: 072, Loss: 8.1235, Train: 0.5280, Test: 0.5617\n",
      "Epoch: 073, Loss: 7.9699, Train: 0.5279, Test: 0.5616\n",
      "Epoch: 074, Loss: 7.8135, Train: 0.5278, Test: 0.5617\n",
      "Epoch: 075, Loss: 7.6706, Train: 0.5277, Test: 0.5617\n",
      "Epoch: 076, Loss: 7.5310, Train: 0.5276, Test: 0.5617\n",
      "Epoch: 077, Loss: 7.3865, Train: 0.5275, Test: 0.5618\n",
      "Epoch: 078, Loss: 7.2510, Train: 0.5274, Test: 0.5618\n",
      "Epoch: 079, Loss: 7.1158, Train: 0.5272, Test: 0.5618\n",
      "Epoch: 080, Loss: 6.9917, Train: 0.5271, Test: 0.5618\n",
      "Epoch: 081, Loss: 6.8567, Train: 0.5270, Test: 0.5619\n",
      "Epoch: 082, Loss: 6.7407, Train: 0.5268, Test: 0.5619\n",
      "Epoch: 083, Loss: 6.6202, Train: 0.5267, Test: 0.5619\n",
      "Epoch: 084, Loss: 6.5046, Train: 0.5265, Test: 0.5620\n",
      "Epoch: 085, Loss: 6.3866, Train: 0.5264, Test: 0.5620\n",
      "Epoch: 086, Loss: 6.2736, Train: 0.5262, Test: 0.5620\n",
      "Epoch: 087, Loss: 6.1652, Train: 0.5261, Test: 0.5620\n",
      "Epoch: 088, Loss: 6.0606, Train: 0.5259, Test: 0.5620\n",
      "Epoch: 089, Loss: 5.9541, Train: 0.5257, Test: 0.5620\n",
      "Epoch: 090, Loss: 5.8551, Train: 0.5255, Test: 0.5620\n",
      "Epoch: 091, Loss: 5.7590, Train: 0.5253, Test: 0.5620\n",
      "Epoch: 092, Loss: 5.6625, Train: 0.5251, Test: 0.5619\n",
      "Epoch: 093, Loss: 5.5683, Train: 0.5250, Test: 0.5620\n",
      "Epoch: 094, Loss: 5.4757, Train: 0.5248, Test: 0.5619\n",
      "Epoch: 095, Loss: 5.3892, Train: 0.5246, Test: 0.5619\n",
      "Epoch: 096, Loss: 5.3032, Train: 0.5245, Test: 0.5619\n",
      "Epoch: 097, Loss: 5.2180, Train: 0.5243, Test: 0.5619\n",
      "Epoch: 098, Loss: 5.1335, Train: 0.5241, Test: 0.5620\n",
      "Epoch: 099, Loss: 5.0525, Train: 0.5238, Test: 0.5620\n",
      "Epoch: 100, Loss: 4.9742, Train: 0.5236, Test: 0.5620\n",
      "Epoch: 101, Loss: 4.8957, Train: 0.5234, Test: 0.5620\n",
      "Epoch: 102, Loss: 4.8187, Train: 0.5232, Test: 0.5619\n",
      "Epoch: 103, Loss: 4.7477, Train: 0.5229, Test: 0.5620\n",
      "Epoch: 104, Loss: 4.6727, Train: 0.5227, Test: 0.5619\n",
      "Epoch: 105, Loss: 4.6040, Train: 0.5225, Test: 0.5619\n",
      "Epoch: 106, Loss: 4.5274, Train: 0.5222, Test: 0.5619\n",
      "Epoch: 107, Loss: 4.4626, Train: 0.5220, Test: 0.5620\n",
      "Epoch: 108, Loss: 4.3953, Train: 0.5218, Test: 0.5620\n",
      "Epoch: 109, Loss: 4.3296, Train: 0.5215, Test: 0.5620\n",
      "Epoch: 110, Loss: 4.2661, Train: 0.5213, Test: 0.5620\n",
      "Epoch: 111, Loss: 4.2033, Train: 0.5210, Test: 0.5620\n",
      "Epoch: 112, Loss: 4.1373, Train: 0.5208, Test: 0.5620\n",
      "Epoch: 113, Loss: 4.0763, Train: 0.5205, Test: 0.5621\n",
      "Epoch: 114, Loss: 4.0142, Train: 0.5202, Test: 0.5621\n",
      "Epoch: 115, Loss: 3.9543, Train: 0.5200, Test: 0.5622\n",
      "Epoch: 116, Loss: 3.8960, Train: 0.5197, Test: 0.5623\n",
      "Epoch: 117, Loss: 3.8386, Train: 0.5194, Test: 0.5623\n",
      "Epoch: 118, Loss: 3.7858, Train: 0.5191, Test: 0.5624\n",
      "Epoch: 119, Loss: 3.7292, Train: 0.5188, Test: 0.5625\n",
      "Epoch: 120, Loss: 3.6757, Train: 0.5185, Test: 0.5626\n",
      "Epoch: 121, Loss: 3.6202, Train: 0.5182, Test: 0.5626\n",
      "Epoch: 122, Loss: 3.5679, Train: 0.5179, Test: 0.5627\n",
      "Epoch: 123, Loss: 3.5153, Train: 0.5176, Test: 0.5628\n",
      "Epoch: 124, Loss: 3.4669, Train: 0.5172, Test: 0.5629\n",
      "Epoch: 125, Loss: 3.4162, Train: 0.5169, Test: 0.5629\n",
      "Epoch: 126, Loss: 3.3689, Train: 0.5165, Test: 0.5630\n",
      "Epoch: 127, Loss: 3.3184, Train: 0.5162, Test: 0.5631\n",
      "Epoch: 128, Loss: 3.2706, Train: 0.5159, Test: 0.5632\n",
      "Epoch: 129, Loss: 3.2277, Train: 0.5155, Test: 0.5633\n",
      "Epoch: 130, Loss: 3.1810, Train: 0.5152, Test: 0.5634\n",
      "Epoch: 131, Loss: 3.1368, Train: 0.5149, Test: 0.5634\n",
      "Epoch: 132, Loss: 3.0941, Train: 0.5145, Test: 0.5635\n",
      "Epoch: 133, Loss: 3.0495, Train: 0.5141, Test: 0.5636\n",
      "Epoch: 134, Loss: 3.0083, Train: 0.5138, Test: 0.5636\n",
      "Epoch: 135, Loss: 2.9661, Train: 0.5134, Test: 0.5637\n",
      "Epoch: 136, Loss: 2.9259, Train: 0.5130, Test: 0.5637\n",
      "Epoch: 137, Loss: 2.8886, Train: 0.5126, Test: 0.5638\n",
      "Epoch: 138, Loss: 2.8475, Train: 0.5122, Test: 0.5639\n",
      "Epoch: 139, Loss: 2.8099, Train: 0.5118, Test: 0.5640\n",
      "Epoch: 140, Loss: 2.7724, Train: 0.5114, Test: 0.5640\n",
      "Epoch: 141, Loss: 2.7346, Train: 0.5109, Test: 0.5641\n",
      "Epoch: 142, Loss: 2.7010, Train: 0.5105, Test: 0.5642\n",
      "Epoch: 143, Loss: 2.6650, Train: 0.5100, Test: 0.5643\n",
      "Epoch: 144, Loss: 2.6301, Train: 0.5095, Test: 0.5643\n",
      "Epoch: 145, Loss: 2.5967, Train: 0.5091, Test: 0.5644\n",
      "Epoch: 146, Loss: 2.5647, Train: 0.5086, Test: 0.5645\n",
      "Epoch: 147, Loss: 2.5323, Train: 0.5082, Test: 0.5646\n",
      "Epoch: 148, Loss: 2.5006, Train: 0.5078, Test: 0.5646\n",
      "Epoch: 149, Loss: 2.4681, Train: 0.5074, Test: 0.5647\n",
      "Epoch: 150, Loss: 2.4371, Train: 0.5070, Test: 0.5647\n",
      "Epoch: 151, Loss: 2.4072, Train: 0.5067, Test: 0.5648\n",
      "Epoch: 152, Loss: 2.3787, Train: 0.5064, Test: 0.5649\n",
      "Epoch: 153, Loss: 2.3500, Train: 0.5060, Test: 0.5650\n",
      "Epoch: 154, Loss: 2.3209, Train: 0.5057, Test: 0.5650\n",
      "Epoch: 155, Loss: 2.2945, Train: 0.5054, Test: 0.5651\n",
      "Epoch: 156, Loss: 2.2665, Train: 0.5050, Test: 0.5652\n",
      "Epoch: 157, Loss: 2.2403, Train: 0.5047, Test: 0.5653\n",
      "Epoch: 158, Loss: 2.2149, Train: 0.5044, Test: 0.5653\n",
      "Epoch: 159, Loss: 2.1893, Train: 0.5041, Test: 0.5654\n",
      "Epoch: 160, Loss: 2.1626, Train: 0.5038, Test: 0.5654\n",
      "Epoch: 161, Loss: 2.1405, Train: 0.5035, Test: 0.5655\n",
      "Epoch: 162, Loss: 2.1156, Train: 0.5032, Test: 0.5655\n",
      "Epoch: 163, Loss: 2.0936, Train: 0.5030, Test: 0.5656\n",
      "Epoch: 164, Loss: 2.0698, Train: 0.5027, Test: 0.5656\n",
      "Epoch: 165, Loss: 2.0462, Train: 0.5025, Test: 0.5657\n",
      "Epoch: 166, Loss: 2.0252, Train: 0.5022, Test: 0.5658\n",
      "Epoch: 167, Loss: 2.0031, Train: 0.5020, Test: 0.5658\n",
      "Epoch: 168, Loss: 1.9821, Train: 0.5018, Test: 0.5659\n",
      "Epoch: 169, Loss: 1.9607, Train: 0.5016, Test: 0.5659\n",
      "Epoch: 170, Loss: 1.9409, Train: 0.5013, Test: 0.5659\n",
      "Epoch: 171, Loss: 1.9197, Train: 0.5011, Test: 0.5660\n",
      "Epoch: 172, Loss: 1.9007, Train: 0.5009, Test: 0.5660\n",
      "Epoch: 173, Loss: 1.8798, Train: 0.5007, Test: 0.5661\n",
      "Epoch: 174, Loss: 1.8628, Train: 0.5004, Test: 0.5661\n",
      "Epoch: 175, Loss: 1.8443, Train: 0.5002, Test: 0.5661\n",
      "Epoch: 176, Loss: 1.8263, Train: 0.5000, Test: 0.5662\n",
      "Epoch: 177, Loss: 1.8073, Train: 0.4997, Test: 0.5662\n",
      "Epoch: 178, Loss: 1.7907, Train: 0.4995, Test: 0.5663\n",
      "Epoch: 179, Loss: 1.7735, Train: 0.4992, Test: 0.5663\n",
      "Epoch: 180, Loss: 1.7571, Train: 0.4989, Test: 0.5663\n",
      "Epoch: 181, Loss: 1.7401, Train: 0.4987, Test: 0.5663\n",
      "Epoch: 182, Loss: 1.7248, Train: 0.4985, Test: 0.5664\n",
      "Epoch: 183, Loss: 1.7080, Train: 0.4982, Test: 0.5664\n",
      "Epoch: 184, Loss: 1.6918, Train: 0.4980, Test: 0.5664\n",
      "Epoch: 185, Loss: 1.6783, Train: 0.4977, Test: 0.5664\n",
      "Epoch: 186, Loss: 1.6617, Train: 0.4974, Test: 0.5663\n",
      "Epoch: 187, Loss: 1.6481, Train: 0.4970, Test: 0.5663\n",
      "Epoch: 188, Loss: 1.6335, Train: 0.4967, Test: 0.5663\n",
      "Epoch: 189, Loss: 1.6191, Train: 0.4963, Test: 0.5662\n",
      "Epoch: 190, Loss: 1.6053, Train: 0.4960, Test: 0.5662\n",
      "Epoch: 191, Loss: 1.5919, Train: 0.4957, Test: 0.5661\n",
      "Epoch: 192, Loss: 1.5786, Train: 0.4953, Test: 0.5661\n",
      "Epoch: 193, Loss: 1.5659, Train: 0.4950, Test: 0.5660\n",
      "Epoch: 194, Loss: 1.5524, Train: 0.4946, Test: 0.5660\n",
      "Epoch: 195, Loss: 1.5401, Train: 0.4943, Test: 0.5659\n",
      "Epoch: 196, Loss: 1.5272, Train: 0.4940, Test: 0.5659\n",
      "Epoch: 197, Loss: 1.5150, Train: 0.4937, Test: 0.5658\n",
      "Epoch: 198, Loss: 1.5032, Train: 0.4934, Test: 0.5657\n",
      "Epoch: 199, Loss: 1.4916, Train: 0.4931, Test: 0.5657\n",
      "Epoch: 200, Loss: 1.4802, Train: 0.4929, Test: 0.5656\n",
      "Epoch: 201, Loss: 1.4690, Train: 0.4927, Test: 0.5655\n",
      "Epoch: 202, Loss: 1.4573, Train: 0.4925, Test: 0.5655\n",
      "Epoch: 203, Loss: 1.4464, Train: 0.4922, Test: 0.5654\n",
      "Epoch: 204, Loss: 1.4355, Train: 0.4919, Test: 0.5654\n",
      "Epoch: 205, Loss: 1.4243, Train: 0.4916, Test: 0.5654\n",
      "Epoch: 206, Loss: 1.4153, Train: 0.4913, Test: 0.5653\n",
      "Epoch: 207, Loss: 1.4044, Train: 0.4910, Test: 0.5652\n",
      "Epoch: 208, Loss: 1.3941, Train: 0.4906, Test: 0.5651\n",
      "Epoch: 209, Loss: 1.3844, Train: 0.4901, Test: 0.5650\n",
      "Epoch: 210, Loss: 1.3746, Train: 0.4896, Test: 0.5649\n",
      "Epoch: 211, Loss: 1.3650, Train: 0.4891, Test: 0.5649\n",
      "Epoch: 212, Loss: 1.3564, Train: 0.4885, Test: 0.5647\n",
      "Epoch: 213, Loss: 1.3473, Train: 0.4878, Test: 0.5646\n",
      "Epoch: 214, Loss: 1.3379, Train: 0.4872, Test: 0.5644\n",
      "Epoch: 215, Loss: 1.3293, Train: 0.4867, Test: 0.5643\n",
      "Epoch: 216, Loss: 1.3206, Train: 0.4862, Test: 0.5642\n",
      "Epoch: 217, Loss: 1.3113, Train: 0.4858, Test: 0.5640\n",
      "Epoch: 218, Loss: 1.3038, Train: 0.4855, Test: 0.5638\n",
      "Epoch: 219, Loss: 1.2948, Train: 0.4852, Test: 0.5636\n",
      "Epoch: 220, Loss: 1.2865, Train: 0.4849, Test: 0.5634\n",
      "Epoch: 221, Loss: 1.2784, Train: 0.4847, Test: 0.5632\n",
      "Epoch: 222, Loss: 1.2706, Train: 0.4845, Test: 0.5629\n",
      "Epoch: 223, Loss: 1.2628, Train: 0.4843, Test: 0.5627\n",
      "Epoch: 224, Loss: 1.2557, Train: 0.4842, Test: 0.5625\n",
      "Epoch: 225, Loss: 1.2480, Train: 0.4840, Test: 0.5624\n",
      "Epoch: 226, Loss: 1.2401, Train: 0.4839, Test: 0.5622\n",
      "Epoch: 227, Loss: 1.2328, Train: 0.4838, Test: 0.5619\n",
      "Epoch: 228, Loss: 1.2265, Train: 0.4836, Test: 0.5617\n",
      "Epoch: 229, Loss: 1.2189, Train: 0.4835, Test: 0.5613\n",
      "Epoch: 230, Loss: 1.2122, Train: 0.4834, Test: 0.5608\n",
      "Epoch: 231, Loss: 1.2052, Train: 0.4833, Test: 0.5605\n",
      "Epoch: 232, Loss: 1.1983, Train: 0.4832, Test: 0.5601\n",
      "Epoch: 233, Loss: 1.1913, Train: 0.4830, Test: 0.5598\n",
      "Epoch: 234, Loss: 1.1851, Train: 0.4829, Test: 0.5594\n",
      "Epoch: 235, Loss: 1.1792, Train: 0.4829, Test: 0.5591\n",
      "Epoch: 236, Loss: 1.1727, Train: 0.4828, Test: 0.5588\n",
      "Epoch: 237, Loss: 1.1663, Train: 0.4826, Test: 0.5585\n",
      "Epoch: 238, Loss: 1.1604, Train: 0.4826, Test: 0.5582\n",
      "Epoch: 239, Loss: 1.1544, Train: 0.4824, Test: 0.5578\n",
      "Epoch: 240, Loss: 1.1487, Train: 0.4823, Test: 0.5574\n",
      "Epoch: 241, Loss: 1.1426, Train: 0.4822, Test: 0.5570\n",
      "Epoch: 242, Loss: 1.1368, Train: 0.4820, Test: 0.5567\n",
      "Epoch: 243, Loss: 1.1311, Train: 0.4819, Test: 0.5563\n",
      "Epoch: 244, Loss: 1.1255, Train: 0.4818, Test: 0.5559\n",
      "Epoch: 245, Loss: 1.1203, Train: 0.4816, Test: 0.5555\n",
      "Epoch: 246, Loss: 1.1152, Train: 0.4813, Test: 0.5552\n",
      "Epoch: 247, Loss: 1.1100, Train: 0.4811, Test: 0.5548\n",
      "Epoch: 248, Loss: 1.1044, Train: 0.4809, Test: 0.5545\n",
      "Epoch: 249, Loss: 1.0990, Train: 0.4808, Test: 0.5541\n",
      "Epoch: 250, Loss: 1.0941, Train: 0.4807, Test: 0.5537\n",
      "Epoch: 251, Loss: 1.0889, Train: 0.4806, Test: 0.5532\n",
      "Epoch: 252, Loss: 1.0841, Train: 0.4804, Test: 0.5527\n",
      "Epoch: 253, Loss: 1.0787, Train: 0.4802, Test: 0.5523\n",
      "Epoch: 254, Loss: 1.0745, Train: 0.4800, Test: 0.5518\n",
      "Epoch: 255, Loss: 1.0694, Train: 0.4799, Test: 0.5513\n",
      "Epoch: 256, Loss: 1.0646, Train: 0.4798, Test: 0.5508\n",
      "Epoch: 257, Loss: 1.0603, Train: 0.4797, Test: 0.5502\n",
      "Epoch: 258, Loss: 1.0556, Train: 0.4796, Test: 0.5497\n",
      "Epoch: 259, Loss: 1.0508, Train: 0.4796, Test: 0.5492\n",
      "Epoch: 260, Loss: 1.0466, Train: 0.4796, Test: 0.5486\n",
      "Epoch: 261, Loss: 1.0422, Train: 0.4796, Test: 0.5481\n",
      "Epoch: 262, Loss: 1.0379, Train: 0.4796, Test: 0.5476\n",
      "Epoch: 263, Loss: 1.0336, Train: 0.4795, Test: 0.5470\n",
      "Epoch: 264, Loss: 1.0294, Train: 0.4795, Test: 0.5464\n",
      "Epoch: 265, Loss: 1.0254, Train: 0.4794, Test: 0.5457\n",
      "Epoch: 266, Loss: 1.0212, Train: 0.4795, Test: 0.5451\n",
      "Epoch: 267, Loss: 1.0167, Train: 0.4794, Test: 0.5444\n",
      "Epoch: 268, Loss: 1.0129, Train: 0.4793, Test: 0.5437\n",
      "Epoch: 269, Loss: 1.0087, Train: 0.4793, Test: 0.5429\n",
      "Epoch: 270, Loss: 1.0051, Train: 0.4792, Test: 0.5420\n",
      "Epoch: 271, Loss: 1.0008, Train: 0.4792, Test: 0.5413\n",
      "Epoch: 272, Loss: 0.9973, Train: 0.4790, Test: 0.5405\n",
      "Epoch: 273, Loss: 0.9929, Train: 0.4790, Test: 0.5398\n",
      "Epoch: 274, Loss: 0.9895, Train: 0.4789, Test: 0.5391\n",
      "Epoch: 275, Loss: 0.9855, Train: 0.4788, Test: 0.5383\n",
      "Epoch: 276, Loss: 0.9824, Train: 0.4786, Test: 0.5375\n",
      "Epoch: 277, Loss: 0.9782, Train: 0.4785, Test: 0.5367\n",
      "Epoch: 278, Loss: 0.9750, Train: 0.4783, Test: 0.5359\n",
      "Epoch: 279, Loss: 0.9711, Train: 0.4781, Test: 0.5350\n",
      "Epoch: 280, Loss: 0.9678, Train: 0.4781, Test: 0.5341\n",
      "Epoch: 281, Loss: 0.9645, Train: 0.4780, Test: 0.5333\n",
      "Epoch: 282, Loss: 0.9605, Train: 0.4779, Test: 0.5324\n",
      "Epoch: 283, Loss: 0.9575, Train: 0.4778, Test: 0.5315\n",
      "Epoch: 284, Loss: 0.9537, Train: 0.4777, Test: 0.5306\n",
      "Epoch: 285, Loss: 0.9503, Train: 0.4776, Test: 0.5296\n",
      "Epoch: 286, Loss: 0.9470, Train: 0.4774, Test: 0.5286\n",
      "Epoch: 287, Loss: 0.9439, Train: 0.4772, Test: 0.5275\n",
      "Epoch: 288, Loss: 0.9403, Train: 0.4771, Test: 0.5264\n",
      "Epoch: 289, Loss: 0.9371, Train: 0.4769, Test: 0.5250\n",
      "Epoch: 290, Loss: 0.9338, Train: 0.4767, Test: 0.5240\n",
      "Epoch: 291, Loss: 0.9306, Train: 0.4765, Test: 0.5230\n",
      "Epoch: 292, Loss: 0.9274, Train: 0.4762, Test: 0.5219\n",
      "Epoch: 293, Loss: 0.9243, Train: 0.4758, Test: 0.5209\n",
      "Epoch: 294, Loss: 0.9213, Train: 0.4755, Test: 0.5197\n",
      "Epoch: 295, Loss: 0.9180, Train: 0.4753, Test: 0.5185\n",
      "Epoch: 296, Loss: 0.9148, Train: 0.4752, Test: 0.5172\n",
      "Epoch: 297, Loss: 0.9117, Train: 0.4750, Test: 0.5160\n",
      "Epoch: 298, Loss: 0.9088, Train: 0.4748, Test: 0.5148\n",
      "Epoch: 299, Loss: 0.9058, Train: 0.4745, Test: 0.5135\n",
      "Epoch: 300, Loss: 0.9028, Train: 0.4741, Test: 0.5122\n",
      "Epoch: 301, Loss: 0.8997, Train: 0.4737, Test: 0.5108\n",
      "Epoch: 302, Loss: 0.8970, Train: 0.4734, Test: 0.5094\n",
      "Epoch: 303, Loss: 0.8940, Train: 0.4730, Test: 0.5080\n",
      "Epoch: 304, Loss: 0.8909, Train: 0.4727, Test: 0.5066\n",
      "Epoch: 305, Loss: 0.8879, Train: 0.4724, Test: 0.5052\n",
      "Epoch: 306, Loss: 0.8851, Train: 0.4721, Test: 0.5036\n",
      "Epoch: 307, Loss: 0.8824, Train: 0.4717, Test: 0.5021\n",
      "Epoch: 308, Loss: 0.8794, Train: 0.4714, Test: 0.5007\n",
      "Epoch: 309, Loss: 0.8766, Train: 0.4710, Test: 0.4993\n",
      "Epoch: 310, Loss: 0.8739, Train: 0.4707, Test: 0.4978\n",
      "Epoch: 311, Loss: 0.8711, Train: 0.4704, Test: 0.4963\n",
      "Epoch: 312, Loss: 0.8684, Train: 0.4701, Test: 0.4949\n",
      "Epoch: 313, Loss: 0.8656, Train: 0.4698, Test: 0.4934\n",
      "Epoch: 314, Loss: 0.8630, Train: 0.4694, Test: 0.4919\n",
      "Epoch: 315, Loss: 0.8603, Train: 0.4691, Test: 0.4905\n",
      "Epoch: 316, Loss: 0.8576, Train: 0.4687, Test: 0.4892\n",
      "Epoch: 317, Loss: 0.8550, Train: 0.4684, Test: 0.4877\n",
      "Epoch: 318, Loss: 0.8523, Train: 0.4680, Test: 0.4863\n",
      "Epoch: 319, Loss: 0.8500, Train: 0.4677, Test: 0.4849\n",
      "Epoch: 320, Loss: 0.8471, Train: 0.4673, Test: 0.4832\n",
      "Epoch: 321, Loss: 0.8446, Train: 0.4669, Test: 0.4813\n",
      "Epoch: 322, Loss: 0.8420, Train: 0.4667, Test: 0.4793\n",
      "Epoch: 323, Loss: 0.8396, Train: 0.4663, Test: 0.4779\n",
      "Epoch: 324, Loss: 0.8369, Train: 0.4660, Test: 0.4765\n",
      "Epoch: 325, Loss: 0.8347, Train: 0.4657, Test: 0.4752\n",
      "Epoch: 326, Loss: 0.8321, Train: 0.4654, Test: 0.4739\n",
      "Epoch: 327, Loss: 0.8297, Train: 0.4651, Test: 0.4725\n",
      "Epoch: 328, Loss: 0.8274, Train: 0.4649, Test: 0.4711\n",
      "Epoch: 329, Loss: 0.8250, Train: 0.4647, Test: 0.4697\n",
      "Epoch: 330, Loss: 0.8224, Train: 0.4644, Test: 0.4685\n",
      "Epoch: 331, Loss: 0.8201, Train: 0.4643, Test: 0.4673\n",
      "Epoch: 332, Loss: 0.8179, Train: 0.4642, Test: 0.4659\n",
      "Epoch: 333, Loss: 0.8156, Train: 0.4641, Test: 0.4644\n",
      "Epoch: 334, Loss: 0.8133, Train: 0.4640, Test: 0.4630\n",
      "Epoch: 335, Loss: 0.8111, Train: 0.4640, Test: 0.4615\n",
      "Epoch: 336, Loss: 0.8089, Train: 0.4634, Test: 0.4597\n",
      "Epoch: 337, Loss: 0.8066, Train: 0.4627, Test: 0.4582\n",
      "Epoch: 338, Loss: 0.8045, Train: 0.4627, Test: 0.4566\n",
      "Epoch: 339, Loss: 0.8024, Train: 0.4627, Test: 0.4550\n",
      "Epoch: 340, Loss: 0.8001, Train: 0.4628, Test: 0.4532\n",
      "Epoch: 341, Loss: 0.7980, Train: 0.4630, Test: 0.4516\n",
      "Epoch: 342, Loss: 0.7961, Train: 0.4632, Test: 0.4504\n",
      "Epoch: 343, Loss: 0.7941, Train: 0.4633, Test: 0.4489\n",
      "Epoch: 344, Loss: 0.7921, Train: 0.4635, Test: 0.4475\n",
      "Epoch: 345, Loss: 0.7901, Train: 0.4637, Test: 0.4461\n",
      "Epoch: 346, Loss: 0.7880, Train: 0.4640, Test: 0.4448\n",
      "Epoch: 347, Loss: 0.7862, Train: 0.4643, Test: 0.4435\n",
      "Epoch: 348, Loss: 0.7842, Train: 0.4646, Test: 0.4423\n",
      "Epoch: 349, Loss: 0.7825, Train: 0.4649, Test: 0.4410\n",
      "Epoch: 350, Loss: 0.7804, Train: 0.4651, Test: 0.4395\n",
      "Epoch: 351, Loss: 0.7787, Train: 0.4653, Test: 0.4380\n",
      "Epoch: 352, Loss: 0.7770, Train: 0.4654, Test: 0.4366\n",
      "Epoch: 353, Loss: 0.7752, Train: 0.4657, Test: 0.4353\n",
      "Epoch: 354, Loss: 0.7735, Train: 0.4660, Test: 0.4342\n",
      "Epoch: 355, Loss: 0.7717, Train: 0.4664, Test: 0.4330\n",
      "Epoch: 356, Loss: 0.7700, Train: 0.4667, Test: 0.4319\n",
      "Epoch: 357, Loss: 0.7685, Train: 0.4670, Test: 0.4308\n",
      "Epoch: 358, Loss: 0.7668, Train: 0.4674, Test: 0.4297\n",
      "Epoch: 359, Loss: 0.7652, Train: 0.4677, Test: 0.4287\n",
      "Epoch: 360, Loss: 0.7637, Train: 0.4679, Test: 0.4277\n",
      "Epoch: 361, Loss: 0.7621, Train: 0.4682, Test: 0.4268\n",
      "Epoch: 362, Loss: 0.7608, Train: 0.4685, Test: 0.4258\n",
      "Epoch: 363, Loss: 0.7591, Train: 0.4688, Test: 0.4248\n",
      "Epoch: 364, Loss: 0.7578, Train: 0.4690, Test: 0.4238\n",
      "Epoch: 365, Loss: 0.7563, Train: 0.4693, Test: 0.4229\n",
      "Epoch: 366, Loss: 0.7551, Train: 0.4696, Test: 0.4218\n",
      "Epoch: 367, Loss: 0.7537, Train: 0.4699, Test: 0.4206\n",
      "Epoch: 368, Loss: 0.7523, Train: 0.4701, Test: 0.4191\n",
      "Epoch: 369, Loss: 0.7511, Train: 0.4704, Test: 0.4182\n",
      "Epoch: 370, Loss: 0.7499, Train: 0.4707, Test: 0.4175\n",
      "Epoch: 371, Loss: 0.7487, Train: 0.4709, Test: 0.4168\n",
      "Epoch: 372, Loss: 0.7474, Train: 0.4711, Test: 0.4161\n",
      "Epoch: 373, Loss: 0.7462, Train: 0.4714, Test: 0.4155\n",
      "Epoch: 374, Loss: 0.7450, Train: 0.4717, Test: 0.4149\n",
      "Epoch: 375, Loss: 0.7440, Train: 0.4719, Test: 0.4143\n",
      "Epoch: 376, Loss: 0.7429, Train: 0.4721, Test: 0.4138\n",
      "Epoch: 377, Loss: 0.7417, Train: 0.4723, Test: 0.4132\n",
      "Epoch: 378, Loss: 0.7406, Train: 0.4726, Test: 0.4127\n",
      "Epoch: 379, Loss: 0.7397, Train: 0.4728, Test: 0.4122\n",
      "Epoch: 380, Loss: 0.7387, Train: 0.4730, Test: 0.4118\n",
      "Epoch: 381, Loss: 0.7377, Train: 0.4733, Test: 0.4114\n",
      "Epoch: 382, Loss: 0.7368, Train: 0.4735, Test: 0.4109\n",
      "Epoch: 383, Loss: 0.7358, Train: 0.4737, Test: 0.4105\n",
      "Epoch: 384, Loss: 0.7349, Train: 0.4739, Test: 0.4102\n",
      "Epoch: 385, Loss: 0.7341, Train: 0.4742, Test: 0.4098\n",
      "Epoch: 386, Loss: 0.7333, Train: 0.4744, Test: 0.4094\n",
      "Epoch: 387, Loss: 0.7323, Train: 0.4746, Test: 0.4090\n",
      "Epoch: 388, Loss: 0.7315, Train: 0.4748, Test: 0.4086\n",
      "Epoch: 389, Loss: 0.7308, Train: 0.4750, Test: 0.4081\n",
      "Epoch: 390, Loss: 0.7300, Train: 0.4752, Test: 0.4077\n",
      "Epoch: 391, Loss: 0.7293, Train: 0.4755, Test: 0.4073\n",
      "Epoch: 392, Loss: 0.7286, Train: 0.4757, Test: 0.4069\n",
      "Epoch: 393, Loss: 0.7278, Train: 0.4759, Test: 0.4066\n",
      "Epoch: 394, Loss: 0.7271, Train: 0.4762, Test: 0.4062\n",
      "Epoch: 395, Loss: 0.7264, Train: 0.4763, Test: 0.4059\n",
      "Epoch: 396, Loss: 0.7258, Train: 0.4765, Test: 0.4056\n",
      "Epoch: 397, Loss: 0.7251, Train: 0.4767, Test: 0.4053\n",
      "Epoch: 398, Loss: 0.7246, Train: 0.4769, Test: 0.4050\n",
      "Epoch: 399, Loss: 0.7239, Train: 0.4770, Test: 0.4047\n",
      "Epoch: 400, Loss: 0.7233, Train: 0.4771, Test: 0.4045\n",
      "Epoch: 401, Loss: 0.7228, Train: 0.4772, Test: 0.4042\n",
      "Epoch: 402, Loss: 0.7222, Train: 0.4774, Test: 0.4039\n",
      "Epoch: 403, Loss: 0.7217, Train: 0.4775, Test: 0.4036\n",
      "Epoch: 404, Loss: 0.7211, Train: 0.4776, Test: 0.4033\n",
      "Epoch: 405, Loss: 0.7206, Train: 0.4777, Test: 0.4031\n",
      "Epoch: 406, Loss: 0.7201, Train: 0.4778, Test: 0.4028\n",
      "Epoch: 407, Loss: 0.7197, Train: 0.4780, Test: 0.4025\n",
      "Epoch: 408, Loss: 0.7192, Train: 0.4781, Test: 0.4022\n",
      "Epoch: 409, Loss: 0.7187, Train: 0.4783, Test: 0.4021\n",
      "Epoch: 410, Loss: 0.7182, Train: 0.4785, Test: 0.4019\n",
      "Epoch: 411, Loss: 0.7179, Train: 0.4786, Test: 0.4017\n",
      "Epoch: 412, Loss: 0.7174, Train: 0.4788, Test: 0.4016\n",
      "Epoch: 413, Loss: 0.7170, Train: 0.4789, Test: 0.4014\n",
      "Epoch: 414, Loss: 0.7166, Train: 0.4790, Test: 0.4013\n",
      "Epoch: 415, Loss: 0.7162, Train: 0.4792, Test: 0.4011\n",
      "Epoch: 416, Loss: 0.7158, Train: 0.4793, Test: 0.4009\n",
      "Epoch: 417, Loss: 0.7154, Train: 0.4794, Test: 0.4008\n",
      "Epoch: 418, Loss: 0.7151, Train: 0.4795, Test: 0.4007\n",
      "Epoch: 419, Loss: 0.7147, Train: 0.4796, Test: 0.4005\n",
      "Epoch: 420, Loss: 0.7144, Train: 0.4797, Test: 0.4004\n",
      "Epoch: 421, Loss: 0.7140, Train: 0.4799, Test: 0.4003\n",
      "Epoch: 422, Loss: 0.7137, Train: 0.4800, Test: 0.4001\n",
      "Epoch: 423, Loss: 0.7134, Train: 0.4801, Test: 0.4000\n",
      "Epoch: 424, Loss: 0.7132, Train: 0.4803, Test: 0.3999\n",
      "Epoch: 425, Loss: 0.7129, Train: 0.4804, Test: 0.3997\n",
      "Epoch: 426, Loss: 0.7125, Train: 0.4805, Test: 0.3996\n",
      "Epoch: 427, Loss: 0.7122, Train: 0.4806, Test: 0.3995\n",
      "Epoch: 428, Loss: 0.7120, Train: 0.4807, Test: 0.3994\n",
      "Epoch: 429, Loss: 0.7117, Train: 0.4808, Test: 0.3992\n",
      "Epoch: 430, Loss: 0.7115, Train: 0.4809, Test: 0.3991\n",
      "Epoch: 431, Loss: 0.7112, Train: 0.4810, Test: 0.3990\n",
      "Epoch: 432, Loss: 0.7110, Train: 0.4811, Test: 0.3989\n",
      "Epoch: 433, Loss: 0.7107, Train: 0.4812, Test: 0.3989\n",
      "Epoch: 434, Loss: 0.7105, Train: 0.4813, Test: 0.3988\n",
      "Epoch: 435, Loss: 0.7102, Train: 0.4814, Test: 0.3987\n",
      "Epoch: 436, Loss: 0.7100, Train: 0.4815, Test: 0.3986\n",
      "Epoch: 437, Loss: 0.7097, Train: 0.4815, Test: 0.3985\n",
      "Epoch: 438, Loss: 0.7095, Train: 0.4816, Test: 0.3984\n",
      "Epoch: 439, Loss: 0.7094, Train: 0.4817, Test: 0.3983\n",
      "Epoch: 440, Loss: 0.7092, Train: 0.4818, Test: 0.3983\n",
      "Epoch: 441, Loss: 0.7090, Train: 0.4819, Test: 0.3982\n",
      "Epoch: 442, Loss: 0.7088, Train: 0.4820, Test: 0.3981\n",
      "Epoch: 443, Loss: 0.7086, Train: 0.4821, Test: 0.3980\n",
      "Epoch: 444, Loss: 0.7084, Train: 0.4822, Test: 0.3979\n",
      "Epoch: 445, Loss: 0.7082, Train: 0.4822, Test: 0.3979\n",
      "Epoch: 446, Loss: 0.7081, Train: 0.4823, Test: 0.3978\n",
      "Epoch: 447, Loss: 0.7079, Train: 0.4824, Test: 0.3977\n",
      "Epoch: 448, Loss: 0.7077, Train: 0.4824, Test: 0.3976\n",
      "Epoch: 449, Loss: 0.7076, Train: 0.4825, Test: 0.3975\n",
      "Epoch: 450, Loss: 0.7074, Train: 0.4826, Test: 0.3974\n",
      "Epoch: 451, Loss: 0.7072, Train: 0.4826, Test: 0.3974\n",
      "Epoch: 452, Loss: 0.7070, Train: 0.4827, Test: 0.3973\n",
      "Epoch: 453, Loss: 0.7069, Train: 0.4828, Test: 0.3972\n",
      "Epoch: 454, Loss: 0.7068, Train: 0.4828, Test: 0.3972\n",
      "Epoch: 455, Loss: 0.7067, Train: 0.4829, Test: 0.3971\n",
      "Epoch: 456, Loss: 0.7065, Train: 0.4830, Test: 0.3970\n",
      "Epoch: 457, Loss: 0.7064, Train: 0.4831, Test: 0.3969\n",
      "Epoch: 458, Loss: 0.7063, Train: 0.4831, Test: 0.3969\n",
      "Epoch: 459, Loss: 0.7061, Train: 0.4832, Test: 0.3968\n",
      "Epoch: 460, Loss: 0.7060, Train: 0.4833, Test: 0.3967\n",
      "Epoch: 461, Loss: 0.7059, Train: 0.4833, Test: 0.3967\n",
      "Epoch: 462, Loss: 0.7058, Train: 0.4834, Test: 0.3966\n",
      "Epoch: 463, Loss: 0.7057, Train: 0.4835, Test: 0.3966\n",
      "Epoch: 464, Loss: 0.7055, Train: 0.4835, Test: 0.3965\n",
      "Epoch: 465, Loss: 0.7054, Train: 0.4836, Test: 0.3965\n",
      "Epoch: 466, Loss: 0.7053, Train: 0.4836, Test: 0.3965\n",
      "Epoch: 467, Loss: 0.7052, Train: 0.4837, Test: 0.3964\n",
      "Epoch: 468, Loss: 0.7051, Train: 0.4837, Test: 0.3964\n",
      "Epoch: 469, Loss: 0.7050, Train: 0.4838, Test: 0.3963\n",
      "Epoch: 470, Loss: 0.7049, Train: 0.4838, Test: 0.3963\n",
      "Epoch: 471, Loss: 0.7048, Train: 0.4838, Test: 0.3963\n",
      "Epoch: 472, Loss: 0.7047, Train: 0.4839, Test: 0.3962\n",
      "Epoch: 473, Loss: 0.7046, Train: 0.4839, Test: 0.3962\n",
      "Epoch: 474, Loss: 0.7045, Train: 0.4840, Test: 0.3962\n",
      "Epoch: 475, Loss: 0.7044, Train: 0.4840, Test: 0.3961\n",
      "Epoch: 476, Loss: 0.7043, Train: 0.4841, Test: 0.3961\n",
      "Epoch: 477, Loss: 0.7043, Train: 0.4841, Test: 0.3961\n",
      "Epoch: 478, Loss: 0.7042, Train: 0.4842, Test: 0.3960\n",
      "Epoch: 479, Loss: 0.7041, Train: 0.4842, Test: 0.3960\n",
      "Epoch: 480, Loss: 0.7040, Train: 0.4842, Test: 0.3960\n",
      "Epoch: 481, Loss: 0.7039, Train: 0.4843, Test: 0.3959\n",
      "Epoch: 482, Loss: 0.7039, Train: 0.4843, Test: 0.3959\n",
      "Epoch: 483, Loss: 0.7038, Train: 0.4844, Test: 0.3959\n",
      "Epoch: 484, Loss: 0.7037, Train: 0.4844, Test: 0.3958\n",
      "Epoch: 485, Loss: 0.7036, Train: 0.4844, Test: 0.3958\n",
      "Epoch: 486, Loss: 0.7035, Train: 0.4845, Test: 0.3958\n",
      "Epoch: 487, Loss: 0.7035, Train: 0.4845, Test: 0.3957\n",
      "Epoch: 488, Loss: 0.7034, Train: 0.4845, Test: 0.3957\n",
      "Epoch: 489, Loss: 0.7033, Train: 0.4846, Test: 0.3957\n",
      "Epoch: 490, Loss: 0.7033, Train: 0.4846, Test: 0.3956\n",
      "Epoch: 491, Loss: 0.7032, Train: 0.4846, Test: 0.3956\n",
      "Epoch: 492, Loss: 0.7031, Train: 0.4846, Test: 0.3955\n",
      "Epoch: 493, Loss: 0.7031, Train: 0.4847, Test: 0.3955\n",
      "Epoch: 494, Loss: 0.7030, Train: 0.4847, Test: 0.3955\n",
      "Epoch: 495, Loss: 0.7030, Train: 0.4847, Test: 0.3955\n",
      "Epoch: 496, Loss: 0.7029, Train: 0.4848, Test: 0.3955\n",
      "Epoch: 497, Loss: 0.7028, Train: 0.4848, Test: 0.3954\n",
      "Epoch: 498, Loss: 0.7028, Train: 0.4848, Test: 0.3954\n",
      "Epoch: 499, Loss: 0.7027, Train: 0.4849, Test: 0.3954\n",
      "Epoch: 500, Loss: 0.7027, Train: 0.4849, Test: 0.3954\n",
      "Best Test: 0.5664\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data = train_data.to(device)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels,edge_dim):\n",
    "        super().__init__()\n",
    "        # heads = 8\n",
    "        # droupout?, concat?\n",
    "        dropout = .2\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels,edge_dim=edge_dim,dropout=dropo)\n",
    "        self.conv2 = GATConv(hidden_channels, out_channels,edge_dim=edge_dim)\n",
    "\n",
    "    def encode(self, x, edge_index, edge_attr):\n",
    "        x = self.conv1(x, edge_index, edge_attr).relu()\n",
    "        return self.conv2(x, edge_index, edge_attr)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "num_features = 2\n",
    "edge_dim = 3\n",
    "hidden_features = 8\n",
    "model = Net(num_features, hidden_features, num_features,edge_dim).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=.001)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index, train_data.edge_attr)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index, data.edge_attr)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "losses = []\n",
    "best_test_auc = 0\n",
    "for epoch in range(1, 501):\n",
    "    loss = train()\n",
    "    #val_auc = test(val_data)\n",
    "    train_auc = test(train_data)\n",
    "    test_auc = test(test_data)\n",
    "    if test_auc > best_test_auc:\n",
    "        best_test_auc = test_auc\n",
    "    # print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "    #       f'Test: {test_auc:.4f}')\n",
    "    losses.append(loss)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_auc:.4f}, Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Best Test: {best_test_auc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index, test_data.edge_attr)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adbaa03-8a13-4409-976f-296e10cf0213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa1b10-4ce9-42b2-b2c3-d283d2798c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
