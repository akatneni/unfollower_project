{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ec1640d-123d-4c76-ba7b-6b2cc1cd5d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import negative_sampling\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060860fa-6c79-4c94-9131-6455dd4ba228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def open_files():\n",
    "    with open('./Unfollower/15weeks_follower_dict.pkl', 'rb') as f:\n",
    "        follower = pickle.load(f)\n",
    "\n",
    "    with open('./Unfollower/15weeks_friend_dict.pkl', 'rb') as f:\n",
    "        friend = pickle.load(f)\n",
    "\n",
    "    with open('./Unfollower/e_tweet_dict.pkl', 'rb') as f:\n",
    "        e_tweet = pickle.load(f)\n",
    "\n",
    "    with open('./Unfollower/m_tweet_dict.pkl', 'rb') as f:\n",
    "        m_tweet = pickle.load(f)\n",
    "\n",
    "    with open('./Unfollower/r_tweet_dict.pkl', 'rb') as f:\n",
    "        r_tweet = pickle.load(f)\n",
    "        \n",
    "    return (follower,friend,e_tweet,m_tweet,r_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "970b653a-7657-4c3c-afe6-f68c82e5b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = (0,9)\n",
    "test_range = (10,14)\n",
    "\n",
    "\n",
    "# make a networkx graph in order to find the clustering coefficient of the nodes\n",
    "def getTrainClustering(follower,friend,edge_attr,edge_to_in,node_set):\n",
    "    G = nx.Graph()\n",
    "    for key in follower:\n",
    "        if len(follower[key][train_range[1]]) == 2 and key in node_set:\n",
    "            for f in follower[key][train_range[1]][1]:\n",
    "                if f in node_set:\n",
    "                    G.add_edge(key,f)\n",
    "\n",
    "    for key in friend:\n",
    "        if len(friend[key][train_range[1]]) == 2 and key in node_set:\n",
    "            for f in friend[key][train_range[1]][1]:\n",
    "                if f in node_set:\n",
    "                    G.add_edge(f,key)\n",
    "              \n",
    "    print(\"Getting train common neighbors\")\n",
    "    #i = 0\n",
    "    for key in edge_to_in:\n",
    "        if edge_attr[edge_to_in[key]][0] == 0.0 and key[0] in G and key[1] in G:\n",
    "            neighbors = sum(1 for _ in nx.common_neighbors(G,key[0],key[1]))\n",
    "            edge_attr[edge_to_in[key]][0] = neighbors\n",
    "            if (key[1],key[0]) in edge_to_in:\n",
    "                edge_attr[edge_to_in[(key[1],key[0])]][0] = neighbors\n",
    "    print(\"Getting train clustering\")  \n",
    "    cluster_coeffs = nx.clustering(G)\n",
    "    return (cluster_coeffs,edge_attr)\n",
    "\n",
    "\n",
    "    # make another for the test set\n",
    "def getTestClustering(follower,friend,edge_attr,edge_to_in,node_set):\n",
    "    G2 = nx.Graph()\n",
    "    for key in follower:\n",
    "        if len(follower[key][test_range[1]]) == 2 and key in node_set:\n",
    "            for f in follower[key][test_range[1]][1]:\n",
    "                if f in node_set:\n",
    "                    G2.add_edge(key,f)\n",
    "\n",
    "    for key in friend:\n",
    "        if len(friend[key][test_range[1]]) == 2 and key in node_set:\n",
    "            for f in friend[key][test_range[1]][1]:\n",
    "                if f in node_set:\n",
    "                    G2.add_edge(f,key)\n",
    "    \n",
    "    print(\"Getting test common neighbors\")\n",
    "    for key in edge_to_in:\n",
    "        if edge_attr[edge_to_in[key]][0] == 0.0 and key[0] in G2 and key[1] in G2:\n",
    "            neighbors = sum(1 for _ in nx.common_neighbors(G2,key[0],key[1]))\n",
    "            edge_attr[edge_to_in[key]][0] = neighbors\n",
    "            if (key[1],key[0]) in edge_to_in:\n",
    "                edge_attr[edge_to_in[(key[1],key[0])]][0] = neighbors       \n",
    "    print(\"Getting test clustering\")     \n",
    "    cluster_coeffs = nx.clustering(G2)\n",
    "    return (cluster_coeffs,edge_attr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba110697-01a1-4921-9c8a-3d994ba9af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_edges(follower,friend,id_to_in,node_set):\n",
    "    nodes1 = []\n",
    "    nodes2 = []\n",
    "\n",
    "    # mapping of edge tuple to index in edge_index tensor\n",
    "    edge_to_in = {}\n",
    "\n",
    "    # Create edge lists for the train Data object\n",
    "    for key in follower:\n",
    "        if len(follower[key][train_range[1]]) == 2 and key in node_set:\n",
    "            for f in follower[key][train_range[1]][1]:\n",
    "                if f in node_set:\n",
    "                    edge_to_in[(key,f)] = len(nodes1)\n",
    "                    nodes1.append(id_to_in[key])\n",
    "                    nodes2.append(id_to_in[f])\n",
    "\n",
    "    for key in friend:\n",
    "        if len(friend[key][train_range[1]]) == 2 and key in node_set:\n",
    "            for f in friend[key][train_range[1]][1]:\n",
    "                if f in node_set:\n",
    "                    edge_to_in[(f,key)] = len(nodes1)\n",
    "                    nodes1.append(id_to_in[f])\n",
    "                    nodes2.append(id_to_in[key])\n",
    "                \n",
    "    edge_label = [0.0]*len(nodes1)\n",
    "    \n",
    "    # find edges that have been removed\n",
    "    for key in follower:\n",
    "        if len(follower[key][train_range[0]]) == 2 and key in node_set:\n",
    "            for f in follower[key][train_range[0]][1]:\n",
    "                if (key,f) not in edge_to_in and f in node_set:\n",
    "                    edge_label.append(1.0)\n",
    "                    edge_to_in[(key,f)] = len(nodes1)\n",
    "                    nodes1.append(id_to_in[key])\n",
    "                    nodes2.append(id_to_in[f])\n",
    "                    \n",
    "    for key in friend:\n",
    "        if len(friend[key][train_range[0]]) == 2 and key in node_set:\n",
    "            for f in friend[key][train_range[0]][1]:\n",
    "                if (f,key) not in edge_to_in and f in node_set:\n",
    "                    edge_label.append(1.0)\n",
    "                    edge_to_in[(f,key)] = len(nodes1)\n",
    "                    nodes1.append(id_to_in[f])\n",
    "                    nodes2.append(id_to_in[key])\n",
    "                    \n",
    "    return (nodes1,nodes2,edge_label,edge_to_in)\n",
    "\n",
    "def get_test_edges(follower,friend,id_to_in,node_set):\n",
    "    nodes3 = []\n",
    "    nodes4 = []\n",
    "    edge_to_in2 = {}\n",
    "\n",
    "\n",
    "    # Create edge lists for the test Data object\n",
    "    for key in follower:\n",
    "        if len(follower[key][test_range[1]]) == 2 and key in node_set:\n",
    "            for f in follower[key][test_range[1]][1]:\n",
    "                if f in node_set:\n",
    "                    edge_to_in2[(key,f)] = len(nodes3)\n",
    "                    nodes3.append(id_to_in[key])\n",
    "                    nodes4.append(id_to_in[f])\n",
    "\n",
    "    for key in friend:\n",
    "        if len(friend[key][test_range[1]]) == 2 and key in node_set:\n",
    "            for f in friend[key][test_range[1]][1]:\n",
    "                if f in node_set:\n",
    "                    edge_to_in2[(f,key)] = len(nodes3)\n",
    "                    nodes3.append(id_to_in[f])\n",
    "                    nodes4.append(id_to_in[key])\n",
    "                \n",
    "    edge_label = [0.0]*len(nodes3)\n",
    "    \n",
    "    # find edges that have been removed\n",
    "    for key in follower:\n",
    "        if len(follower[key][test_range[0]]) == 2 and key in node_set:\n",
    "            for f in follower[key][test_range[0]][1]:\n",
    "                if (key,f) not in edge_to_in2 and f in node_set:\n",
    "                    edge_label.append(1.0)\n",
    "                    edge_to_in2[(key,f)] = len(nodes3)\n",
    "                    nodes3.append(id_to_in[key])\n",
    "                    nodes4.append(id_to_in[f])\n",
    "                    \n",
    "    for key in friend:\n",
    "        if len(friend[key][test_range[0]]) == 2 and key in node_set:\n",
    "            for f in friend[key][test_range[0]][1]:\n",
    "                if (f,key) not in edge_to_in2 and f in node_set:\n",
    "                    edge_label.append(1.0)\n",
    "                    edge_to_in2[(f,key)] = len(nodes3)\n",
    "                    nodes3.append(id_to_in[f])\n",
    "                    nodes4.append(id_to_in[key])\n",
    "                \n",
    "    return (nodes3,nodes4,edge_label,edge_to_in2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4686b5b5-93fe-412b-9f50-8882d0fe857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# put clustering coefficients in feature array\n",
    "\n",
    "def add_clustering(x,n,in_to_id,cluster_coeffs):\n",
    "    for i in range(n):\n",
    "        key = str(in_to_id[i])\n",
    "        if key in cluster_coeffs:\n",
    "            x[i][0] = cluster_coeffs[key]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f1fa78d-0efa-42e2-958e-cdb3a90f5ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add number of tweets to feature list x\n",
    "\n",
    "def add_tweets_train(x,n,in_to_id,e_tweet,node_set):\n",
    "    for i in range(n):\n",
    "        key = in_to_id[i]\n",
    "        x[i][1] = 0\n",
    "        for j in range(train_range[1]):\n",
    "            if key in e_tweet['train'][j+1]:\n",
    "                x[i][1] += len(e_tweet['train'][j+1][key])\n",
    "                \n",
    "def add_tweets_test(x2,n,in_to_id,e_tweet,node_set):\n",
    "    for i in range(n):\n",
    "        key = in_to_id[i]\n",
    "        x2[i][1] = 0\n",
    "        for j in range(test_range[0],test_range[1]):\n",
    "            if key in e_tweet['test'][j+1]:\n",
    "                x2[i][1] += len(e_tweet['test'][j+1][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "737df115-3409-4b9b-b608-882cde0df175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_edge_tweets_train(edge_attr,n,edge_to_in,nodes1,nodes2,m_tweet,r_tweet):\n",
    "    for key in edge_to_in:\n",
    "        for j in range(train_range[1]):\n",
    "            if key[0] in m_tweet['train'][j+1] and key[1] in m_tweet['train'][j+1][key[0]]:\n",
    "                edge_attr[edge_to_in[key]][1] += len(m_tweet['train'][j+1][key[0]][key[1]])\n",
    "            if key[0] in r_tweet['train'][j+1] and key[1] in r_tweet['train'][j+1][key[0]]:\n",
    "                edge_attr[edge_to_in[key]][1] += len(r_tweet['train'][j+1][key[0]][key[1]])\n",
    "                \n",
    "def add_edge_tweets_test(edge_attr,n,edge_to_in,nodes1,nodes2,m_tweet,r_tweet):\n",
    "    for key in edge_to_in:\n",
    "        for j in range(test_range[0],test_range[1]):\n",
    "            if key[0] in m_tweet['test'][j+1] and key[1] in m_tweet['test'][j+1][key[0]]:\n",
    "                edge_attr[edge_to_in[key]][1] += len(m_tweet['test'][j+1][key[0]][key[1]])\n",
    "            if key[0] in r_tweet['test'][j+1] and key[1] in r_tweet['test'][j+1][key[0]]:\n",
    "                edge_attr[edge_to_in[key]][1] += len(r_tweet['test'][j+1][key[0]][key[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34834938-bb28-4ae7-9ed9-cdc67f61ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data():\n",
    "    print(\"loading files\")\n",
    "    follower,friend,e_tweet,m_tweet,r_tweet = open_files()\n",
    "    # make a mapping of the person's id to their index in the node list\n",
    "    # The index will be the index of their x values when making the Data object\n",
    "    id_to_in1 = {}\n",
    "    in_to_id1 = []\n",
    "    id_to_in2 = {}\n",
    "    in_to_id2 = []\n",
    "    i1 = 0\n",
    "    i2 = 0\n",
    "    # minimum followers needed to be added to the graph\n",
    "    min_count = 100\n",
    "    train_nodes = set()\n",
    "    test_nodes = set()\n",
    "    for key in follower:\n",
    "        if len(follower[key][train_range[1]]) == 2 and len(follower[key][train_range[1]][1]) > min_count:\n",
    "            id_to_in1[key] = i1\n",
    "            i1 += 1\n",
    "            train_nodes.add(key)\n",
    "            in_to_id1.append(key)\n",
    "        if len(follower[key][test_range[1]]) == 2 and len(follower[key][test_range[1]][1]) > min_count:\n",
    "            id_to_in2[key] = i2\n",
    "            i2 += 1\n",
    "            test_nodes.add(key)\n",
    "            in_to_id2.append(key)\n",
    "    \n",
    "    print(\"getting edges\")\n",
    "    # get edges\n",
    "    nodes1,nodes2,edge_label_train,edge_to_in_train = get_train_edges(follower,friend,id_to_in1,train_nodes)\n",
    "    nodes3,nodes4,edge_label_test,edge_to_in_test = get_test_edges(follower,friend,id_to_in2,test_nodes)\n",
    "    \n",
    "    # make edge feature arrays\n",
    "    num_edges_train = len(nodes1)\n",
    "    num_edges_test = len(nodes3)\n",
    "    edge_attr1 = [[0 for i in range(2)] for j in range(num_edges_train)]\n",
    "    edge_attr2 = [[0 for i in range(2)] for j in range(num_edges_test)]\n",
    "    \n",
    "    # calculate the clustering coefficients and common neighbors\n",
    "    cluster_coeffs,edge_attr1 = getTrainClustering(follower,friend,edge_attr1,edge_to_in_train,train_nodes)\n",
    "    cluster_coeffs2,edge_attr2 = getTestClustering(follower,friend,edge_attr2,edge_to_in_test,test_nodes)\n",
    "    print(\"exited nx graph calculations\")\n",
    "    \n",
    "    # add retweets and mentions\n",
    "    add_edge_tweets_train(edge_attr1,num_edges_train,edge_to_in_train,nodes1,nodes2,m_tweet,r_tweet)\n",
    "    add_edge_tweets_train(edge_attr2,num_edges_test,edge_to_in_test,nodes3,nodes4,m_tweet,r_tweet)\n",
    "    \n",
    "    \n",
    "    # make node feature arrays\n",
    "    n_train = len(in_to_id1)\n",
    "    n_test = len(in_to_id2)\n",
    "    x = [[0 for i in range(2)] for j in range(n_train)]\n",
    "    x2 = [[0 for i in range(2)] for j in range(n_test)]\n",
    "    \n",
    "    \n",
    "    print(\"adding clustering to feature array\")\n",
    "    # add clustering coefficients to the feature arrays\n",
    "    add_clustering(x,n_train,in_to_id1,cluster_coeffs)\n",
    "    add_clustering(x2,n_test,in_to_id2,cluster_coeffs2)\n",
    "    \n",
    "    \n",
    "    print(\"adding number of tweets\")\n",
    "    # add number of tweets per user\n",
    "    add_tweets_train(x,n_train,in_to_id1,e_tweet,train_nodes)\n",
    "    add_tweets_test(x2,n_test,in_to_id2,e_tweet,test_nodes)\n",
    "    \n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    x2 = torch.tensor(x2, dtype=torch.float)\n",
    "    edge_index = torch.tensor([nodes1,nodes2], dtype=torch.long)\n",
    "    edge_index2 = torch.tensor([nodes3,nodes4], dtype=torch.long)\n",
    "    edge_label = torch.tensor(edge_label_train, dtype=torch.float)\n",
    "    edge_label2 = torch.tensor(edge_label_test, dtype=torch.float)\n",
    "    edge_attr1 = torch.tensor(edge_attr1, dtype=torch.float)\n",
    "    edge_attr2 = torch.tensor(edge_attr2, dtype=torch.float)\n",
    "    \n",
    "    train_data = Data(x=x, edge_index=edge_index, edge_label_index=edge_index, edge_label=edge_label,edge_attr=edge_attr1)\n",
    "    test_data = Data(x=x2, edge_index=edge_index2, edge_label_index=edge_index2, edge_label=edge_label2,edge_attr=edge_attr2)\n",
    "    \n",
    "    print(\"Done\")\n",
    "    return (train_data,test_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdb37ef2-084c-4019-bb8f-7b11a8344f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading files\n",
      "getting edges\n",
      "Getting train common neighbors\n",
      "Getting train clustering\n",
      "Getting test common neighbors\n",
      "Getting test clustering\n",
      "exited nx graph calculations\n",
      "adding clustering to feature array\n",
      "adding number of tweets\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_data,test_data = make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7251791a-31f0-4514-a9e4-c24ee04d806f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3154562\n",
      "1.0\n",
      "3137190\n",
      "3154562\n"
     ]
    }
   ],
   "source": [
    "l = train_data.edge_label.tolist()\n",
    "pos_edges = -1\n",
    "print(train_data.edge_label_index.size(1))\n",
    "print(l[-1])\n",
    "for j in range(len(l)):\n",
    "    if pos_edges == -1 and l[j] == 1:\n",
    "        pos_edges = j\n",
    "print(pos_edges)\n",
    "print(len(l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078e1fb-dee9-4e3f-81d6-ea3c977ce52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10000000149011612, 0.0], [1.0, 0.0], [0.4346197247505188, 5.0], [0.11166881769895554, 9.0], [0.0, 0.0], [0.0, 0.0], [0.2554112672805786, 7.0], [0.0, 0.0], [0.20346319675445557, 0.0], [0.4346764385700226, 0.0], [0.0, 14.0], [0.0679602101445198, 21.0], [0.20705881714820862, 0.0], [0.08176100999116898, 4.0], [1.0, 0.0], [0.4000000059604645, 0.0], [0.1428571492433548, 0.0], [0.12946158647537231, 1.0], [0.0, 0.0], [0.07621333748102188, 1.0]]\n",
      "Epoch: 001, Loss: 9.3419, Train: 0.5345, Test: 0.5784\n",
      "Epoch: 002, Loss: 13.5071, Train: 0.5345, Test: 0.5783\n",
      "Epoch: 003, Loss: 7.9284, Train: 0.5344, Test: 0.5783\n",
      "Epoch: 004, Loss: 10.2939, Train: 0.5344, Test: 0.5783\n",
      "Epoch: 005, Loss: 9.8050, Train: 0.5343, Test: 0.5783\n",
      "Epoch: 006, Loss: 8.4763, Train: 0.5343, Test: 0.5783\n",
      "Epoch: 007, Loss: 10.4235, Train: 0.5342, Test: 0.5782\n",
      "Epoch: 008, Loss: 8.0204, Train: 0.5342, Test: 0.5782\n",
      "Epoch: 009, Loss: 9.6446, Train: 0.5342, Test: 0.5782\n",
      "Epoch: 010, Loss: 8.1146, Train: 0.5341, Test: 0.5782\n",
      "Epoch: 011, Loss: 6.9492, Train: 0.5341, Test: 0.5782\n",
      "Epoch: 012, Loss: 9.1080, Train: 0.5341, Test: 0.5781\n",
      "Epoch: 013, Loss: 10.0643, Train: 0.5340, Test: 0.5781\n",
      "Epoch: 014, Loss: 8.6337, Train: 0.5340, Test: 0.5781\n",
      "Epoch: 015, Loss: 10.2747, Train: 0.5339, Test: 0.5781\n",
      "Epoch: 016, Loss: 9.6570, Train: 0.5339, Test: 0.5781\n",
      "Epoch: 017, Loss: 9.8672, Train: 0.5338, Test: 0.5780\n",
      "Epoch: 018, Loss: 12.5105, Train: 0.5338, Test: 0.5780\n",
      "Epoch: 019, Loss: 3.8993, Train: 0.5338, Test: 0.5780\n",
      "Epoch: 020, Loss: 9.6125, Train: 0.5337, Test: 0.5780\n",
      "Epoch: 021, Loss: 8.0390, Train: 0.5337, Test: 0.5780\n",
      "Epoch: 022, Loss: 8.9873, Train: 0.5336, Test: 0.5779\n",
      "Epoch: 023, Loss: 8.0507, Train: 0.5336, Test: 0.5779\n",
      "Epoch: 024, Loss: 7.6637, Train: 0.5335, Test: 0.5779\n",
      "Epoch: 025, Loss: 9.4538, Train: 0.5335, Test: 0.5779\n",
      "Epoch: 026, Loss: 7.9074, Train: 0.5334, Test: 0.5778\n",
      "Epoch: 027, Loss: 6.7832, Train: 0.5334, Test: 0.5778\n",
      "Epoch: 028, Loss: 7.5166, Train: 0.5333, Test: 0.5778\n",
      "Epoch: 029, Loss: 10.0115, Train: 0.5333, Test: 0.5778\n",
      "Epoch: 030, Loss: 9.5453, Train: 0.5332, Test: 0.5777\n",
      "Epoch: 031, Loss: 10.5089, Train: 0.5332, Test: 0.5777\n",
      "Epoch: 032, Loss: 8.8908, Train: 0.5331, Test: 0.5777\n",
      "Epoch: 033, Loss: 7.0630, Train: 0.5331, Test: 0.5777\n",
      "Epoch: 034, Loss: 10.4314, Train: 0.5331, Test: 0.5776\n",
      "Epoch: 035, Loss: 11.0752, Train: 0.5330, Test: 0.5776\n",
      "Epoch: 036, Loss: 11.7390, Train: 0.5330, Test: 0.5776\n",
      "Epoch: 037, Loss: 9.8878, Train: 0.5329, Test: 0.5776\n",
      "Epoch: 038, Loss: 6.5335, Train: 0.5329, Test: 0.5776\n",
      "Epoch: 039, Loss: 7.3779, Train: 0.5329, Test: 0.5775\n",
      "Epoch: 040, Loss: 8.5460, Train: 0.5328, Test: 0.5775\n",
      "Epoch: 041, Loss: 9.9283, Train: 0.5328, Test: 0.5775\n",
      "Epoch: 042, Loss: 9.3581, Train: 0.5328, Test: 0.5775\n",
      "Epoch: 043, Loss: 7.3548, Train: 0.5327, Test: 0.5774\n",
      "Epoch: 044, Loss: 8.6710, Train: 0.5327, Test: 0.5774\n",
      "Epoch: 045, Loss: 6.9474, Train: 0.5326, Test: 0.5774\n",
      "Epoch: 046, Loss: 7.4917, Train: 0.5326, Test: 0.5773\n",
      "Epoch: 047, Loss: 11.1600, Train: 0.5326, Test: 0.5773\n",
      "Epoch: 048, Loss: 9.5778, Train: 0.5325, Test: 0.5773\n",
      "Epoch: 049, Loss: 10.6062, Train: 0.5325, Test: 0.5773\n",
      "Epoch: 050, Loss: 9.4198, Train: 0.5324, Test: 0.5772\n",
      "Epoch: 051, Loss: 9.4917, Train: 0.5324, Test: 0.5772\n",
      "Epoch: 052, Loss: 8.1725, Train: 0.5323, Test: 0.5772\n",
      "Epoch: 053, Loss: 8.9953, Train: 0.5323, Test: 0.5772\n",
      "Epoch: 054, Loss: 9.7328, Train: 0.5322, Test: 0.5772\n",
      "Epoch: 055, Loss: 9.1136, Train: 0.5322, Test: 0.5771\n",
      "Epoch: 056, Loss: 9.3981, Train: 0.5322, Test: 0.5771\n",
      "Epoch: 057, Loss: 7.1856, Train: 0.5321, Test: 0.5771\n",
      "Epoch: 058, Loss: 6.7022, Train: 0.5321, Test: 0.5771\n",
      "Epoch: 059, Loss: 9.9223, Train: 0.5320, Test: 0.5770\n",
      "Epoch: 060, Loss: 6.2338, Train: 0.5320, Test: 0.5770\n",
      "Epoch: 061, Loss: 9.0326, Train: 0.5319, Test: 0.5770\n",
      "Epoch: 062, Loss: 6.4552, Train: 0.5319, Test: 0.5770\n",
      "Epoch: 063, Loss: 4.2525, Train: 0.5319, Test: 0.5770\n",
      "Epoch: 064, Loss: 8.7295, Train: 0.5318, Test: 0.5769\n",
      "Epoch: 065, Loss: 10.6827, Train: 0.5318, Test: 0.5769\n",
      "Epoch: 066, Loss: 9.1320, Train: 0.5317, Test: 0.5769\n",
      "Epoch: 067, Loss: 9.6198, Train: 0.5317, Test: 0.5769\n",
      "Epoch: 068, Loss: 9.4618, Train: 0.5317, Test: 0.5768\n",
      "Epoch: 069, Loss: 9.7416, Train: 0.5316, Test: 0.5768\n",
      "Epoch: 070, Loss: 7.3867, Train: 0.5316, Test: 0.5768\n",
      "Epoch: 071, Loss: 7.0996, Train: 0.5315, Test: 0.5768\n",
      "Epoch: 072, Loss: 7.1286, Train: 0.5315, Test: 0.5767\n",
      "Epoch: 073, Loss: 7.5979, Train: 0.5314, Test: 0.5767\n",
      "Epoch: 074, Loss: 7.0863, Train: 0.5314, Test: 0.5767\n",
      "Epoch: 075, Loss: 6.7289, Train: 0.5314, Test: 0.5767\n",
      "Epoch: 076, Loss: 5.8200, Train: 0.5313, Test: 0.5766\n",
      "Epoch: 077, Loss: 9.5629, Train: 0.5313, Test: 0.5766\n",
      "Epoch: 078, Loss: 9.8427, Train: 0.5313, Test: 0.5766\n",
      "Epoch: 079, Loss: 8.9655, Train: 0.5312, Test: 0.5766\n",
      "Epoch: 080, Loss: 8.6005, Train: 0.5312, Test: 0.5766\n",
      "Epoch: 081, Loss: 8.0676, Train: 0.5311, Test: 0.5765\n",
      "Epoch: 082, Loss: 6.2266, Train: 0.5311, Test: 0.5765\n",
      "Epoch: 083, Loss: 10.0667, Train: 0.5310, Test: 0.5765\n",
      "Epoch: 084, Loss: 8.6115, Train: 0.5310, Test: 0.5765\n",
      "Epoch: 085, Loss: 8.3136, Train: 0.5309, Test: 0.5764\n",
      "Epoch: 086, Loss: 3.2181, Train: 0.5309, Test: 0.5764\n",
      "Epoch: 087, Loss: 9.1370, Train: 0.5309, Test: 0.5764\n",
      "Epoch: 088, Loss: 8.2264, Train: 0.5308, Test: 0.5764\n",
      "Epoch: 089, Loss: 7.1356, Train: 0.5308, Test: 0.5763\n",
      "Epoch: 090, Loss: 9.1732, Train: 0.5307, Test: 0.5763\n",
      "Epoch: 091, Loss: 9.8729, Train: 0.5307, Test: 0.5763\n",
      "Epoch: 092, Loss: 7.2177, Train: 0.5306, Test: 0.5763\n",
      "Epoch: 093, Loss: 9.0945, Train: 0.5306, Test: 0.5762\n",
      "Epoch: 094, Loss: 8.1095, Train: 0.5306, Test: 0.5762\n",
      "Epoch: 095, Loss: 7.8244, Train: 0.5305, Test: 0.5762\n",
      "Epoch: 096, Loss: 4.2542, Train: 0.5305, Test: 0.5761\n",
      "Epoch: 097, Loss: 7.7557, Train: 0.5304, Test: 0.5761\n",
      "Epoch: 098, Loss: 5.6737, Train: 0.5304, Test: 0.5761\n",
      "Epoch: 099, Loss: 10.1777, Train: 0.5304, Test: 0.5761\n",
      "Epoch: 100, Loss: 8.3718, Train: 0.5303, Test: 0.5760\n",
      "Epoch: 101, Loss: 9.0020, Train: 0.5302, Test: 0.5760\n",
      "Epoch: 102, Loss: 8.2204, Train: 0.5302, Test: 0.5760\n",
      "Epoch: 103, Loss: 8.8193, Train: 0.5302, Test: 0.5759\n",
      "Epoch: 104, Loss: 6.6710, Train: 0.5301, Test: 0.5759\n",
      "Epoch: 105, Loss: 6.8962, Train: 0.5301, Test: 0.5759\n",
      "Epoch: 106, Loss: 7.3639, Train: 0.5300, Test: 0.5759\n",
      "Epoch: 107, Loss: 8.8271, Train: 0.5300, Test: 0.5758\n",
      "Epoch: 108, Loss: 9.7926, Train: 0.5300, Test: 0.5758\n",
      "Epoch: 109, Loss: 7.8683, Train: 0.5299, Test: 0.5758\n",
      "Epoch: 110, Loss: 6.5172, Train: 0.5299, Test: 0.5758\n",
      "Epoch: 111, Loss: 7.2713, Train: 0.5298, Test: 0.5757\n",
      "Epoch: 112, Loss: 9.9375, Train: 0.5298, Test: 0.5757\n",
      "Epoch: 113, Loss: 8.5854, Train: 0.5297, Test: 0.5757\n",
      "Epoch: 114, Loss: 7.3683, Train: 0.5297, Test: 0.5757\n",
      "Epoch: 115, Loss: 5.2367, Train: 0.5297, Test: 0.5756\n",
      "Epoch: 116, Loss: 8.1934, Train: 0.5296, Test: 0.5756\n",
      "Epoch: 117, Loss: 5.5131, Train: 0.5295, Test: 0.5756\n",
      "Epoch: 118, Loss: 6.4195, Train: 0.5295, Test: 0.5756\n",
      "Epoch: 119, Loss: 7.8911, Train: 0.5294, Test: 0.5755\n",
      "Epoch: 120, Loss: 7.2050, Train: 0.5294, Test: 0.5755\n",
      "Epoch: 121, Loss: 7.4524, Train: 0.5294, Test: 0.5755\n",
      "Epoch: 122, Loss: 7.7964, Train: 0.5293, Test: 0.5755\n",
      "Epoch: 123, Loss: 10.3732, Train: 0.5293, Test: 0.5754\n",
      "Epoch: 124, Loss: 8.0904, Train: 0.5292, Test: 0.5754\n",
      "Epoch: 125, Loss: 5.4185, Train: 0.5292, Test: 0.5754\n",
      "Epoch: 126, Loss: 6.2312, Train: 0.5291, Test: 0.5754\n",
      "Epoch: 127, Loss: 7.2008, Train: 0.5291, Test: 0.5753\n",
      "Epoch: 128, Loss: 8.0654, Train: 0.5291, Test: 0.5753\n",
      "Epoch: 129, Loss: 6.3450, Train: 0.5290, Test: 0.5753\n",
      "Epoch: 130, Loss: 6.8290, Train: 0.5290, Test: 0.5752\n",
      "Epoch: 131, Loss: 7.6119, Train: 0.5289, Test: 0.5752\n",
      "Epoch: 132, Loss: 8.7537, Train: 0.5289, Test: 0.5752\n",
      "Epoch: 133, Loss: 8.2814, Train: 0.5289, Test: 0.5752\n",
      "Epoch: 134, Loss: 7.6587, Train: 0.5288, Test: 0.5751\n",
      "Epoch: 135, Loss: 5.3749, Train: 0.5288, Test: 0.5751\n",
      "Epoch: 136, Loss: 5.4479, Train: 0.5287, Test: 0.5751\n",
      "Epoch: 137, Loss: 8.0505, Train: 0.5287, Test: 0.5750\n",
      "Epoch: 138, Loss: 8.1593, Train: 0.5287, Test: 0.5750\n",
      "Epoch: 139, Loss: 7.6793, Train: 0.5286, Test: 0.5750\n",
      "Epoch: 140, Loss: 6.3424, Train: 0.5286, Test: 0.5750\n",
      "Epoch: 141, Loss: 7.8884, Train: 0.5285, Test: 0.5749\n",
      "Epoch: 142, Loss: 8.4090, Train: 0.5285, Test: 0.5749\n",
      "Epoch: 143, Loss: 5.3518, Train: 0.5285, Test: 0.5749\n",
      "Epoch: 144, Loss: 2.8633, Train: 0.5284, Test: 0.5748\n",
      "Epoch: 145, Loss: 5.2950, Train: 0.5284, Test: 0.5748\n",
      "Epoch: 146, Loss: 6.6812, Train: 0.5283, Test: 0.5748\n",
      "Epoch: 147, Loss: 8.3252, Train: 0.5283, Test: 0.5748\n",
      "Epoch: 148, Loss: 6.9733, Train: 0.5283, Test: 0.5747\n",
      "Epoch: 149, Loss: 7.0352, Train: 0.5282, Test: 0.5747\n",
      "Epoch: 150, Loss: 8.3003, Train: 0.5282, Test: 0.5747\n",
      "Epoch: 151, Loss: 8.7132, Train: 0.5282, Test: 0.5747\n",
      "Epoch: 152, Loss: 7.9313, Train: 0.5281, Test: 0.5746\n",
      "Epoch: 153, Loss: 6.5089, Train: 0.5281, Test: 0.5746\n",
      "Epoch: 154, Loss: 4.3858, Train: 0.5281, Test: 0.5746\n",
      "Epoch: 155, Loss: 6.8961, Train: 0.5280, Test: 0.5746\n",
      "Epoch: 156, Loss: 8.5668, Train: 0.5280, Test: 0.5745\n",
      "Epoch: 157, Loss: 5.9713, Train: 0.5279, Test: 0.5745\n",
      "Epoch: 158, Loss: 7.8424, Train: 0.5279, Test: 0.5745\n",
      "Epoch: 159, Loss: 7.1336, Train: 0.5279, Test: 0.5744\n",
      "Epoch: 160, Loss: 6.2644, Train: 0.5278, Test: 0.5744\n",
      "Epoch: 161, Loss: 5.9195, Train: 0.5278, Test: 0.5744\n",
      "Epoch: 162, Loss: 5.8459, Train: 0.5277, Test: 0.5743\n",
      "Epoch: 163, Loss: 7.5063, Train: 0.5277, Test: 0.5743\n",
      "Epoch: 164, Loss: 7.2595, Train: 0.5277, Test: 0.5743\n",
      "Epoch: 165, Loss: 8.4928, Train: 0.5276, Test: 0.5743\n",
      "Epoch: 166, Loss: 7.5560, Train: 0.5276, Test: 0.5742\n",
      "Epoch: 167, Loss: 6.1147, Train: 0.5276, Test: 0.5742\n",
      "Epoch: 168, Loss: 6.6812, Train: 0.5275, Test: 0.5742\n",
      "Epoch: 169, Loss: 6.5606, Train: 0.5275, Test: 0.5742\n",
      "Epoch: 170, Loss: 6.4245, Train: 0.5275, Test: 0.5741\n",
      "Epoch: 171, Loss: 5.5157, Train: 0.5274, Test: 0.5741\n",
      "Epoch: 172, Loss: 7.2969, Train: 0.5274, Test: 0.5741\n",
      "Epoch: 173, Loss: 3.9243, Train: 0.5274, Test: 0.5740\n",
      "Epoch: 174, Loss: 8.4988, Train: 0.5273, Test: 0.5740\n",
      "Epoch: 175, Loss: 5.1414, Train: 0.5273, Test: 0.5740\n",
      "Epoch: 176, Loss: 6.1020, Train: 0.5273, Test: 0.5740\n",
      "Epoch: 177, Loss: 6.4719, Train: 0.5272, Test: 0.5739\n",
      "Epoch: 178, Loss: 5.7997, Train: 0.5272, Test: 0.5739\n",
      "Epoch: 179, Loss: 6.9608, Train: 0.5272, Test: 0.5739\n",
      "Epoch: 180, Loss: 7.1554, Train: 0.5271, Test: 0.5739\n",
      "Epoch: 181, Loss: 6.8477, Train: 0.5271, Test: 0.5738\n",
      "Epoch: 182, Loss: 6.5695, Train: 0.5271, Test: 0.5738\n",
      "Epoch: 183, Loss: 7.7260, Train: 0.5270, Test: 0.5738\n",
      "Epoch: 184, Loss: 4.3968, Train: 0.5270, Test: 0.5737\n",
      "Epoch: 185, Loss: 6.1878, Train: 0.5269, Test: 0.5737\n",
      "Epoch: 186, Loss: 5.4801, Train: 0.5269, Test: 0.5737\n",
      "Epoch: 187, Loss: 5.8528, Train: 0.5268, Test: 0.5737\n",
      "Epoch: 188, Loss: 7.0035, Train: 0.5268, Test: 0.5736\n",
      "Epoch: 189, Loss: 5.1978, Train: 0.5268, Test: 0.5736\n",
      "Epoch: 190, Loss: 6.6031, Train: 0.5267, Test: 0.5736\n",
      "Epoch: 191, Loss: 7.0922, Train: 0.5267, Test: 0.5736\n",
      "Epoch: 192, Loss: 7.3201, Train: 0.5267, Test: 0.5735\n",
      "Epoch: 193, Loss: 6.2413, Train: 0.5266, Test: 0.5735\n",
      "Epoch: 194, Loss: 5.6769, Train: 0.5266, Test: 0.5735\n",
      "Epoch: 195, Loss: 6.5454, Train: 0.5266, Test: 0.5734\n",
      "Epoch: 196, Loss: 4.9772, Train: 0.5265, Test: 0.5734\n",
      "Epoch: 197, Loss: 6.2054, Train: 0.5265, Test: 0.5734\n",
      "Epoch: 198, Loss: 7.9172, Train: 0.5264, Test: 0.5734\n",
      "Epoch: 199, Loss: 7.0724, Train: 0.5264, Test: 0.5733\n",
      "Epoch: 200, Loss: 8.1044, Train: 0.5264, Test: 0.5733\n",
      "Epoch: 201, Loss: 10.2254, Train: 0.5263, Test: 0.5733\n",
      "Epoch: 202, Loss: 4.8219, Train: 0.5263, Test: 0.5732\n",
      "Epoch: 203, Loss: 6.2002, Train: 0.5263, Test: 0.5732\n",
      "Epoch: 204, Loss: 6.9116, Train: 0.5262, Test: 0.5732\n",
      "Epoch: 205, Loss: 4.8908, Train: 0.5262, Test: 0.5732\n",
      "Epoch: 206, Loss: 6.8513, Train: 0.5261, Test: 0.5731\n",
      "Epoch: 207, Loss: 6.7373, Train: 0.5261, Test: 0.5731\n",
      "Epoch: 208, Loss: 6.4124, Train: 0.5260, Test: 0.5731\n",
      "Epoch: 209, Loss: 7.0631, Train: 0.5257, Test: 0.5731\n",
      "Epoch: 210, Loss: 5.4666, Train: 0.5255, Test: 0.5730\n",
      "Epoch: 211, Loss: 4.6432, Train: 0.5254, Test: 0.5730\n",
      "Epoch: 212, Loss: 5.5263, Train: 0.5254, Test: 0.5730\n",
      "Epoch: 213, Loss: 7.1436, Train: 0.5253, Test: 0.5730\n",
      "Epoch: 214, Loss: 6.7834, Train: 0.5253, Test: 0.5729\n",
      "Epoch: 215, Loss: 6.7777, Train: 0.5253, Test: 0.5729\n",
      "Epoch: 216, Loss: 7.0042, Train: 0.5252, Test: 0.5729\n",
      "Epoch: 217, Loss: 6.8515, Train: 0.5252, Test: 0.5728\n",
      "Epoch: 218, Loss: 6.9828, Train: 0.5251, Test: 0.5728\n",
      "Epoch: 219, Loss: 7.0025, Train: 0.5251, Test: 0.5728\n",
      "Epoch: 220, Loss: 7.0061, Train: 0.5250, Test: 0.5728\n",
      "Epoch: 221, Loss: 3.9921, Train: 0.5250, Test: 0.5727\n",
      "Epoch: 222, Loss: 5.6446, Train: 0.5249, Test: 0.5727\n",
      "Epoch: 223, Loss: 6.5162, Train: 0.5249, Test: 0.5727\n",
      "Epoch: 224, Loss: 7.3908, Train: 0.5249, Test: 0.5727\n",
      "Epoch: 225, Loss: 6.1454, Train: 0.5248, Test: 0.5726\n",
      "Epoch: 226, Loss: 4.3435, Train: 0.5248, Test: 0.5726\n",
      "Epoch: 227, Loss: 6.2103, Train: 0.5247, Test: 0.5726\n",
      "Epoch: 228, Loss: 8.3587, Train: 0.5247, Test: 0.5725\n",
      "Epoch: 229, Loss: 6.3699, Train: 0.5247, Test: 0.5725\n",
      "Epoch: 230, Loss: 5.1134, Train: 0.5246, Test: 0.5725\n",
      "Epoch: 231, Loss: 7.9713, Train: 0.5246, Test: 0.5725\n",
      "Epoch: 232, Loss: 5.1028, Train: 0.5246, Test: 0.5724\n",
      "Epoch: 233, Loss: 5.4824, Train: 0.5245, Test: 0.5724\n",
      "Epoch: 234, Loss: 5.4434, Train: 0.5245, Test: 0.5724\n",
      "Epoch: 235, Loss: 6.1318, Train: 0.5245, Test: 0.5724\n",
      "Epoch: 236, Loss: 6.0428, Train: 0.5244, Test: 0.5723\n",
      "Epoch: 237, Loss: 4.6244, Train: 0.5244, Test: 0.5723\n",
      "Epoch: 238, Loss: 5.5157, Train: 0.5244, Test: 0.5723\n",
      "Epoch: 239, Loss: 5.9955, Train: 0.5244, Test: 0.5723\n",
      "Epoch: 240, Loss: 5.0330, Train: 0.5243, Test: 0.5722\n",
      "Epoch: 241, Loss: 5.7695, Train: 0.5243, Test: 0.5722\n",
      "Epoch: 242, Loss: 5.8348, Train: 0.5243, Test: 0.5722\n",
      "Epoch: 243, Loss: 5.3167, Train: 0.5242, Test: 0.5722\n",
      "Epoch: 244, Loss: 7.6322, Train: 0.5242, Test: 0.5721\n",
      "Epoch: 245, Loss: 6.5163, Train: 0.5242, Test: 0.5721\n",
      "Epoch: 246, Loss: 6.5179, Train: 0.5241, Test: 0.5721\n",
      "Epoch: 247, Loss: 4.6854, Train: 0.5241, Test: 0.5721\n",
      "Epoch: 248, Loss: 4.5187, Train: 0.5241, Test: 0.5720\n",
      "Epoch: 249, Loss: 6.2095, Train: 0.5240, Test: 0.5720\n",
      "Epoch: 250, Loss: 6.0842, Train: 0.5240, Test: 0.5720\n",
      "Epoch: 251, Loss: 6.0625, Train: 0.5240, Test: 0.5719\n",
      "Epoch: 252, Loss: 6.6158, Train: 0.5240, Test: 0.5719\n",
      "Epoch: 253, Loss: 6.3755, Train: 0.5239, Test: 0.5719\n",
      "Epoch: 254, Loss: 5.1154, Train: 0.5239, Test: 0.5719\n",
      "Epoch: 255, Loss: 4.4006, Train: 0.5238, Test: 0.5718\n",
      "Epoch: 256, Loss: 4.0209, Train: 0.5238, Test: 0.5718\n",
      "Epoch: 257, Loss: 5.3235, Train: 0.5238, Test: 0.5718\n",
      "Epoch: 258, Loss: 3.4040, Train: 0.5238, Test: 0.5717\n",
      "Epoch: 259, Loss: 5.7917, Train: 0.5237, Test: 0.5717\n",
      "Epoch: 260, Loss: 5.6966, Train: 0.5237, Test: 0.5717\n",
      "Epoch: 261, Loss: 5.7333, Train: 0.5237, Test: 0.5717\n",
      "Epoch: 262, Loss: 4.7021, Train: 0.5236, Test: 0.5716\n",
      "Epoch: 263, Loss: 5.1781, Train: 0.5236, Test: 0.5716\n",
      "Epoch: 264, Loss: 6.0455, Train: 0.5236, Test: 0.5716\n",
      "Epoch: 265, Loss: 5.3462, Train: 0.5236, Test: 0.5715\n",
      "Epoch: 266, Loss: 5.9114, Train: 0.5235, Test: 0.5715\n",
      "Epoch: 267, Loss: 4.5744, Train: 0.5235, Test: 0.5715\n",
      "Epoch: 268, Loss: 6.5755, Train: 0.5235, Test: 0.5714\n",
      "Epoch: 269, Loss: 5.4827, Train: 0.5234, Test: 0.5714\n",
      "Epoch: 270, Loss: 4.2688, Train: 0.5234, Test: 0.5714\n",
      "Epoch: 271, Loss: 6.3702, Train: 0.5234, Test: 0.5713\n",
      "Epoch: 272, Loss: 4.4047, Train: 0.5233, Test: 0.5713\n",
      "Epoch: 273, Loss: 4.5200, Train: 0.5233, Test: 0.5713\n",
      "Epoch: 274, Loss: 5.9757, Train: 0.5233, Test: 0.5713\n",
      "Epoch: 275, Loss: 5.1871, Train: 0.5233, Test: 0.5712\n",
      "Epoch: 276, Loss: 4.9447, Train: 0.5233, Test: 0.5712\n",
      "Epoch: 277, Loss: 4.9245, Train: 0.5232, Test: 0.5712\n",
      "Epoch: 278, Loss: 4.2543, Train: 0.5232, Test: 0.5711\n",
      "Epoch: 279, Loss: 6.8512, Train: 0.5232, Test: 0.5711\n",
      "Epoch: 280, Loss: 5.6996, Train: 0.5232, Test: 0.5711\n",
      "Epoch: 281, Loss: 4.4085, Train: 0.5232, Test: 0.5710\n",
      "Epoch: 282, Loss: 5.5820, Train: 0.5231, Test: 0.5710\n",
      "Epoch: 283, Loss: 6.1611, Train: 0.5231, Test: 0.5710\n",
      "Epoch: 284, Loss: 5.1954, Train: 0.5231, Test: 0.5709\n",
      "Epoch: 285, Loss: 5.3168, Train: 0.5231, Test: 0.5709\n",
      "Epoch: 286, Loss: 4.0044, Train: 0.5230, Test: 0.5709\n",
      "Epoch: 287, Loss: 4.9182, Train: 0.5230, Test: 0.5708\n",
      "Epoch: 288, Loss: 3.9505, Train: 0.5230, Test: 0.5708\n",
      "Epoch: 289, Loss: 2.4014, Train: 0.5230, Test: 0.5708\n",
      "Epoch: 290, Loss: 4.0975, Train: 0.5229, Test: 0.5707\n",
      "Epoch: 291, Loss: 5.8067, Train: 0.5229, Test: 0.5707\n",
      "Epoch: 292, Loss: 6.1602, Train: 0.5229, Test: 0.5707\n",
      "Epoch: 293, Loss: 5.0513, Train: 0.5229, Test: 0.5706\n",
      "Epoch: 294, Loss: 4.9932, Train: 0.5228, Test: 0.5706\n",
      "Epoch: 295, Loss: 5.7985, Train: 0.5228, Test: 0.5706\n",
      "Epoch: 296, Loss: 3.8644, Train: 0.5228, Test: 0.5705\n",
      "Epoch: 297, Loss: 5.3701, Train: 0.5227, Test: 0.5705\n",
      "Epoch: 298, Loss: 6.3698, Train: 0.5227, Test: 0.5705\n",
      "Epoch: 299, Loss: 3.2838, Train: 0.5227, Test: 0.5704\n",
      "Epoch: 300, Loss: 4.4806, Train: 0.5226, Test: 0.5704\n",
      "Epoch: 301, Loss: 5.2842, Train: 0.5226, Test: 0.5704\n",
      "Epoch: 302, Loss: 7.3490, Train: 0.5226, Test: 0.5703\n",
      "Epoch: 303, Loss: 4.5356, Train: 0.5225, Test: 0.5703\n",
      "Epoch: 304, Loss: 5.2828, Train: 0.5225, Test: 0.5702\n",
      "Epoch: 305, Loss: 4.3496, Train: 0.5225, Test: 0.5702\n",
      "Epoch: 306, Loss: 4.6862, Train: 0.5225, Test: 0.5701\n",
      "Epoch: 307, Loss: 4.9815, Train: 0.5224, Test: 0.5701\n",
      "Epoch: 308, Loss: 4.2499, Train: 0.5224, Test: 0.5700\n",
      "Epoch: 309, Loss: 5.9195, Train: 0.5224, Test: 0.5699\n",
      "Epoch: 310, Loss: 4.6300, Train: 0.5224, Test: 0.5699\n",
      "Epoch: 311, Loss: 4.4478, Train: 0.5224, Test: 0.5698\n",
      "Epoch: 312, Loss: 2.5675, Train: 0.5223, Test: 0.5698\n",
      "Epoch: 313, Loss: 6.6644, Train: 0.5223, Test: 0.5698\n",
      "Epoch: 314, Loss: 2.9881, Train: 0.5223, Test: 0.5697\n",
      "Epoch: 315, Loss: 4.2151, Train: 0.5222, Test: 0.5697\n",
      "Epoch: 316, Loss: 4.7028, Train: 0.5222, Test: 0.5697\n",
      "Epoch: 317, Loss: 3.8047, Train: 0.5222, Test: 0.5696\n",
      "Epoch: 318, Loss: 4.4212, Train: 0.5222, Test: 0.5696\n",
      "Epoch: 319, Loss: 4.7078, Train: 0.5221, Test: 0.5696\n",
      "Epoch: 320, Loss: 5.2151, Train: 0.5221, Test: 0.5695\n",
      "Epoch: 321, Loss: 4.8380, Train: 0.5221, Test: 0.5695\n",
      "Epoch: 322, Loss: 4.6783, Train: 0.5221, Test: 0.5695\n",
      "Epoch: 323, Loss: 5.1182, Train: 0.5220, Test: 0.5694\n",
      "Epoch: 324, Loss: 3.5842, Train: 0.5220, Test: 0.5694\n",
      "Epoch: 325, Loss: 3.7025, Train: 0.5220, Test: 0.5694\n",
      "Epoch: 326, Loss: 6.3755, Train: 0.5220, Test: 0.5693\n",
      "Epoch: 327, Loss: 4.2087, Train: 0.5220, Test: 0.5693\n",
      "Epoch: 328, Loss: 5.1381, Train: 0.5219, Test: 0.5692\n",
      "Epoch: 329, Loss: 4.9583, Train: 0.5219, Test: 0.5692\n",
      "Epoch: 330, Loss: 5.3220, Train: 0.5219, Test: 0.5692\n",
      "Epoch: 331, Loss: 5.2596, Train: 0.5219, Test: 0.5691\n",
      "Epoch: 332, Loss: 5.0434, Train: 0.5219, Test: 0.5691\n",
      "Epoch: 333, Loss: 4.7365, Train: 0.5218, Test: 0.5691\n",
      "Epoch: 334, Loss: 3.9873, Train: 0.5218, Test: 0.5690\n",
      "Epoch: 335, Loss: 4.3246, Train: 0.5218, Test: 0.5690\n",
      "Epoch: 336, Loss: 4.1993, Train: 0.5218, Test: 0.5690\n",
      "Epoch: 337, Loss: 5.1489, Train: 0.5217, Test: 0.5689\n",
      "Epoch: 338, Loss: 4.1462, Train: 0.5217, Test: 0.5689\n",
      "Epoch: 339, Loss: 4.9354, Train: 0.5217, Test: 0.5689\n",
      "Epoch: 340, Loss: 4.6749, Train: 0.5217, Test: 0.5688\n",
      "Epoch: 341, Loss: 4.7063, Train: 0.5217, Test: 0.5688\n",
      "Epoch: 342, Loss: 3.2355, Train: 0.5216, Test: 0.5688\n",
      "Epoch: 343, Loss: 5.3518, Train: 0.5216, Test: 0.5687\n",
      "Epoch: 344, Loss: 3.5604, Train: 0.5216, Test: 0.5687\n",
      "Epoch: 345, Loss: 3.3946, Train: 0.5216, Test: 0.5687\n",
      "Epoch: 346, Loss: 4.6885, Train: 0.5215, Test: 0.5687\n",
      "Epoch: 347, Loss: 3.9813, Train: 0.5215, Test: 0.5686\n",
      "Epoch: 348, Loss: 3.2917, Train: 0.5214, Test: 0.5686\n",
      "Epoch: 349, Loss: 5.2798, Train: 0.5213, Test: 0.5686\n",
      "Epoch: 350, Loss: 3.9292, Train: 0.5212, Test: 0.5685\n",
      "Epoch: 351, Loss: 2.6952, Train: 0.5211, Test: 0.5685\n",
      "Epoch: 352, Loss: 5.2085, Train: 0.5211, Test: 0.5685\n",
      "Epoch: 353, Loss: 3.9954, Train: 0.5210, Test: 0.5684\n",
      "Epoch: 354, Loss: 3.9843, Train: 0.5210, Test: 0.5684\n",
      "Epoch: 355, Loss: 4.8453, Train: 0.5210, Test: 0.5684\n",
      "Epoch: 356, Loss: 4.0026, Train: 0.5209, Test: 0.5683\n",
      "Epoch: 357, Loss: 6.3439, Train: 0.5209, Test: 0.5683\n",
      "Epoch: 358, Loss: 3.7343, Train: 0.5209, Test: 0.5683\n",
      "Epoch: 359, Loss: 3.9178, Train: 0.5209, Test: 0.5682\n",
      "Epoch: 360, Loss: 3.9167, Train: 0.5208, Test: 0.5682\n",
      "Epoch: 361, Loss: 4.9227, Train: 0.5208, Test: 0.5682\n",
      "Epoch: 362, Loss: 4.0787, Train: 0.5208, Test: 0.5681\n",
      "Epoch: 363, Loss: 4.4846, Train: 0.5208, Test: 0.5681\n",
      "Epoch: 364, Loss: 4.9942, Train: 0.5207, Test: 0.5681\n",
      "Epoch: 365, Loss: 4.0876, Train: 0.5207, Test: 0.5680\n",
      "Epoch: 366, Loss: 3.4811, Train: 0.5207, Test: 0.5680\n",
      "Epoch: 367, Loss: 2.9263, Train: 0.5207, Test: 0.5679\n",
      "Epoch: 368, Loss: 4.0968, Train: 0.5207, Test: 0.5679\n",
      "Epoch: 369, Loss: 4.7288, Train: 0.5206, Test: 0.5679\n",
      "Epoch: 370, Loss: 3.6767, Train: 0.5206, Test: 0.5678\n",
      "Epoch: 371, Loss: 3.2006, Train: 0.5206, Test: 0.5678\n",
      "Epoch: 372, Loss: 4.6555, Train: 0.5206, Test: 0.5677\n",
      "Epoch: 373, Loss: 5.0095, Train: 0.5205, Test: 0.5677\n",
      "Epoch: 374, Loss: 3.4225, Train: 0.5205, Test: 0.5676\n",
      "Epoch: 375, Loss: 4.5942, Train: 0.5205, Test: 0.5676\n",
      "Epoch: 376, Loss: 4.8497, Train: 0.5205, Test: 0.5676\n",
      "Epoch: 377, Loss: 2.3988, Train: 0.5204, Test: 0.5675\n",
      "Epoch: 378, Loss: 4.2928, Train: 0.5204, Test: 0.5675\n",
      "Epoch: 379, Loss: 4.1580, Train: 0.5204, Test: 0.5675\n",
      "Epoch: 380, Loss: 3.9602, Train: 0.5204, Test: 0.5674\n",
      "Epoch: 381, Loss: 2.4784, Train: 0.5203, Test: 0.5674\n",
      "Epoch: 382, Loss: 4.3367, Train: 0.5203, Test: 0.5673\n",
      "Epoch: 383, Loss: 4.4176, Train: 0.5203, Test: 0.5673\n",
      "Epoch: 384, Loss: 3.7247, Train: 0.5203, Test: 0.5673\n",
      "Epoch: 385, Loss: 5.1254, Train: 0.5202, Test: 0.5672\n",
      "Epoch: 386, Loss: 4.2127, Train: 0.5202, Test: 0.5672\n",
      "Epoch: 387, Loss: 3.3344, Train: 0.5202, Test: 0.5672\n",
      "Epoch: 388, Loss: 3.9453, Train: 0.5201, Test: 0.5671\n",
      "Epoch: 389, Loss: 5.3650, Train: 0.5201, Test: 0.5671\n",
      "Epoch: 390, Loss: 4.1346, Train: 0.5201, Test: 0.5671\n",
      "Epoch: 391, Loss: 4.4506, Train: 0.5201, Test: 0.5670\n",
      "Epoch: 392, Loss: 4.6295, Train: 0.5200, Test: 0.5670\n",
      "Epoch: 393, Loss: 5.1116, Train: 0.5200, Test: 0.5669\n",
      "Epoch: 394, Loss: 3.2933, Train: 0.5200, Test: 0.5669\n",
      "Epoch: 395, Loss: 3.7417, Train: 0.5200, Test: 0.5669\n",
      "Epoch: 396, Loss: 3.7797, Train: 0.5199, Test: 0.5668\n",
      "Epoch: 397, Loss: 3.8716, Train: 0.5199, Test: 0.5668\n",
      "Epoch: 398, Loss: 4.5977, Train: 0.5199, Test: 0.5668\n",
      "Epoch: 399, Loss: 5.0402, Train: 0.5199, Test: 0.5667\n",
      "Epoch: 400, Loss: 4.2669, Train: 0.5198, Test: 0.5667\n",
      "Epoch: 401, Loss: 3.1659, Train: 0.5198, Test: 0.5667\n",
      "Epoch: 402, Loss: 3.0298, Train: 0.5198, Test: 0.5666\n",
      "Epoch: 403, Loss: 4.6878, Train: 0.5198, Test: 0.5666\n",
      "Epoch: 404, Loss: 3.6404, Train: 0.5197, Test: 0.5665\n",
      "Epoch: 405, Loss: 5.0469, Train: 0.5197, Test: 0.5665\n",
      "Epoch: 406, Loss: 4.9246, Train: 0.5197, Test: 0.5665\n",
      "Epoch: 407, Loss: 4.1768, Train: 0.5197, Test: 0.5664\n",
      "Epoch: 408, Loss: 4.7599, Train: 0.5196, Test: 0.5664\n",
      "Epoch: 409, Loss: 3.2311, Train: 0.5196, Test: 0.5664\n",
      "Epoch: 410, Loss: 3.6665, Train: 0.5196, Test: 0.5664\n",
      "Epoch: 411, Loss: 4.4767, Train: 0.5196, Test: 0.5664\n",
      "Epoch: 412, Loss: 3.8158, Train: 0.5195, Test: 0.5664\n",
      "Epoch: 413, Loss: 4.6012, Train: 0.5195, Test: 0.5665\n",
      "Epoch: 414, Loss: 2.8953, Train: 0.5195, Test: 0.5665\n",
      "Epoch: 415, Loss: 4.0363, Train: 0.5195, Test: 0.5666\n",
      "Epoch: 416, Loss: 4.2351, Train: 0.5195, Test: 0.5666\n",
      "Epoch: 417, Loss: 3.9718, Train: 0.5194, Test: 0.5665\n",
      "Epoch: 418, Loss: 2.8096, Train: 0.5194, Test: 0.5665\n",
      "Epoch: 419, Loss: 4.0366, Train: 0.5194, Test: 0.5665\n",
      "Epoch: 420, Loss: 3.5836, Train: 0.5194, Test: 0.5664\n",
      "Epoch: 421, Loss: 3.6182, Train: 0.5194, Test: 0.5664\n",
      "Epoch: 422, Loss: 3.4816, Train: 0.5193, Test: 0.5664\n",
      "Epoch: 423, Loss: 4.6523, Train: 0.5193, Test: 0.5663\n",
      "Epoch: 424, Loss: 4.2733, Train: 0.5193, Test: 0.5663\n",
      "Epoch: 425, Loss: 4.1529, Train: 0.5193, Test: 0.5662\n",
      "Epoch: 426, Loss: 3.0277, Train: 0.5193, Test: 0.5662\n",
      "Epoch: 427, Loss: 4.1927, Train: 0.5192, Test: 0.5662\n",
      "Epoch: 428, Loss: 2.9216, Train: 0.5192, Test: 0.5661\n",
      "Epoch: 429, Loss: 3.3293, Train: 0.5192, Test: 0.5661\n",
      "Epoch: 430, Loss: 4.3873, Train: 0.5192, Test: 0.5660\n",
      "Epoch: 431, Loss: 3.5781, Train: 0.5191, Test: 0.5660\n",
      "Epoch: 432, Loss: 4.3062, Train: 0.5191, Test: 0.5660\n",
      "Epoch: 433, Loss: 4.0611, Train: 0.5191, Test: 0.5659\n",
      "Epoch: 434, Loss: 4.4570, Train: 0.5191, Test: 0.5659\n",
      "Epoch: 435, Loss: 4.0567, Train: 0.5191, Test: 0.5658\n",
      "Epoch: 436, Loss: 3.4055, Train: 0.5191, Test: 0.5658\n",
      "Epoch: 437, Loss: 3.8062, Train: 0.5190, Test: 0.5658\n",
      "Epoch: 438, Loss: 3.5676, Train: 0.5190, Test: 0.5657\n",
      "Epoch: 439, Loss: 3.8560, Train: 0.5190, Test: 0.5657\n",
      "Epoch: 440, Loss: 4.5945, Train: 0.5190, Test: 0.5656\n",
      "Epoch: 441, Loss: 3.2509, Train: 0.5190, Test: 0.5656\n",
      "Epoch: 442, Loss: 4.1111, Train: 0.5190, Test: 0.5655\n",
      "Epoch: 443, Loss: 4.0181, Train: 0.5190, Test: 0.5655\n",
      "Epoch: 444, Loss: 2.8896, Train: 0.5189, Test: 0.5654\n",
      "Epoch: 445, Loss: 4.0333, Train: 0.5189, Test: 0.5654\n",
      "Epoch: 446, Loss: 3.7395, Train: 0.5189, Test: 0.5653\n",
      "Epoch: 447, Loss: 4.6763, Train: 0.5189, Test: 0.5653\n",
      "Epoch: 448, Loss: 2.8175, Train: 0.5189, Test: 0.5653\n",
      "Epoch: 449, Loss: 6.0188, Train: 0.5189, Test: 0.5653\n",
      "Epoch: 450, Loss: 3.7697, Train: 0.5189, Test: 0.5652\n",
      "Epoch: 451, Loss: 3.5891, Train: 0.5188, Test: 0.5652\n",
      "Epoch: 452, Loss: 4.0240, Train: 0.5188, Test: 0.5651\n",
      "Epoch: 453, Loss: 3.4001, Train: 0.5188, Test: 0.5651\n",
      "Epoch: 454, Loss: 2.3199, Train: 0.5188, Test: 0.5651\n",
      "Epoch: 455, Loss: 3.1956, Train: 0.5188, Test: 0.5650\n",
      "Epoch: 456, Loss: 4.1284, Train: 0.5188, Test: 0.5650\n",
      "Epoch: 457, Loss: 3.1466, Train: 0.5188, Test: 0.5649\n",
      "Epoch: 458, Loss: 3.2192, Train: 0.5187, Test: 0.5649\n",
      "Epoch: 459, Loss: 3.7376, Train: 0.5187, Test: 0.5649\n",
      "Epoch: 460, Loss: 4.0280, Train: 0.5187, Test: 0.5648\n",
      "Epoch: 461, Loss: 2.5009, Train: 0.5187, Test: 0.5648\n",
      "Epoch: 462, Loss: 2.6122, Train: 0.5187, Test: 0.5647\n",
      "Epoch: 463, Loss: 2.4515, Train: 0.5187, Test: 0.5647\n",
      "Epoch: 464, Loss: 3.3318, Train: 0.5187, Test: 0.5647\n",
      "Epoch: 465, Loss: 2.5875, Train: 0.5187, Test: 0.5646\n",
      "Epoch: 466, Loss: 3.4824, Train: 0.5188, Test: 0.5646\n",
      "Epoch: 467, Loss: 3.7388, Train: 0.5188, Test: 0.5646\n",
      "Epoch: 468, Loss: 2.6873, Train: 0.5189, Test: 0.5645\n",
      "Epoch: 469, Loss: 1.7758, Train: 0.5190, Test: 0.5645\n",
      "Epoch: 470, Loss: 3.1637, Train: 0.5191, Test: 0.5645\n",
      "Epoch: 471, Loss: 2.5813, Train: 0.5191, Test: 0.5644\n",
      "Epoch: 472, Loss: 3.0686, Train: 0.5191, Test: 0.5644\n",
      "Epoch: 473, Loss: 4.0296, Train: 0.5191, Test: 0.5644\n",
      "Epoch: 474, Loss: 3.2420, Train: 0.5191, Test: 0.5643\n",
      "Epoch: 475, Loss: 2.9495, Train: 0.5191, Test: 0.5643\n",
      "Epoch: 476, Loss: 2.4570, Train: 0.5191, Test: 0.5642\n",
      "Epoch: 477, Loss: 3.1210, Train: 0.5191, Test: 0.5642\n",
      "Epoch: 478, Loss: 3.0548, Train: 0.5190, Test: 0.5642\n",
      "Epoch: 479, Loss: 3.9617, Train: 0.5190, Test: 0.5641\n",
      "Epoch: 480, Loss: 4.0754, Train: 0.5190, Test: 0.5641\n",
      "Epoch: 481, Loss: 3.8993, Train: 0.5190, Test: 0.5641\n",
      "Epoch: 482, Loss: 2.8595, Train: 0.5190, Test: 0.5640\n",
      "Epoch: 483, Loss: 3.6186, Train: 0.5190, Test: 0.5640\n",
      "Epoch: 484, Loss: 2.8515, Train: 0.5190, Test: 0.5639\n",
      "Epoch: 485, Loss: 2.7680, Train: 0.5189, Test: 0.5639\n",
      "Epoch: 486, Loss: 3.8376, Train: 0.5189, Test: 0.5639\n",
      "Epoch: 487, Loss: 2.7407, Train: 0.5189, Test: 0.5638\n",
      "Epoch: 488, Loss: 3.3235, Train: 0.5189, Test: 0.5638\n",
      "Epoch: 489, Loss: 2.0925, Train: 0.5189, Test: 0.5638\n",
      "Epoch: 490, Loss: 3.3742, Train: 0.5189, Test: 0.5637\n",
      "Epoch: 491, Loss: 2.0002, Train: 0.5188, Test: 0.5637\n",
      "Epoch: 492, Loss: 3.0569, Train: 0.5188, Test: 0.5636\n",
      "Epoch: 493, Loss: 3.5107, Train: 0.5188, Test: 0.5636\n",
      "Epoch: 494, Loss: 3.7632, Train: 0.5188, Test: 0.5636\n",
      "Epoch: 495, Loss: 3.3167, Train: 0.5188, Test: 0.5635\n",
      "Epoch: 496, Loss: 3.8958, Train: 0.5188, Test: 0.5635\n",
      "Epoch: 497, Loss: 3.3102, Train: 0.5188, Test: 0.5635\n",
      "Epoch: 498, Loss: 3.2421, Train: 0.5188, Test: 0.5634\n",
      "Epoch: 499, Loss: 3.4372, Train: 0.5187, Test: 0.5634\n",
      "Epoch: 500, Loss: 2.1074, Train: 0.5187, Test: 0.5633\n",
      "Best Test: 0.5784\n"
     ]
    }
   ],
   "source": [
    "print(train_data.x.tolist()[:20])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data = train_data.to(device)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # heads = 8\n",
    "        dropout = .2\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_channels, out_channels, dropout=dropout)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "num_features = 2\n",
    "hidden_features = 8\n",
    "model = Net(num_features, hidden_features, num_features).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=.0001)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    length = len(train_data.edge_index[0])-pos_edges\n",
    "    neg_edge_index = torch.tensor([[0 for _ in range(length)] for _ in range(2)],dtype=torch.long)\n",
    "    indices = [i for i in range(pos_edges)]\n",
    "    sample = random.sample(indices,length)\n",
    "    for i in range(length):\n",
    "        neg_edge_index[0][i] = train_data.edge_index[0][sample[i]]\n",
    "        neg_edge_index[1][i] = train_data.edge_index[1][sample[i]]\n",
    "    \n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index[:,pos_edges:], neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "\n",
    "    edge_label = torch.tensor([1.0]*length+[0.0]*length,dtype=torch.float)\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "losses = []\n",
    "best_test_auc = 0\n",
    "for epoch in range(1, 501):\n",
    "    loss = train()\n",
    "    #val_auc = test(val_data)\n",
    "    train_auc = test(train_data)\n",
    "    test_auc = test(test_data)\n",
    "    if test_auc > best_test_auc:\n",
    "        best_test_auc = test_auc\n",
    "    # print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "    #       f'Test: {test_auc:.4f}')\n",
    "    losses.append(loss)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_auc:.4f}, Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Best Test: {best_test_auc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64584fae-de30-418f-a7f8-f9b637fd724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 43.9372, Train: 0.4917, Test: 0.5260\n",
      "Epoch: 002, Loss: 19.3505, Train: 0.5028, Test: 0.5314\n",
      "Epoch: 003, Loss: 7.7371, Train: 0.5068, Test: 0.5337\n",
      "Epoch: 004, Loss: 10.8568, Train: 0.5064, Test: 0.5335\n",
      "Epoch: 005, Loss: 36.3203, Train: 0.5052, Test: 0.5333\n",
      "Epoch: 006, Loss: 4.6728, Train: 0.5030, Test: 0.5321\n",
      "Epoch: 007, Loss: 3.1074, Train: 0.5004, Test: 0.5299\n",
      "Epoch: 008, Loss: 4.5942, Train: 0.4985, Test: 0.5276\n",
      "Epoch: 009, Loss: 72.4550, Train: 0.4982, Test: 0.5256\n",
      "Epoch: 010, Loss: 2.9468, Train: 0.4975, Test: 0.5242\n",
      "Epoch: 011, Loss: 6.9976, Train: 0.4975, Test: 0.5234\n",
      "Epoch: 012, Loss: 4.8594, Train: 0.4974, Test: 0.5232\n",
      "Epoch: 013, Loss: 6.5714, Train: 0.4968, Test: 0.5229\n",
      "Epoch: 014, Loss: 5.0014, Train: 0.4960, Test: 0.5214\n",
      "Epoch: 015, Loss: 2.3518, Train: 0.4948, Test: 0.5203\n",
      "Epoch: 016, Loss: 32.5418, Train: 0.4944, Test: 0.5191\n",
      "Epoch: 017, Loss: 2.7960, Train: 0.4947, Test: 0.5181\n",
      "Epoch: 018, Loss: 4.7589, Train: 0.4943, Test: 0.5176\n",
      "Epoch: 019, Loss: 2.4865, Train: 0.4944, Test: 0.5168\n",
      "Epoch: 020, Loss: 4.6288, Train: 0.4921, Test: 0.5163\n",
      "Epoch: 021, Loss: 6.1218, Train: 0.4923, Test: 0.5158\n",
      "Epoch: 022, Loss: 65.8290, Train: 0.4923, Test: 0.5156\n",
      "Epoch: 023, Loss: 3.5257, Train: 0.4927, Test: 0.5153\n",
      "Epoch: 024, Loss: 2.9685, Train: 0.4930, Test: 0.5149\n",
      "Epoch: 025, Loss: 2.3882, Train: 0.4931, Test: 0.5145\n",
      "Epoch: 026, Loss: 23.8178, Train: 0.4932, Test: 0.5144\n",
      "Epoch: 027, Loss: 3.7335, Train: 0.4932, Test: 0.5140\n",
      "Epoch: 028, Loss: 5.3812, Train: 0.4928, Test: 0.5137\n",
      "Epoch: 029, Loss: 2.6002, Train: 0.4938, Test: 0.5135\n",
      "Epoch: 030, Loss: 2.7486, Train: 0.4942, Test: 0.5132\n",
      "Epoch: 031, Loss: 2.1168, Train: 0.4944, Test: 0.5130\n",
      "Epoch: 032, Loss: 16.1733, Train: 0.4945, Test: 0.5128\n",
      "Epoch: 033, Loss: 2.8863, Train: 0.4948, Test: 0.5128\n",
      "Epoch: 034, Loss: 23.1646, Train: 0.4951, Test: 0.5126\n",
      "Epoch: 035, Loss: 4.2967, Train: 0.4952, Test: 0.5126\n",
      "Epoch: 036, Loss: 2.2520, Train: 0.4952, Test: 0.5126\n",
      "Epoch: 037, Loss: 2.2432, Train: 0.4953, Test: 0.5127\n",
      "Epoch: 038, Loss: 2.0901, Train: 0.4956, Test: 0.5127\n",
      "Epoch: 039, Loss: 4.4164, Train: 0.4959, Test: 0.5129\n",
      "Epoch: 040, Loss: 2.7683, Train: 0.4983, Test: 0.5132\n",
      "Epoch: 041, Loss: 2.2224, Train: 0.4990, Test: 0.5135\n",
      "Epoch: 042, Loss: 1.8793, Train: 0.4998, Test: 0.5135\n",
      "Epoch: 043, Loss: 3.4515, Train: 0.5006, Test: 0.5132\n",
      "Epoch: 044, Loss: 38.2568, Train: 0.5008, Test: 0.5130\n",
      "Epoch: 045, Loss: 3.7629, Train: 0.5009, Test: 0.5132\n",
      "Epoch: 046, Loss: 2.9170, Train: 0.5011, Test: 0.5132\n",
      "Epoch: 047, Loss: 2.2059, Train: 0.5016, Test: 0.5136\n",
      "Epoch: 048, Loss: 4.5897, Train: 0.5029, Test: 0.5143\n",
      "Epoch: 049, Loss: 1.4469, Train: 0.5041, Test: 0.5148\n",
      "Epoch: 050, Loss: 1.7079, Train: 0.5058, Test: 0.5149\n",
      "Epoch: 051, Loss: 3.3560, Train: 0.5068, Test: 0.5150\n",
      "Epoch: 052, Loss: 1.1529, Train: 0.5073, Test: 0.5153\n",
      "Epoch: 053, Loss: 1.6136, Train: 0.5076, Test: 0.5155\n",
      "Epoch: 054, Loss: 1.7173, Train: 0.5081, Test: 0.5156\n",
      "Epoch: 055, Loss: 1.5001, Train: 0.5084, Test: 0.5160\n",
      "Epoch: 056, Loss: 6.0013, Train: 0.5086, Test: 0.5160\n",
      "Epoch: 057, Loss: 1.3437, Train: 0.5087, Test: 0.5162\n",
      "Epoch: 058, Loss: 16.1313, Train: 0.5089, Test: 0.5164\n",
      "Epoch: 059, Loss: 1.4820, Train: 0.5089, Test: 0.5165\n",
      "Epoch: 060, Loss: 1.9457, Train: 0.5090, Test: 0.5165\n",
      "Epoch: 061, Loss: 1.8435, Train: 0.5091, Test: 0.5166\n",
      "Epoch: 062, Loss: 2.2440, Train: 0.5092, Test: 0.5165\n",
      "Epoch: 063, Loss: 1.9376, Train: 0.5092, Test: 0.5165\n",
      "Epoch: 064, Loss: 2.7919, Train: 0.5093, Test: 0.5165\n",
      "Epoch: 065, Loss: 1.6070, Train: 0.5094, Test: 0.5166\n",
      "Epoch: 066, Loss: 1.8690, Train: 0.5093, Test: 0.5166\n",
      "Epoch: 067, Loss: 1.6211, Train: 0.5093, Test: 0.5167\n",
      "Epoch: 068, Loss: 1.8472, Train: 0.5091, Test: 0.5167\n",
      "Epoch: 069, Loss: 2.4693, Train: 0.5090, Test: 0.5166\n",
      "Epoch: 070, Loss: 1.4287, Train: 0.5086, Test: 0.5161\n",
      "Epoch: 071, Loss: 8.1881, Train: 0.5085, Test: 0.5160\n",
      "Epoch: 072, Loss: 1.3562, Train: 0.5084, Test: 0.5162\n",
      "Epoch: 073, Loss: 1.2724, Train: 0.5083, Test: 0.5165\n",
      "Epoch: 074, Loss: 2.2982, Train: 0.5084, Test: 0.5167\n",
      "Epoch: 075, Loss: 1.6054, Train: 0.5087, Test: 0.5169\n",
      "Epoch: 076, Loss: 1.4745, Train: 0.5089, Test: 0.5172\n",
      "Epoch: 077, Loss: 10.5033, Train: 0.5089, Test: 0.5174\n",
      "Epoch: 078, Loss: 16.6761, Train: 0.5088, Test: 0.5175\n",
      "Epoch: 079, Loss: 1.5114, Train: 0.5091, Test: 0.5177\n",
      "Epoch: 080, Loss: 1.5521, Train: 0.5092, Test: 0.5179\n",
      "Epoch: 081, Loss: 1.3314, Train: 0.5092, Test: 0.5180\n",
      "Epoch: 082, Loss: 1.8636, Train: 0.5094, Test: 0.5180\n",
      "Epoch: 083, Loss: 4.5404, Train: 0.5094, Test: 0.5180\n",
      "Epoch: 084, Loss: 1.1890, Train: 0.5095, Test: 0.5181\n",
      "Epoch: 085, Loss: 1.1867, Train: 0.5096, Test: 0.5181\n",
      "Epoch: 086, Loss: 1.2790, Train: 0.5095, Test: 0.5181\n",
      "Epoch: 087, Loss: 3.6370, Train: 0.5095, Test: 0.5183\n",
      "Epoch: 088, Loss: 1.5262, Train: 0.5096, Test: 0.5184\n",
      "Epoch: 089, Loss: 1.2518, Train: 0.5096, Test: 0.5183\n",
      "Epoch: 090, Loss: 2.3064, Train: 0.5097, Test: 0.5183\n",
      "Epoch: 091, Loss: 1.1351, Train: 0.5098, Test: 0.5184\n",
      "Epoch: 092, Loss: 1.5609, Train: 0.5099, Test: 0.5185\n",
      "Epoch: 093, Loss: 1.0833, Train: 0.5100, Test: 0.5185\n",
      "Epoch: 094, Loss: 1.4245, Train: 0.5101, Test: 0.5185\n",
      "Epoch: 095, Loss: 3.4716, Train: 0.5101, Test: 0.5187\n",
      "Epoch: 096, Loss: 6.1410, Train: 0.5103, Test: 0.5189\n",
      "Epoch: 097, Loss: 1.6903, Train: 0.5105, Test: 0.5192\n",
      "Epoch: 098, Loss: 1.1906, Train: 0.5107, Test: 0.5195\n",
      "Epoch: 099, Loss: 2.2643, Train: 0.5109, Test: 0.5198\n",
      "Epoch: 100, Loss: 9.8640, Train: 0.5111, Test: 0.5200\n",
      "Epoch: 101, Loss: 1.8427, Train: 0.5113, Test: 0.5201\n",
      "Epoch: 102, Loss: 1.3345, Train: 0.5115, Test: 0.5201\n",
      "Epoch: 103, Loss: 1.1299, Train: 0.5116, Test: 0.5202\n",
      "Epoch: 104, Loss: 1.4904, Train: 0.5117, Test: 0.5202\n",
      "Epoch: 105, Loss: 1.0276, Train: 0.5117, Test: 0.5202\n",
      "Epoch: 106, Loss: 1.0497, Train: 0.5119, Test: 0.5203\n",
      "Epoch: 107, Loss: 1.2278, Train: 0.5122, Test: 0.5204\n",
      "Epoch: 108, Loss: 0.9337, Train: 0.5126, Test: 0.5205\n",
      "Epoch: 109, Loss: 1.9480, Train: 0.5128, Test: 0.5205\n",
      "Epoch: 110, Loss: 7.0550, Train: 0.5129, Test: 0.5207\n",
      "Epoch: 111, Loss: 1.0770, Train: 0.5130, Test: 0.5208\n",
      "Epoch: 112, Loss: 1.4842, Train: 0.5132, Test: 0.5209\n",
      "Epoch: 113, Loss: 1.0456, Train: 0.5135, Test: 0.5210\n",
      "Epoch: 114, Loss: 1.0118, Train: 0.5136, Test: 0.5211\n",
      "Epoch: 115, Loss: 1.6067, Train: 0.5137, Test: 0.5212\n",
      "Epoch: 116, Loss: 1.2096, Train: 0.5138, Test: 0.5213\n",
      "Epoch: 117, Loss: 1.0750, Train: 0.5138, Test: 0.5214\n",
      "Epoch: 118, Loss: 1.0601, Train: 0.5139, Test: 0.5215\n",
      "Epoch: 119, Loss: 1.0174, Train: 0.5139, Test: 0.5216\n",
      "Epoch: 120, Loss: 1.1550, Train: 0.5139, Test: 0.5216\n",
      "Epoch: 121, Loss: 1.0636, Train: 0.5138, Test: 0.5215\n",
      "Epoch: 122, Loss: 1.6312, Train: 0.5138, Test: 0.5215\n",
      "Epoch: 123, Loss: 1.4347, Train: 0.5137, Test: 0.5214\n",
      "Epoch: 124, Loss: 1.0518, Train: 0.5136, Test: 0.5214\n",
      "Epoch: 125, Loss: 0.9967, Train: 0.5135, Test: 0.5213\n",
      "Epoch: 126, Loss: 7.4942, Train: 0.5134, Test: 0.5215\n",
      "Epoch: 127, Loss: 1.0545, Train: 0.5132, Test: 0.5215\n",
      "Epoch: 128, Loss: 1.4389, Train: 0.5128, Test: 0.5216\n",
      "Epoch: 129, Loss: 2.1122, Train: 0.5126, Test: 0.5217\n",
      "Epoch: 130, Loss: 1.0103, Train: 0.5126, Test: 0.5218\n",
      "Epoch: 131, Loss: 1.2518, Train: 0.5125, Test: 0.5219\n",
      "Epoch: 132, Loss: 2.4370, Train: 0.5125, Test: 0.5220\n",
      "Epoch: 133, Loss: 0.9542, Train: 0.5125, Test: 0.5221\n",
      "Epoch: 134, Loss: 1.0676, Train: 0.5125, Test: 0.5222\n",
      "Epoch: 135, Loss: 1.2002, Train: 0.5125, Test: 0.5224\n",
      "Epoch: 136, Loss: 1.5664, Train: 0.5125, Test: 0.5226\n",
      "Epoch: 137, Loss: 1.5699, Train: 0.5126, Test: 0.5229\n",
      "Epoch: 138, Loss: 2.1126, Train: 0.5126, Test: 0.5231\n",
      "Epoch: 139, Loss: 1.2754, Train: 0.5126, Test: 0.5233\n",
      "Epoch: 140, Loss: 1.0988, Train: 0.5127, Test: 0.5235\n",
      "Epoch: 141, Loss: 1.7067, Train: 0.5127, Test: 0.5237\n",
      "Epoch: 142, Loss: 1.2258, Train: 0.5128, Test: 0.5239\n",
      "Epoch: 143, Loss: 0.9839, Train: 0.5128, Test: 0.5241\n",
      "Epoch: 144, Loss: 6.2769, Train: 0.5128, Test: 0.5243\n",
      "Epoch: 145, Loss: 0.9594, Train: 0.5128, Test: 0.5245\n",
      "Epoch: 146, Loss: 0.9415, Train: 0.5128, Test: 0.5248\n",
      "Epoch: 147, Loss: 1.4090, Train: 0.5129, Test: 0.5250\n",
      "Epoch: 148, Loss: 0.9845, Train: 0.5130, Test: 0.5252\n",
      "Epoch: 149, Loss: 1.1166, Train: 0.5131, Test: 0.5254\n",
      "Epoch: 150, Loss: 0.9326, Train: 0.5131, Test: 0.5255\n",
      "Epoch: 151, Loss: 1.5230, Train: 0.5131, Test: 0.5257\n",
      "Epoch: 152, Loss: 1.3020, Train: 0.5131, Test: 0.5258\n",
      "Epoch: 153, Loss: 0.8682, Train: 0.5131, Test: 0.5260\n",
      "Epoch: 154, Loss: 0.8612, Train: 0.5131, Test: 0.5261\n",
      "Epoch: 155, Loss: 0.9056, Train: 0.5131, Test: 0.5262\n",
      "Epoch: 156, Loss: 5.8741, Train: 0.5130, Test: 0.5265\n",
      "Epoch: 157, Loss: 1.2291, Train: 0.5129, Test: 0.5267\n",
      "Epoch: 158, Loss: 0.8923, Train: 0.5126, Test: 0.5269\n",
      "Epoch: 159, Loss: 0.9917, Train: 0.5125, Test: 0.5271\n",
      "Epoch: 160, Loss: 1.1369, Train: 0.5124, Test: 0.5273\n",
      "Epoch: 161, Loss: 0.9575, Train: 0.5124, Test: 0.5275\n",
      "Epoch: 162, Loss: 1.1614, Train: 0.5123, Test: 0.5277\n",
      "Epoch: 163, Loss: 0.9393, Train: 0.5123, Test: 0.5278\n",
      "Epoch: 164, Loss: 0.9061, Train: 0.5122, Test: 0.5280\n",
      "Epoch: 165, Loss: 3.1249, Train: 0.5121, Test: 0.5281\n",
      "Epoch: 166, Loss: 0.8395, Train: 0.5119, Test: 0.5282\n",
      "Epoch: 167, Loss: 4.7072, Train: 0.5118, Test: 0.5283\n",
      "Epoch: 168, Loss: 0.8306, Train: 0.5117, Test: 0.5285\n",
      "Epoch: 169, Loss: 0.8352, Train: 0.5116, Test: 0.5287\n",
      "Epoch: 170, Loss: 0.9082, Train: 0.5114, Test: 0.5288\n",
      "Epoch: 171, Loss: 1.4166, Train: 0.5113, Test: 0.5290\n",
      "Epoch: 172, Loss: 0.9450, Train: 0.5111, Test: 0.5292\n",
      "Epoch: 173, Loss: 4.2901, Train: 0.5110, Test: 0.5296\n",
      "Epoch: 174, Loss: 1.0597, Train: 0.5108, Test: 0.5300\n",
      "Epoch: 175, Loss: 0.8203, Train: 0.5106, Test: 0.5303\n",
      "Epoch: 176, Loss: 0.9670, Train: 0.5104, Test: 0.5306\n",
      "Epoch: 177, Loss: 0.8192, Train: 0.5102, Test: 0.5308\n",
      "Epoch: 178, Loss: 1.3662, Train: 0.5100, Test: 0.5310\n",
      "Epoch: 179, Loss: 0.8986, Train: 0.5100, Test: 0.5312\n",
      "Epoch: 180, Loss: 1.1589, Train: 0.5099, Test: 0.5313\n",
      "Epoch: 181, Loss: 0.8920, Train: 0.5099, Test: 0.5314\n",
      "Epoch: 182, Loss: 0.9562, Train: 0.5097, Test: 0.5314\n",
      "Epoch: 183, Loss: 0.9766, Train: 0.5095, Test: 0.5314\n",
      "Epoch: 184, Loss: 0.8944, Train: 0.5093, Test: 0.5313\n",
      "Epoch: 185, Loss: 1.1786, Train: 0.5089, Test: 0.5312\n",
      "Epoch: 186, Loss: 0.9038, Train: 0.5087, Test: 0.5311\n",
      "Epoch: 187, Loss: 3.5774, Train: 0.5086, Test: 0.5312\n",
      "Epoch: 188, Loss: 0.8801, Train: 0.5086, Test: 0.5312\n",
      "Epoch: 189, Loss: 0.8112, Train: 0.5087, Test: 0.5312\n",
      "Epoch: 190, Loss: 0.8229, Train: 0.5087, Test: 0.5312\n",
      "Epoch: 191, Loss: 0.8022, Train: 0.5089, Test: 0.5312\n",
      "Epoch: 192, Loss: 1.3344, Train: 0.5089, Test: 0.5311\n",
      "Epoch: 193, Loss: 0.7897, Train: 0.5090, Test: 0.5311\n",
      "Epoch: 194, Loss: 3.4969, Train: 0.5089, Test: 0.5312\n",
      "Epoch: 195, Loss: 0.8183, Train: 0.5089, Test: 0.5313\n",
      "Epoch: 196, Loss: 0.9574, Train: 0.5090, Test: 0.5314\n",
      "Epoch: 197, Loss: 0.7911, Train: 0.5091, Test: 0.5314\n",
      "Epoch: 198, Loss: 1.5226, Train: 0.5092, Test: 0.5314\n",
      "Epoch: 199, Loss: 1.1083, Train: 0.5091, Test: 0.5315\n",
      "Epoch: 200, Loss: 0.8485, Train: 0.5091, Test: 0.5315\n",
      "Epoch: 201, Loss: 0.9511, Train: 0.5091, Test: 0.5316\n",
      "Epoch: 202, Loss: 0.8151, Train: 0.5091, Test: 0.5316\n",
      "Epoch: 203, Loss: 0.8230, Train: 0.5091, Test: 0.5316\n",
      "Epoch: 204, Loss: 0.8563, Train: 0.5091, Test: 0.5316\n",
      "Epoch: 205, Loss: 0.8483, Train: 0.5091, Test: 0.5317\n",
      "Epoch: 206, Loss: 1.1040, Train: 0.5091, Test: 0.5317\n",
      "Epoch: 207, Loss: 0.9308, Train: 0.5091, Test: 0.5318\n",
      "Epoch: 208, Loss: 0.8480, Train: 0.5090, Test: 0.5318\n",
      "Epoch: 209, Loss: 0.9275, Train: 0.5091, Test: 0.5318\n",
      "Epoch: 210, Loss: 0.8062, Train: 0.5091, Test: 0.5318\n",
      "Epoch: 211, Loss: 0.8449, Train: 0.5091, Test: 0.5318\n",
      "Epoch: 212, Loss: 0.8133, Train: 0.5091, Test: 0.5318\n",
      "Epoch: 213, Loss: 0.8449, Train: 0.5091, Test: 0.5318\n",
      "Epoch: 214, Loss: 0.8949, Train: 0.5091, Test: 0.5318\n",
      "Epoch: 215, Loss: 0.9377, Train: 0.5090, Test: 0.5318\n",
      "Epoch: 216, Loss: 0.8489, Train: 0.5090, Test: 0.5318\n",
      "Epoch: 217, Loss: 0.8475, Train: 0.5090, Test: 0.5317\n",
      "Epoch: 218, Loss: 0.8152, Train: 0.5090, Test: 0.5317\n",
      "Epoch: 219, Loss: 0.7715, Train: 0.5090, Test: 0.5317\n",
      "Epoch: 220, Loss: 0.7880, Train: 0.5090, Test: 0.5317\n",
      "Epoch: 221, Loss: 0.8345, Train: 0.5090, Test: 0.5317\n",
      "Epoch: 222, Loss: 0.7516, Train: 0.5091, Test: 0.5317\n",
      "Epoch: 223, Loss: 0.9642, Train: 0.5091, Test: 0.5317\n",
      "Epoch: 224, Loss: 1.2509, Train: 0.5091, Test: 0.5317\n",
      "Epoch: 225, Loss: 0.7670, Train: 0.5091, Test: 0.5317\n",
      "Epoch: 226, Loss: 1.0550, Train: 0.5091, Test: 0.5317\n",
      "Epoch: 227, Loss: 1.2020, Train: 0.5090, Test: 0.5318\n",
      "Epoch: 228, Loss: 0.9229, Train: 0.5090, Test: 0.5318\n",
      "Epoch: 229, Loss: 0.8825, Train: 0.5090, Test: 0.5318\n",
      "Epoch: 230, Loss: 0.7610, Train: 0.5090, Test: 0.5317\n",
      "Epoch: 231, Loss: 0.8325, Train: 0.5089, Test: 0.5316\n",
      "Epoch: 232, Loss: 1.6599, Train: 0.5089, Test: 0.5315\n",
      "Epoch: 233, Loss: 0.8120, Train: 0.5088, Test: 0.5314\n",
      "Epoch: 234, Loss: 0.9123, Train: 0.5088, Test: 0.5313\n",
      "Epoch: 235, Loss: 0.9504, Train: 0.5088, Test: 0.5313\n",
      "Epoch: 236, Loss: 0.8118, Train: 0.5087, Test: 0.5312\n",
      "Epoch: 237, Loss: 0.7751, Train: 0.5087, Test: 0.5311\n",
      "Epoch: 238, Loss: 1.5968, Train: 0.5086, Test: 0.5311\n",
      "Epoch: 239, Loss: 0.7943, Train: 0.5086, Test: 0.5311\n",
      "Epoch: 240, Loss: 0.9096, Train: 0.5086, Test: 0.5311\n",
      "Epoch: 241, Loss: 1.0771, Train: 0.5085, Test: 0.5311\n",
      "Epoch: 242, Loss: 1.2436, Train: 0.5085, Test: 0.5310\n",
      "Epoch: 243, Loss: 0.7857, Train: 0.5085, Test: 0.5310\n",
      "Epoch: 244, Loss: 1.9674, Train: 0.5085, Test: 0.5311\n",
      "Epoch: 245, Loss: 0.8663, Train: 0.5085, Test: 0.5312\n",
      "Epoch: 246, Loss: 1.4985, Train: 0.5084, Test: 0.5313\n",
      "Epoch: 247, Loss: 1.3621, Train: 0.5084, Test: 0.5314\n",
      "Epoch: 248, Loss: 0.8648, Train: 0.5084, Test: 0.5316\n",
      "Epoch: 249, Loss: 0.7904, Train: 0.5083, Test: 0.5317\n",
      "Epoch: 250, Loss: 0.8442, Train: 0.5082, Test: 0.5317\n",
      "Epoch: 251, Loss: 0.8177, Train: 0.5081, Test: 0.5318\n",
      "Epoch: 252, Loss: 0.8384, Train: 0.5080, Test: 0.5318\n",
      "Epoch: 253, Loss: 0.7748, Train: 0.5080, Test: 0.5318\n",
      "Epoch: 254, Loss: 0.9338, Train: 0.5079, Test: 0.5318\n",
      "Epoch: 255, Loss: 0.7842, Train: 0.5078, Test: 0.5317\n",
      "Epoch: 256, Loss: 0.8670, Train: 0.5077, Test: 0.5317\n",
      "Epoch: 257, Loss: 0.7562, Train: 0.5077, Test: 0.5318\n",
      "Epoch: 258, Loss: 0.9834, Train: 0.5076, Test: 0.5317\n",
      "Epoch: 259, Loss: 0.9122, Train: 0.5076, Test: 0.5317\n",
      "Epoch: 260, Loss: 0.7512, Train: 0.5075, Test: 0.5316\n",
      "Epoch: 261, Loss: 0.8621, Train: 0.5075, Test: 0.5316\n",
      "Epoch: 262, Loss: 0.8130, Train: 0.5074, Test: 0.5315\n",
      "Epoch: 263, Loss: 0.8773, Train: 0.5073, Test: 0.5314\n",
      "Epoch: 264, Loss: 0.7795, Train: 0.5073, Test: 0.5313\n",
      "Epoch: 265, Loss: 0.7961, Train: 0.5072, Test: 0.5312\n",
      "Epoch: 266, Loss: 0.8127, Train: 0.5072, Test: 0.5312\n",
      "Epoch: 267, Loss: 1.4502, Train: 0.5071, Test: 0.5312\n",
      "Epoch: 268, Loss: 0.7714, Train: 0.5069, Test: 0.5312\n",
      "Epoch: 269, Loss: 1.0229, Train: 0.5067, Test: 0.5312\n",
      "Epoch: 270, Loss: 1.6054, Train: 0.5064, Test: 0.5312\n",
      "Epoch: 271, Loss: 0.7700, Train: 0.5061, Test: 0.5312\n",
      "Epoch: 272, Loss: 0.7650, Train: 0.5058, Test: 0.5313\n",
      "Epoch: 273, Loss: 0.7778, Train: 0.5055, Test: 0.5312\n",
      "Epoch: 274, Loss: 0.8861, Train: 0.5053, Test: 0.5313\n",
      "Epoch: 275, Loss: 0.8455, Train: 0.5052, Test: 0.5313\n",
      "Epoch: 276, Loss: 0.9541, Train: 0.5051, Test: 0.5314\n",
      "Epoch: 277, Loss: 0.8043, Train: 0.5051, Test: 0.5314\n",
      "Epoch: 278, Loss: 1.7376, Train: 0.5050, Test: 0.5316\n",
      "Epoch: 279, Loss: 0.7914, Train: 0.5048, Test: 0.5317\n",
      "Epoch: 280, Loss: 0.7724, Train: 0.5047, Test: 0.5319\n",
      "Epoch: 281, Loss: 0.7599, Train: 0.5046, Test: 0.5320\n",
      "Epoch: 282, Loss: 0.8752, Train: 0.5045, Test: 0.5321\n",
      "Epoch: 283, Loss: 0.8489, Train: 0.5044, Test: 0.5322\n",
      "Epoch: 284, Loss: 0.7833, Train: 0.5043, Test: 0.5323\n",
      "Epoch: 285, Loss: 0.7726, Train: 0.5042, Test: 0.5324\n",
      "Epoch: 286, Loss: 0.8473, Train: 0.5041, Test: 0.5324\n",
      "Epoch: 287, Loss: 0.8051, Train: 0.5041, Test: 0.5324\n",
      "Epoch: 288, Loss: 0.8525, Train: 0.5040, Test: 0.5324\n",
      "Epoch: 289, Loss: 0.7708, Train: 0.5039, Test: 0.5324\n",
      "Epoch: 290, Loss: 0.8986, Train: 0.5038, Test: 0.5323\n",
      "Epoch: 291, Loss: 0.7840, Train: 0.5037, Test: 0.5323\n",
      "Epoch: 292, Loss: 0.7541, Train: 0.5036, Test: 0.5322\n",
      "Epoch: 293, Loss: 0.7746, Train: 0.5036, Test: 0.5322\n",
      "Epoch: 294, Loss: 1.5227, Train: 0.5034, Test: 0.5322\n",
      "Epoch: 295, Loss: 0.7709, Train: 0.5032, Test: 0.5322\n",
      "Epoch: 296, Loss: 0.7912, Train: 0.5030, Test: 0.5323\n",
      "Epoch: 297, Loss: 0.8380, Train: 0.5029, Test: 0.5323\n",
      "Epoch: 298, Loss: 1.3172, Train: 0.5027, Test: 0.5324\n",
      "Epoch: 299, Loss: 0.7446, Train: 0.5024, Test: 0.5325\n",
      "Epoch: 300, Loss: 0.7769, Train: 0.5021, Test: 0.5326\n",
      "Epoch: 301, Loss: 0.9106, Train: 0.5017, Test: 0.5326\n",
      "Epoch: 302, Loss: 0.7452, Train: 0.5013, Test: 0.5327\n",
      "Epoch: 303, Loss: 0.7393, Train: 0.5008, Test: 0.5327\n",
      "Epoch: 304, Loss: 0.7368, Train: 0.5004, Test: 0.5328\n",
      "Epoch: 305, Loss: 0.8617, Train: 0.5002, Test: 0.5328\n",
      "Epoch: 306, Loss: 1.0173, Train: 0.5001, Test: 0.5329\n",
      "Epoch: 307, Loss: 0.7416, Train: 0.4999, Test: 0.5329\n",
      "Epoch: 308, Loss: 0.7526, Train: 0.4998, Test: 0.5329\n",
      "Epoch: 309, Loss: 0.8465, Train: 0.4997, Test: 0.5330\n",
      "Epoch: 310, Loss: 0.7576, Train: 0.4996, Test: 0.5330\n",
      "Epoch: 311, Loss: 0.7478, Train: 0.4995, Test: 0.5330\n",
      "Epoch: 312, Loss: 0.7506, Train: 0.4994, Test: 0.5330\n",
      "Epoch: 313, Loss: 0.7414, Train: 0.4993, Test: 0.5330\n",
      "Epoch: 314, Loss: 1.1095, Train: 0.4992, Test: 0.5330\n",
      "Epoch: 315, Loss: 0.8497, Train: 0.4990, Test: 0.5330\n",
      "Epoch: 316, Loss: 0.8057, Train: 0.4989, Test: 0.5330\n",
      "Epoch: 317, Loss: 0.8099, Train: 0.4988, Test: 0.5329\n",
      "Epoch: 318, Loss: 0.7336, Train: 0.4988, Test: 0.5329\n",
      "Epoch: 319, Loss: 0.7823, Train: 0.4987, Test: 0.5329\n",
      "Epoch: 320, Loss: 0.7295, Train: 0.4986, Test: 0.5328\n",
      "Epoch: 321, Loss: 0.8894, Train: 0.4984, Test: 0.5327\n",
      "Epoch: 322, Loss: 0.7488, Train: 0.4983, Test: 0.5326\n",
      "Epoch: 323, Loss: 0.7368, Train: 0.4982, Test: 0.5325\n",
      "Epoch: 324, Loss: 0.7558, Train: 0.4981, Test: 0.5324\n",
      "Epoch: 325, Loss: 0.7368, Train: 0.4980, Test: 0.5323\n",
      "Epoch: 326, Loss: 0.7598, Train: 0.4980, Test: 0.5322\n",
      "Epoch: 327, Loss: 0.7750, Train: 0.4979, Test: 0.5321\n",
      "Epoch: 328, Loss: 0.7592, Train: 0.4978, Test: 0.5320\n",
      "Epoch: 329, Loss: 0.7455, Train: 0.4978, Test: 0.5319\n",
      "Epoch: 330, Loss: 0.7350, Train: 0.4978, Test: 0.5317\n",
      "Epoch: 331, Loss: 0.7929, Train: 0.4977, Test: 0.5316\n",
      "Epoch: 332, Loss: 0.8160, Train: 0.4976, Test: 0.5315\n",
      "Epoch: 333, Loss: 0.7283, Train: 0.4975, Test: 0.5314\n",
      "Epoch: 334, Loss: 0.7395, Train: 0.4974, Test: 0.5313\n",
      "Epoch: 335, Loss: 0.7329, Train: 0.4972, Test: 0.5312\n",
      "Epoch: 336, Loss: 0.7446, Train: 0.4972, Test: 0.5311\n",
      "Epoch: 337, Loss: 0.7439, Train: 0.4971, Test: 0.5310\n",
      "Epoch: 338, Loss: 0.7534, Train: 0.4971, Test: 0.5309\n",
      "Epoch: 339, Loss: 0.7349, Train: 0.4971, Test: 0.5310\n",
      "Epoch: 340, Loss: 0.7365, Train: 0.4971, Test: 0.5310\n",
      "Epoch: 341, Loss: 0.7900, Train: 0.4971, Test: 0.5310\n",
      "Epoch: 342, Loss: 0.8136, Train: 0.4970, Test: 0.5310\n",
      "Epoch: 343, Loss: 0.7735, Train: 0.4969, Test: 0.5309\n",
      "Epoch: 344, Loss: 1.6634, Train: 0.4962, Test: 0.5310\n",
      "Epoch: 345, Loss: 0.7261, Train: 0.4954, Test: 0.5310\n",
      "Epoch: 346, Loss: 0.7561, Train: 0.4948, Test: 0.5310\n",
      "Epoch: 347, Loss: 0.8052, Train: 0.4941, Test: 0.5310\n",
      "Epoch: 348, Loss: 0.7262, Train: 0.4934, Test: 0.5309\n",
      "Epoch: 349, Loss: 0.7514, Train: 0.4929, Test: 0.5309\n",
      "Epoch: 350, Loss: 0.7299, Train: 0.4923, Test: 0.5309\n",
      "Epoch: 351, Loss: 0.7216, Train: 0.4919, Test: 0.5309\n",
      "Epoch: 352, Loss: 0.7255, Train: 0.4916, Test: 0.5308\n",
      "Epoch: 353, Loss: 0.7903, Train: 0.4912, Test: 0.5308\n",
      "Epoch: 354, Loss: 0.7343, Train: 0.4910, Test: 0.5308\n",
      "Epoch: 355, Loss: 0.8952, Train: 0.4906, Test: 0.5308\n",
      "Epoch: 356, Loss: 1.7437, Train: 0.4892, Test: 0.5309\n",
      "Epoch: 357, Loss: 1.0571, Train: 0.4877, Test: 0.5312\n",
      "Epoch: 358, Loss: 0.7401, Train: 0.4863, Test: 0.5315\n",
      "Epoch: 359, Loss: 0.7394, Train: 0.4851, Test: 0.5316\n",
      "Epoch: 360, Loss: 0.9095, Train: 0.4837, Test: 0.5316\n",
      "Epoch: 361, Loss: 0.7903, Train: 0.4826, Test: 0.5314\n",
      "Epoch: 362, Loss: 0.7314, Train: 0.4815, Test: 0.5312\n",
      "Epoch: 363, Loss: 0.7444, Train: 0.4805, Test: 0.5308\n",
      "Epoch: 364, Loss: 0.7389, Train: 0.4797, Test: 0.5304\n",
      "Epoch: 365, Loss: 0.7608, Train: 0.4788, Test: 0.5301\n",
      "Epoch: 366, Loss: 0.7274, Train: 0.4781, Test: 0.5297\n",
      "Epoch: 367, Loss: 0.7280, Train: 0.4774, Test: 0.5293\n",
      "Epoch: 368, Loss: 0.7342, Train: 0.4768, Test: 0.5290\n",
      "Epoch: 369, Loss: 0.7313, Train: 0.4764, Test: 0.5287\n",
      "Epoch: 370, Loss: 0.7438, Train: 0.4762, Test: 0.5284\n",
      "Epoch: 371, Loss: 0.7388, Train: 0.4760, Test: 0.5282\n",
      "Epoch: 372, Loss: 1.0755, Train: 0.4751, Test: 0.5280\n",
      "Epoch: 373, Loss: 0.7311, Train: 0.4745, Test: 0.5277\n",
      "Epoch: 374, Loss: 0.7478, Train: 0.4740, Test: 0.5274\n",
      "Epoch: 375, Loss: 0.7188, Train: 0.4737, Test: 0.5272\n",
      "Epoch: 376, Loss: 0.7342, Train: 0.4734, Test: 0.5269\n",
      "Epoch: 377, Loss: 0.7193, Train: 0.4732, Test: 0.5268\n",
      "Epoch: 378, Loss: 0.7188, Train: 0.4730, Test: 0.5267\n",
      "Epoch: 379, Loss: 0.7272, Train: 0.4730, Test: 0.5267\n",
      "Epoch: 380, Loss: 0.7283, Train: 0.4730, Test: 0.5268\n",
      "Epoch: 381, Loss: 0.7179, Train: 0.4732, Test: 0.5270\n",
      "Epoch: 382, Loss: 0.7315, Train: 0.4734, Test: 0.5272\n",
      "Epoch: 383, Loss: 0.7153, Train: 0.4736, Test: 0.5274\n",
      "Epoch: 384, Loss: 0.8454, Train: 0.4735, Test: 0.5276\n",
      "Epoch: 385, Loss: 0.7150, Train: 0.4733, Test: 0.5277\n",
      "Epoch: 386, Loss: 0.7215, Train: 0.4733, Test: 0.5278\n",
      "Epoch: 387, Loss: 0.7169, Train: 0.4733, Test: 0.5280\n",
      "Epoch: 388, Loss: 0.7706, Train: 0.4731, Test: 0.5281\n",
      "Epoch: 389, Loss: 0.7569, Train: 0.4730, Test: 0.5285\n",
      "Epoch: 390, Loss: 0.7313, Train: 0.4729, Test: 0.5289\n",
      "Epoch: 391, Loss: 0.7229, Train: 0.4728, Test: 0.5291\n",
      "Epoch: 392, Loss: 0.7141, Train: 0.4728, Test: 0.5293\n",
      "Epoch: 393, Loss: 0.7750, Train: 0.4725, Test: 0.5293\n",
      "Epoch: 394, Loss: 0.7215, Train: 0.4723, Test: 0.5293\n",
      "Epoch: 395, Loss: 0.9307, Train: 0.4713, Test: 0.5289\n",
      "Epoch: 396, Loss: 0.7151, Train: 0.4703, Test: 0.5284\n",
      "Epoch: 397, Loss: 0.7125, Train: 0.4694, Test: 0.5278\n",
      "Epoch: 398, Loss: 0.7164, Train: 0.4685, Test: 0.5273\n",
      "Epoch: 399, Loss: 0.7161, Train: 0.4677, Test: 0.5268\n",
      "Epoch: 400, Loss: 0.7126, Train: 0.4670, Test: 0.5263\n",
      "Epoch: 401, Loss: 0.7361, Train: 0.4662, Test: 0.5257\n",
      "Epoch: 402, Loss: 0.7249, Train: 0.4655, Test: 0.5252\n",
      "Epoch: 403, Loss: 0.7105, Train: 0.4649, Test: 0.5248\n",
      "Epoch: 404, Loss: 0.8494, Train: 0.4637, Test: 0.5240\n",
      "Epoch: 405, Loss: 0.7225, Train: 0.4628, Test: 0.5234\n",
      "Epoch: 406, Loss: 0.7231, Train: 0.4620, Test: 0.5229\n",
      "Epoch: 407, Loss: 0.7275, Train: 0.4615, Test: 0.5227\n",
      "Epoch: 408, Loss: 0.7224, Train: 0.4611, Test: 0.5225\n",
      "Epoch: 409, Loss: 0.7169, Train: 0.4608, Test: 0.5225\n",
      "Epoch: 410, Loss: 0.7221, Train: 0.4606, Test: 0.5225\n",
      "Epoch: 411, Loss: 0.7134, Train: 0.4605, Test: 0.5225\n",
      "Epoch: 412, Loss: 0.7194, Train: 0.4605, Test: 0.5226\n",
      "Epoch: 413, Loss: 0.7168, Train: 0.4605, Test: 0.5228\n",
      "Epoch: 414, Loss: 0.7223, Train: 0.4607, Test: 0.5231\n",
      "Epoch: 415, Loss: 0.7096, Train: 0.4608, Test: 0.5233\n",
      "Epoch: 416, Loss: 0.7161, Train: 0.4611, Test: 0.5236\n",
      "Epoch: 417, Loss: 0.7254, Train: 0.4614, Test: 0.5239\n",
      "Epoch: 418, Loss: 0.7220, Train: 0.4616, Test: 0.5241\n",
      "Epoch: 419, Loss: 0.7116, Train: 0.4616, Test: 0.5242\n",
      "Epoch: 420, Loss: 0.7115, Train: 0.4615, Test: 0.5243\n",
      "Epoch: 421, Loss: 0.7108, Train: 0.4614, Test: 0.5243\n",
      "Epoch: 422, Loss: 0.7149, Train: 0.4615, Test: 0.5244\n",
      "Epoch: 423, Loss: 0.7956, Train: 0.4607, Test: 0.5239\n",
      "Epoch: 424, Loss: 0.7147, Train: 0.4600, Test: 0.5234\n",
      "Epoch: 425, Loss: 0.7102, Train: 0.4594, Test: 0.5230\n",
      "Epoch: 426, Loss: 0.7086, Train: 0.4589, Test: 0.5227\n",
      "Epoch: 427, Loss: 0.7210, Train: 0.4584, Test: 0.5224\n",
      "Epoch: 428, Loss: 0.7224, Train: 0.4580, Test: 0.5221\n",
      "Epoch: 429, Loss: 0.7194, Train: 0.4577, Test: 0.5218\n",
      "Epoch: 430, Loss: 0.7134, Train: 0.4572, Test: 0.5215\n",
      "Epoch: 431, Loss: 0.7115, Train: 0.4569, Test: 0.5212\n",
      "Epoch: 432, Loss: 0.7184, Train: 0.4567, Test: 0.5210\n",
      "Epoch: 433, Loss: 0.7246, Train: 0.4564, Test: 0.5209\n",
      "Epoch: 434, Loss: 0.7254, Train: 0.4562, Test: 0.5208\n",
      "Epoch: 435, Loss: 0.7135, Train: 0.4562, Test: 0.5209\n",
      "Epoch: 436, Loss: 0.7108, Train: 0.4563, Test: 0.5211\n",
      "Epoch: 437, Loss: 0.7103, Train: 0.4564, Test: 0.5212\n",
      "Epoch: 438, Loss: 0.7094, Train: 0.4565, Test: 0.5213\n",
      "Epoch: 439, Loss: 0.7128, Train: 0.4567, Test: 0.5216\n",
      "Epoch: 440, Loss: 0.7110, Train: 0.4568, Test: 0.5218\n",
      "Epoch: 441, Loss: 0.7105, Train: 0.4570, Test: 0.5220\n",
      "Epoch: 442, Loss: 0.7269, Train: 0.4570, Test: 0.5221\n",
      "Epoch: 443, Loss: 0.7157, Train: 0.4570, Test: 0.5222\n",
      "Epoch: 444, Loss: 0.7160, Train: 0.4572, Test: 0.5223\n",
      "Epoch: 445, Loss: 0.7149, Train: 0.4574, Test: 0.5225\n",
      "Epoch: 446, Loss: 0.7094, Train: 0.4576, Test: 0.5226\n",
      "Epoch: 447, Loss: 0.7127, Train: 0.4576, Test: 0.5227\n",
      "Epoch: 448, Loss: 0.7171, Train: 0.4575, Test: 0.5227\n",
      "Epoch: 449, Loss: 0.7165, Train: 0.4573, Test: 0.5226\n",
      "Epoch: 450, Loss: 0.7074, Train: 0.4570, Test: 0.5224\n",
      "Epoch: 451, Loss: 0.7150, Train: 0.4568, Test: 0.5222\n",
      "Epoch: 452, Loss: 0.7080, Train: 0.4566, Test: 0.5221\n",
      "Epoch: 453, Loss: 0.7098, Train: 0.4564, Test: 0.5220\n",
      "Epoch: 454, Loss: 0.7180, Train: 0.4564, Test: 0.5220\n",
      "Epoch: 455, Loss: 0.7117, Train: 0.4565, Test: 0.5222\n",
      "Epoch: 456, Loss: 0.7090, Train: 0.4567, Test: 0.5224\n",
      "Epoch: 457, Loss: 0.7172, Train: 0.4571, Test: 0.5228\n",
      "Epoch: 458, Loss: 0.7202, Train: 0.4573, Test: 0.5229\n",
      "Epoch: 459, Loss: 0.7206, Train: 0.4573, Test: 0.5230\n",
      "Epoch: 460, Loss: 0.7215, Train: 0.4574, Test: 0.5230\n",
      "Epoch: 461, Loss: 0.7157, Train: 0.4573, Test: 0.5230\n",
      "Epoch: 462, Loss: 0.7068, Train: 0.4573, Test: 0.5230\n",
      "Epoch: 463, Loss: 0.7098, Train: 0.4574, Test: 0.5230\n",
      "Epoch: 464, Loss: 0.7092, Train: 0.4574, Test: 0.5231\n",
      "Epoch: 465, Loss: 0.7145, Train: 0.4574, Test: 0.5231\n",
      "Epoch: 466, Loss: 0.7113, Train: 0.4574, Test: 0.5230\n",
      "Epoch: 467, Loss: 0.7134, Train: 0.4575, Test: 0.5231\n",
      "Epoch: 468, Loss: 0.7139, Train: 0.4574, Test: 0.5231\n",
      "Epoch: 469, Loss: 0.7073, Train: 0.4574, Test: 0.5231\n",
      "Epoch: 470, Loss: 0.7093, Train: 0.4573, Test: 0.5231\n",
      "Epoch: 471, Loss: 0.7074, Train: 0.4573, Test: 0.5232\n",
      "Epoch: 472, Loss: 0.7082, Train: 0.4573, Test: 0.5232\n",
      "Epoch: 473, Loss: 0.7103, Train: 0.4573, Test: 0.5233\n",
      "Epoch: 474, Loss: 0.7110, Train: 0.4574, Test: 0.5234\n",
      "Epoch: 475, Loss: 0.7137, Train: 0.4574, Test: 0.5235\n",
      "Epoch: 476, Loss: 0.7181, Train: 0.4573, Test: 0.5235\n",
      "Epoch: 477, Loss: 0.7102, Train: 0.4572, Test: 0.5234\n",
      "Epoch: 478, Loss: 0.7114, Train: 0.4570, Test: 0.5233\n",
      "Epoch: 479, Loss: 0.7119, Train: 0.4569, Test: 0.5233\n",
      "Epoch: 480, Loss: 0.7585, Train: 0.4562, Test: 0.5227\n",
      "Epoch: 481, Loss: 0.7093, Train: 0.4556, Test: 0.5222\n",
      "Epoch: 482, Loss: 0.7120, Train: 0.4551, Test: 0.5217\n",
      "Epoch: 483, Loss: 0.7097, Train: 0.4547, Test: 0.5214\n",
      "Epoch: 484, Loss: 0.7119, Train: 0.4544, Test: 0.5213\n",
      "Epoch: 485, Loss: 0.7117, Train: 0.4543, Test: 0.5213\n",
      "Epoch: 486, Loss: 0.7107, Train: 0.4543, Test: 0.5213\n",
      "Epoch: 487, Loss: 0.7080, Train: 0.4543, Test: 0.5213\n",
      "Epoch: 488, Loss: 0.7084, Train: 0.4543, Test: 0.5213\n",
      "Epoch: 489, Loss: 0.7299, Train: 0.4542, Test: 0.5212\n",
      "Epoch: 490, Loss: 0.7143, Train: 0.4541, Test: 0.5212\n",
      "Epoch: 491, Loss: 0.7162, Train: 0.4540, Test: 0.5212\n",
      "Epoch: 492, Loss: 0.7127, Train: 0.4540, Test: 0.5212\n",
      "Epoch: 493, Loss: 0.7155, Train: 0.4540, Test: 0.5213\n",
      "Epoch: 494, Loss: 0.7198, Train: 0.4538, Test: 0.5212\n",
      "Epoch: 495, Loss: 0.7554, Train: 0.4531, Test: 0.5205\n",
      "Epoch: 496, Loss: 0.7128, Train: 0.4524, Test: 0.5200\n",
      "Epoch: 497, Loss: 0.7171, Train: 0.4517, Test: 0.5194\n",
      "Epoch: 498, Loss: 0.7144, Train: 0.4512, Test: 0.5190\n",
      "Epoch: 499, Loss: 0.7187, Train: 0.4507, Test: 0.5186\n",
      "Epoch: 500, Loss: 0.7582, Train: 0.4495, Test: 0.5175\n",
      "Best Test: 0.5337\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data = train_data.to(device)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels,edge_dim):\n",
    "        super().__init__()\n",
    "        # heads = 8\n",
    "        # droupout?, concat?\n",
    "        dropout = 0.1\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels,edge_dim=edge_dim,dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_channels, out_channels,edge_dim=edge_dim,dropout=dropout)\n",
    "\n",
    "    def encode(self, x, edge_index, edge_attr):\n",
    "        x = self.conv1(x, edge_index, edge_attr=edge_attr).relu()\n",
    "        x = self.conv2(x, edge_index, edge_attr=edge_attr)\n",
    "        return x\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "num_features = 2\n",
    "edge_dim = 2\n",
    "hidden_features = 128\n",
    "out_features = 64\n",
    "model = Net(num_features, hidden_features, out_features,edge_dim).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=.001)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index, train_data.edge_attr)\n",
    "\n",
    "    length = len(train_data.edge_index[0])-pos_edges\n",
    "    neg_edge_index = torch.tensor([[0 for _ in range(length)] for _ in range(2)],dtype=torch.long)\n",
    "    indices = [i for i in range(pos_edges)]\n",
    "    sample = random.sample(indices,length)\n",
    "    for i in range(length):\n",
    "        neg_edge_index[0][i] = train_data.edge_index[0][sample[i]]\n",
    "        neg_edge_index[1][i] = train_data.edge_index[1][sample[i]]\n",
    "    \n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index[:,pos_edges:], neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "\n",
    "    edge_label = torch.tensor([1.0]*length+[0.0]*length,dtype=torch.float)\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index, data.edge_attr)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "\n",
    "best_test_auc = 0\n",
    "for epoch in range(1, 501):\n",
    "    if epoch == 100:\n",
    "        optimizer = torch.optim.Adam(params=model.parameters(), lr=.0001)\n",
    "    loss = train()\n",
    "    train_auc = test(train_data)\n",
    "    test_auc = test(test_data)\n",
    "    if test_auc > best_test_auc:\n",
    "        best_test_auc = test_auc\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_auc:.4f}, Test: {test_auc:.4f}')\n",
    "    # print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "\n",
    "print(f'Best Test: {best_test_auc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index, test_data.edge_attr)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adbaa03-8a13-4409-976f-296e10cf0213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa1b10-4ce9-42b2-b2c3-d283d2798c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
