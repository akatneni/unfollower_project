{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ec1640d-123d-4c76-ba7b-6b2cc1cd5d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from multiprocess import Process, Manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060860fa-6c79-4c94-9131-6455dd4ba228",
   "metadata": {},
   "outputs": [],
   "source": [
    "follower = {}\n",
    "friend = {}\n",
    "e_tweet = {}\n",
    "m_tweet = {}\n",
    "r_tweet = {}\n",
    "\n",
    "def open_files():\n",
    "    with open('./Unfollower/15weeks_follower_dict.pkl', 'rb') as f:\n",
    "        follower = pickle.load(f)\n",
    "\n",
    "    with open('./Unfollower/15weeks_friend_dict.pkl', 'rb') as f:\n",
    "        friend = pickle.load(f)\n",
    "\n",
    "    with open('./Unfollower/e_tweet_dict.pkl', 'rb') as f:\n",
    "        e_tweet = pickle.load(f)\n",
    "\n",
    "    with open('./Unfollower/m_tweet_dict.pkl', 'rb') as f:\n",
    "        m_tweet = pickle.load(f)\n",
    "\n",
    "    with open('./Unfollower/r_tweet_dict.pkl', 'rb') as f:\n",
    "        r_tweet = pickle.load(f)\n",
    "        \n",
    "    return (follower,friend,e_tweet,m_tweet,r_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "970b653a-7657-4c3c-afe6-f68c82e5b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = (0,9)\n",
    "test_range = (10,14)\n",
    "\n",
    "\n",
    "# make a networkx graph in order to find the clustering coefficient of the nodes\n",
    "def getTrainClustering(follower,friend,edge_attr,edge_to_in,return_dict):\n",
    "    G = nx.Graph()\n",
    "    for key in follower:\n",
    "        if len(follower[key][train_range[1]]) == 2:\n",
    "            for f in follower[key][train_range[1]][1]:\n",
    "                G.add_edge(key,f)\n",
    "\n",
    "    for key in friend:\n",
    "        if len(friend[key][train_range[1]]) == 2:\n",
    "            for f in friend[key][train_range[1]][1]:\n",
    "                G.add_edge(f,key)\n",
    "              \n",
    "    print(\"Getting train common neighbors\")\n",
    "    i = 0\n",
    "    for key in edge_to_in:\n",
    "        if edge_attr[edge_to_in[key]][0] == 0.0 and key[0] in G and key[1] in G:\n",
    "            neighbors = sum(1 for _ in nx.common_neighbors(G,key[0],key[1]))\n",
    "            edge_attr[edge_to_in[key]][0] = neighbors\n",
    "            if i < 100:\n",
    "                print(f'{ edge_attr[edge_to_in[key]][0]} {neighbors} {edge_to_in[key]}')\n",
    "            i += 1\n",
    "            if (key[1],key[0]) in edge_to_in:\n",
    "                edge_attr[edge_to_in[(key[1],key[0])]][0] = neighbors\n",
    "    print(\"Getting train clustering\")  \n",
    "    cluster_coeffs = nx.clustering(G)\n",
    "    return_dict['edge_attr1'] = edge_attr\n",
    "    return_dict['cluster_coeffs\n",
    "\n",
    "\n",
    "    # make another for the test set\n",
    "def getTestClustering(follower,friend,edge_attr,edge_to_in,cluster_coeffs):\n",
    "    G2 = nx.Graph()\n",
    "    for key in follower:\n",
    "        if len(follower[key][test_range[1]]) == 2:\n",
    "            for f in follower[key][test_range[1]][1]:\n",
    "                G2.add_edge(key,f)\n",
    "\n",
    "    for key in friend:\n",
    "        if len(friend[key][test_range[1]]) == 2:\n",
    "            for f in friend[key][test_range[1]][1]:\n",
    "                G2.add_edge(f,key)\n",
    "    \n",
    "    print(\"Getting test common neighbors\")\n",
    "    for key in edge_to_in:\n",
    "        if edge_attr[edge_to_in[key]][0] == 0.0 and key[0] in G2 and key[1] in G2:\n",
    "            neighbors = sum(1 for _ in nx.common_neighbors(G2,key[0],key[1]))\n",
    "            edge_attr[edge_to_in[key]][0] = neighbors\n",
    "            if (key[1],key[0]) in edge_to_in:\n",
    "                edge_attr[edge_to_in[(key[1],key[0])]][0] = neighbors\n",
    "                    \n",
    "    print(\"Getting test clustering\")     \n",
    "    cluster_coeffs = nx.clustering(G2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba110697-01a1-4921-9c8a-3d994ba9af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_edges(follower,friend,id_to_in):\n",
    "    nodes1 = []\n",
    "    nodes2 = []\n",
    "\n",
    "    # mapping of edge tuple to index in edge_index tensor\n",
    "    edge_to_in = {}\n",
    "\n",
    "    # Create edge lists for the train Data object\n",
    "    for key in follower:\n",
    "        if len(follower[key][train_range[1]]) == 2:\n",
    "            for f in follower[key][train_range[1]][1]:\n",
    "                edge_to_in[(key,f)] = len(nodes1)\n",
    "                nodes1.append(id_to_in[key])\n",
    "                nodes2.append(id_to_in[f])\n",
    "\n",
    "    for key in friend:\n",
    "        if len(friend[key][train_range[1]]) == 2:\n",
    "            for f in friend[key][train_range[1]][1]:\n",
    "                edge_to_in[(f,key)] = len(nodes1)\n",
    "                nodes1.append(id_to_in[f])\n",
    "                nodes2.append(id_to_in[key])\n",
    "                \n",
    "    edge_label = [0.0]*len(nodes1)\n",
    "    \n",
    "    # find edges that have been removed\n",
    "    for key in follower:\n",
    "        if len(follower[key][train_range[0]]) == 2:\n",
    "            for f in follower[key][train_range[0]][1]:\n",
    "                if (key,f) not in edge_to_in:\n",
    "                    edge_label.append(1.0)\n",
    "                    edge_to_in[(key,f)] = len(nodes1)\n",
    "                    nodes1.append(id_to_in[key])\n",
    "                    nodes2.append(id_to_in[f])\n",
    "                    \n",
    "    for key in friend:\n",
    "        if len(friend[key][train_range[0]]) == 2:\n",
    "            for f in friend[key][train_range[0]][1]:\n",
    "                if (f,key) not in edge_to_in:\n",
    "                    edge_label.append(1.0)\n",
    "                    edge_to_in[(f,key)] = len(nodes1)\n",
    "                    nodes1.append(id_to_in[f])\n",
    "                    nodes2.append(id_to_in[key])\n",
    "                    \n",
    "    return (nodes1,nodes2,edge_label,edge_to_in)\n",
    "\n",
    "def get_test_edges(follower,friend,id_to_in):\n",
    "    nodes3 = []\n",
    "    nodes4 = []\n",
    "    edge_to_in2 = {}\n",
    "\n",
    "\n",
    "    # Create edge lists for the test Data object\n",
    "    for key in follower:\n",
    "        if len(follower[key][test_range[1]]) == 2:\n",
    "            for f in follower[key][test_range[1]][1]:\n",
    "                edge_to_in2[(key,f)] = len(nodes3)\n",
    "                nodes3.append(id_to_in[key])\n",
    "                nodes4.append(id_to_in[f])\n",
    "\n",
    "    for key in friend:\n",
    "        if len(friend[key][test_range[1]]) == 2:\n",
    "            for f in friend[key][test_range[1]][1]:\n",
    "                edge_to_in2[(f,key)] = len(nodes3)\n",
    "                nodes3.append(id_to_in[f])\n",
    "                nodes4.append(id_to_in[key])\n",
    "                \n",
    "    edge_label = [0.0]*len(nodes3)\n",
    "    \n",
    "    # find edges that have been removed\n",
    "    for key in follower:\n",
    "        if len(follower[key][test_range[0]]) == 2:\n",
    "            for f in follower[key][test_range[0]][1]:\n",
    "                if (key,f) not in edge_to_in2:\n",
    "                    edge_label.append(1.0)\n",
    "                    edge_to_in2[(key,f)] = len(nodes3)\n",
    "                    nodes3.append(id_to_in[key])\n",
    "                    nodes4.append(id_to_in[f])\n",
    "                    \n",
    "    for key in friend:\n",
    "        if len(friend[key][test_range[0]]) == 2:\n",
    "            for f in friend[key][test_range[0]][1]:\n",
    "                if (f,key) not in edge_to_in2:\n",
    "                    edge_label.append(1.0)\n",
    "                    edge_to_in2[(f,key)] = len(nodes3)\n",
    "                    nodes3.append(id_to_in[f])\n",
    "                    nodes4.append(id_to_in[key])\n",
    "                \n",
    "    return (nodes3,nodes4,edge_label,edge_to_in2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4686b5b5-93fe-412b-9f50-8882d0fe857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# put clustering coefficients in feature array\n",
    "\n",
    "def add_clustering(x,n,in_to_id,cluster_coeffs):\n",
    "    for i in range(n):\n",
    "        key = str(in_to_id[i])\n",
    "        if key in cluster_coeffs:\n",
    "            x[i][0] = cluster_coeffs[key]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f1fa78d-0efa-42e2-958e-cdb3a90f5ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add number of tweets to feature list x\n",
    "\n",
    "def add_tweets_train(x,n,in_to_id,e_tweet):\n",
    "    for i in range(n):\n",
    "        key = str(in_to_id[i])\n",
    "        x[i][1] = 0\n",
    "        for j in range(train_range[1]):\n",
    "            if key in e_tweet['train'][j+1]:\n",
    "                x[i][1] += len(e_tweet['train'][j+1][key])\n",
    "                temp = 'train'\n",
    "                #print(f'i: {i}, val: {x[i][1]}, adding: {len(e_tweet[temp][j+1][key])}, key: {key}')\n",
    "                \n",
    "def add_tweets_test(x2,n,in_to_id,e_tweet):\n",
    "    for i in range(n):\n",
    "        key = str(in_to_id[i])\n",
    "        x2[i][1] = 0\n",
    "        for j in range(test_range[0],test_range[1]):\n",
    "            if key in e_tweet['test'][j+1]:\n",
    "                x2[i][1] += len(e_tweet['test'][j+1][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34834938-bb28-4ae7-9ed9-cdc67f61ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data():\n",
    "    print(\"loading files\")\n",
    "    follower,friend,e_tweet,m_tweet,r_tweet = open_files()\n",
    "    # make a mapping of the person's id to their index in the node list\n",
    "    # The index will be the index of their x values when making the Data object\n",
    "    id_to_in = {}\n",
    "    in_to_id = list(follower.keys())\n",
    "    i = 0\n",
    "    for key in follower:\n",
    "        id_to_in[key] = i\n",
    "        i += 1\n",
    "        \n",
    "    print(\"getting edges\")\n",
    "    # get edges\n",
    "    nodes1,nodes2,edge_label_train,edge_to_in_train = get_train_edges(follower,friend,id_to_in)\n",
    "    nodes3,nodes4,edge_label_test,edge_to_in_test = get_test_edges(follower,friend,id_to_in)\n",
    "    \n",
    "    # make edge feature arrays\n",
    "    num_edges_train = len(nodes1)\n",
    "    num_edges_test = len(nodes3)\n",
    "    edge_attr1 = [[0.0 for i in range(2)] for j in range(num_edges_train)]\n",
    "    edge_attr2 = [[0.0 for i in range(2)] for j in range(num_edges_test)]\n",
    "    \n",
    "    # calculate the clustering coefficients and common neighbors\n",
    "    m = Manager()\n",
    "    return_dict = m.dict()\n",
    "    p1 = Process(target=getTrainClustering, args=(follower,friend,edge_attr1,edge_to_in_train,return_dict,))\n",
    "    p2 = Process(target=getTestClustering, args=(follower,friend,edge_attr2,edge_to_in_test,return_dict,))\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "    p1.join()\n",
    "    p2.join()\n",
    "    edge_attr1 = return_dict['edge_attr1']\n",
    "    edge_attr2 = return_dict['edge_attr2']\n",
    "    cluster_coeffs = return_dict['cluster_coeffs']\n",
    "    cluster_coeffs2 = return_dict['cluster_coeffs2']\n",
    "    #cluster_coeffs = getTrainClustering(follower,friend,edge_attr1,edge_to_in_train)\n",
    "    #cluster_coeffs2 = getTestClustering(follower,friend,edge_attr2,edge_to_in_test)\n",
    "    \n",
    "    # make node feature arrays\n",
    "    n = len(list(id_to_in.keys()))\n",
    "    x = [[0 for i in range(2)] for j in range(n)]\n",
    "    x2 = [[0 for i in range(2)] for j in range(n)]\n",
    "    \n",
    "    # add clustering coefficients to the feature arrays\n",
    "    add_clustering(x,n,in_to_id,cluster_coeffs)\n",
    "    add_clustering(x2,n,in_to_id,cluster_coeffs2)\n",
    "    \n",
    "    \n",
    "    print(\"adding number of tweets\")\n",
    "    # add number of tweets per user\n",
    "    add_tweets_train(x,n,in_to_id,e_tweet)\n",
    "    add_tweets_test(x2,n,in_to_id,e_tweet)\n",
    "    \n",
    "    x = torch.tensor(x, dtype=torch.float)\n",
    "    x2 = torch.tensor(x2, dtype=torch.float)\n",
    "    edge_index = torch.tensor([nodes1,nodes2], dtype=torch.long)\n",
    "    edge_index2 = torch.tensor([nodes3,nodes4], dtype=torch.long)\n",
    "    n_edges = len(nodes1)\n",
    "    n_edges2 = len(nodes3)\n",
    "    edge_label = torch.tensor(edge_label_train, dtype=torch.float)\n",
    "    edge_label2 = torch.tensor(edge_label_test, dtype=torch.float)\n",
    "    edge_attr1 = torch.tensor(edge_attr1,dtype=torch.float)\n",
    "    edge_attr2 = torch.tensor(edge_attr2,dtype=torch.float)\n",
    "    \n",
    "    train_data = Data(x=x, edge_index=edge_index, edge_label_index=edge_index, edge_label=edge_label,edge_attr=edge_attr1)\n",
    "    test_data = Data(x=x2, edge_index=edge_index2, edge_label_index=edge_index2, edge_label=edge_label2,edge_attr=edge_attr2)\n",
    "    \n",
    "    return (train_data,test_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdb37ef2-084c-4019-bb8f-7b11a8344f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting test common neighborsGetting train common neighbors\n",
      "\n",
      "81 81 4549340\n",
      "100 100 3509584\n",
      "29 29 3477762\n",
      "70 70 3562373\n",
      "115 115 4491426\n",
      "123 123 3185878\n",
      "92 92 3899083\n",
      "154 154 3550315\n",
      "46 46 3645857\n",
      "93 93 3035828\n",
      "142 142 5212096\n",
      "82 82 3979297\n",
      "115 115 3795944\n",
      "151 151 4459619\n",
      "82 82 4135094\n",
      "117 117 3765468\n",
      "168 168 5576432\n",
      "45 45 5186875\n",
      "74 74 5359305\n",
      "85 85 3679611\n",
      "37 37 4117943\n",
      "63 63 4799151\n",
      "2 2 4894086\n",
      "91 91 4277089\n",
      "41 41 5059158\n",
      "59 59 5666512\n",
      "45 45 5026624\n",
      "23 23 3521683\n",
      "147 147 4154632\n",
      "149 149 3380996\n",
      "141 141 5490796\n",
      "51 51 4512620\n",
      "17 17 3868811\n",
      "162 162 4263415\n",
      "104 104 2863543\n",
      "128 128 3438641\n",
      "122 122 4820074\n",
      "122 122 2890419\n",
      "40 40 4616566\n",
      "108 108 4179328\n",
      "117 117 4432031\n",
      "126 126 4437973\n",
      "166 166 4497286\n",
      "84 84 4349337\n",
      "141 141 5604785\n",
      "20 20 3422683\n",
      "127 127 3398598\n",
      "160 160 3845349\n",
      "102 102 3979607\n",
      "113 113 4392224\n",
      "147 147 4373118\n",
      "45 45 4760900\n",
      "73 73 2879415\n",
      "140 140 3359767\n",
      "84 84 4285979\n",
      "135 135 3530807\n",
      "101 101 5423646\n",
      "114 114 4331489\n",
      "136 136 5184057\n",
      "124 124 4668758\n",
      "12 12 3267956\n",
      "58 58 3378731\n",
      "90 90 5062212\n",
      "142 142 3549303\n",
      "76 76 5011656\n",
      "98 98 5420037\n",
      "16 16 5585438\n",
      "105 105 5201800\n",
      "44 44 4693036\n",
      "100 100 4980127\n",
      "48 48 5073684\n",
      "47 47 3546845\n",
      "46 46 4178122\n",
      "19 19 3018757\n",
      "2 2 3556506\n",
      "44 44 4970442\n",
      "85 85 2961815\n",
      "57 57 5620542\n",
      "140 140 5312052\n",
      "105 105 5415677\n",
      "119 119 5042626\n",
      "74 74 4875909\n",
      "136 136 4887646\n",
      "149 149 3778646\n",
      "71 71 3158794\n",
      "58 58 4967417\n",
      "36 36 4155037\n",
      "93 93 2937100\n",
      "135 135 3677989\n",
      "116 116 4913890\n",
      "163 163 3260229\n",
      "160 160 3631662\n",
      "41 41 4965695\n",
      "40 40 5493707\n",
      "120 120 3321303\n",
      "45 45 5659568\n",
      "66 66 5421691\n",
      "65 65 3164834\n",
      "92 92 2949775\n",
      "34 34 4964144\n",
      "Getting train clustering\n",
      "Getting test clustering\n"
     ]
    }
   ],
   "source": [
    "train_data,test_data = make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7251791a-31f0-4514-a9e4-c24ee04d806f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, 0.0], [0.0, 0.0], [0.0, 5.0], [0.0, 9.0], [0.0, 0.0], [0.0, 0.0], [0.0, 7.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 14.0], [0.0, 21.0], [0.0, 0.0], [0.0, 4.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 1.0], [0.0, 0.0], [0.0, 1.0], [0.0, 0.0], [0.0, 10.0], [0.0, 0.0], [0.0, 0.0], [0.0, 4.0], [0.0, 0.0], [0.0, 2.0], [0.0, 0.0], [0.0, 4.0], [0.0, 48.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 6.0], [0.0, 0.0], [0.0, 0.0], [0.0, 189.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 4.0], [0.0, 0.0], [0.0, 4.0], [0.0, 0.0], [0.0, 12.0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_data.x.tolist()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078e1fb-dee9-4e3f-81d6-ea3c977ce52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10000000149011612, 0.0], [1.0, 0.0], [0.4346197247505188, 5.0], [0.11166881769895554, 9.0], [0.0, 0.0], [0.0, 0.0], [0.2554112672805786, 7.0], [0.0, 0.0], [0.20346319675445557, 0.0], [0.4346764385700226, 0.0], [0.0, 14.0], [0.0679602101445198, 21.0], [0.20705881714820862, 0.0], [0.08176100999116898, 4.0], [1.0, 0.0], [0.4000000059604645, 0.0], [0.1428571492433548, 0.0], [0.12946158647537231, 1.0], [0.0, 0.0], [0.07621333748102188, 1.0]]\n",
      "Epoch: 001, Loss: 179.9043, Train: 0.5449, Test: 0.5687\n",
      "Epoch: 002, Loss: 163.1438, Train: 0.5492, Test: 0.5694\n",
      "Epoch: 003, Loss: 148.5969, Train: 0.5507, Test: 0.5709\n",
      "Epoch: 004, Loss: 135.9909, Train: 0.5525, Test: 0.5743\n",
      "Epoch: 005, Loss: 124.5348, Train: 0.5534, Test: 0.5773\n",
      "Epoch: 006, Loss: 114.4591, Train: 0.5544, Test: 0.5800\n",
      "Epoch: 007, Loss: 105.4352, Train: 0.5554, Test: 0.5812\n",
      "Epoch: 008, Loss: 97.2333, Train: 0.5558, Test: 0.5827\n",
      "Epoch: 009, Loss: 89.8420, Train: 0.5561, Test: 0.5840\n",
      "Epoch: 010, Loss: 83.0518, Train: 0.5560, Test: 0.5852\n",
      "Epoch: 011, Loss: 76.7357, Train: 0.5558, Test: 0.5859\n",
      "Epoch: 012, Loss: 71.0787, Train: 0.5555, Test: 0.5863\n",
      "Epoch: 013, Loss: 65.7696, Train: 0.5552, Test: 0.5872\n",
      "Epoch: 014, Loss: 60.8990, Train: 0.5548, Test: 0.5880\n",
      "Epoch: 015, Loss: 56.4452, Train: 0.5540, Test: 0.5887\n",
      "Epoch: 016, Loss: 52.2242, Train: 0.5530, Test: 0.5893\n",
      "Epoch: 017, Loss: 48.4005, Train: 0.5519, Test: 0.5891\n",
      "Epoch: 018, Loss: 44.8878, Train: 0.5490, Test: 0.5903\n",
      "Epoch: 019, Loss: 41.5169, Train: 0.5455, Test: 0.5916\n",
      "Epoch: 020, Loss: 38.3938, Train: 0.5429, Test: 0.5923\n",
      "Epoch: 021, Loss: 35.5023, Train: 0.5410, Test: 0.5935\n",
      "Epoch: 022, Loss: 32.8133, Train: 0.5393, Test: 0.5940\n",
      "Epoch: 023, Loss: 30.2845, Train: 0.5363, Test: 0.5950\n",
      "Epoch: 024, Loss: 27.8871, Train: 0.5346, Test: 0.5954\n",
      "Epoch: 025, Loss: 25.6851, Train: 0.5310, Test: 0.5951\n",
      "Epoch: 026, Loss: 23.6127, Train: 0.5262, Test: 0.5945\n",
      "Epoch: 027, Loss: 21.7003, Train: 0.5221, Test: 0.5928\n",
      "Epoch: 028, Loss: 19.9364, Train: 0.5159, Test: 0.5917\n",
      "Epoch: 029, Loss: 18.2835, Train: 0.5084, Test: 0.5891\n",
      "Epoch: 030, Loss: 16.7015, Train: 0.4964, Test: 0.5810\n",
      "Epoch: 031, Loss: 15.2908, Train: 0.4842, Test: 0.5651\n",
      "Epoch: 032, Loss: 13.9873, Train: 0.4736, Test: 0.5399\n",
      "Epoch: 033, Loss: 12.7808, Train: 0.4653, Test: 0.5031\n",
      "Epoch: 034, Loss: 11.6546, Train: 0.4644, Test: 0.4712\n",
      "Epoch: 035, Loss: 10.6214, Train: 0.4645, Test: 0.4491\n",
      "Epoch: 036, Loss: 9.6854, Train: 0.4640, Test: 0.4342\n",
      "Epoch: 037, Loss: 8.8093, Train: 0.4647, Test: 0.4276\n",
      "Epoch: 038, Loss: 8.0230, Train: 0.4646, Test: 0.4221\n",
      "Epoch: 039, Loss: 7.2879, Train: 0.4639, Test: 0.4171\n",
      "Epoch: 040, Loss: 6.6197, Train: 0.4631, Test: 0.4150\n",
      "Epoch: 041, Loss: 6.0210, Train: 0.4626, Test: 0.4131\n",
      "Epoch: 042, Loss: 5.4714, Train: 0.4608, Test: 0.4104\n",
      "Epoch: 043, Loss: 4.9659, Train: 0.4601, Test: 0.4070\n",
      "Epoch: 044, Loss: 4.5109, Train: 0.4582, Test: 0.4043\n",
      "Epoch: 045, Loss: 4.1000, Train: 0.4562, Test: 0.4011\n",
      "Epoch: 046, Loss: 3.7295, Train: 0.4550, Test: 0.3982\n",
      "Epoch: 047, Loss: 3.3904, Train: 0.4541, Test: 0.3961\n",
      "Epoch: 048, Loss: 3.0841, Train: 0.4530, Test: 0.3957\n",
      "Epoch: 049, Loss: 2.8078, Train: 0.4519, Test: 0.3939\n",
      "Epoch: 050, Loss: 2.5628, Train: 0.4500, Test: 0.3923\n",
      "Epoch: 051, Loss: 2.3385, Train: 0.4476, Test: 0.3916\n",
      "Epoch: 052, Loss: 2.1352, Train: 0.4464, Test: 0.3919\n",
      "Epoch: 053, Loss: 1.9540, Train: 0.4447, Test: 0.3925\n",
      "Epoch: 054, Loss: 1.7924, Train: 0.4437, Test: 0.3916\n",
      "Epoch: 055, Loss: 1.6496, Train: 0.4423, Test: 0.3912\n",
      "Epoch: 056, Loss: 1.5206, Train: 0.4407, Test: 0.3909\n",
      "Epoch: 057, Loss: 1.4104, Train: 0.4396, Test: 0.3909\n",
      "Epoch: 058, Loss: 1.3146, Train: 0.4381, Test: 0.3911\n",
      "Epoch: 059, Loss: 1.2343, Train: 0.4367, Test: 0.3916\n",
      "Epoch: 060, Loss: 1.1664, Train: 0.4350, Test: 0.3921\n",
      "Epoch: 061, Loss: 1.1106, Train: 0.4327, Test: 0.3923\n",
      "Epoch: 062, Loss: 1.0627, Train: 0.4298, Test: 0.3939\n",
      "Epoch: 063, Loss: 1.0215, Train: 0.4277, Test: 0.3955\n",
      "Epoch: 064, Loss: 0.9853, Train: 0.4256, Test: 0.3973\n",
      "Epoch: 065, Loss: 0.9517, Train: 0.4245, Test: 0.4008\n",
      "Epoch: 066, Loss: 0.9204, Train: 0.4220, Test: 0.4049\n",
      "Epoch: 067, Loss: 0.8908, Train: 0.4234, Test: 0.4093\n",
      "Epoch: 068, Loss: 0.8642, Train: 0.4260, Test: 0.4136\n",
      "Epoch: 069, Loss: 0.8426, Train: 0.4282, Test: 0.4181\n",
      "Epoch: 070, Loss: 0.8280, Train: 0.4304, Test: 0.4220\n",
      "Epoch: 071, Loss: 0.8193, Train: 0.4313, Test: 0.4258\n",
      "Epoch: 072, Loss: 0.8148, Train: 0.4312, Test: 0.4294\n",
      "Epoch: 073, Loss: 0.8123, Train: 0.4314, Test: 0.4334\n",
      "Epoch: 074, Loss: 0.8103, Train: 0.4309, Test: 0.4379\n",
      "Epoch: 075, Loss: 0.8087, Train: 0.4297, Test: 0.4431\n",
      "Epoch: 076, Loss: 0.8069, Train: 0.4284, Test: 0.4493\n",
      "Epoch: 077, Loss: 0.8050, Train: 0.4270, Test: 0.4565\n",
      "Epoch: 078, Loss: 0.8031, Train: 0.4248, Test: 0.4639\n",
      "Epoch: 079, Loss: 0.8012, Train: 0.4220, Test: 0.4709\n",
      "Epoch: 080, Loss: 0.7992, Train: 0.4202, Test: 0.4780\n",
      "Epoch: 081, Loss: 0.7971, Train: 0.4195, Test: 0.4854\n",
      "Epoch: 082, Loss: 0.7950, Train: 0.4193, Test: 0.4932\n",
      "Epoch: 083, Loss: 0.7929, Train: 0.4193, Test: 0.5008\n",
      "Epoch: 084, Loss: 0.7906, Train: 0.4198, Test: 0.5088\n",
      "Epoch: 085, Loss: 0.7883, Train: 0.4213, Test: 0.5177\n",
      "Epoch: 086, Loss: 0.7858, Train: 0.4240, Test: 0.5275\n",
      "Epoch: 087, Loss: 0.7833, Train: 0.4279, Test: 0.5377\n",
      "Epoch: 088, Loss: 0.7808, Train: 0.4323, Test: 0.5473\n",
      "Epoch: 089, Loss: 0.7782, Train: 0.4371, Test: 0.5560\n",
      "Epoch: 090, Loss: 0.7755, Train: 0.4417, Test: 0.5636\n",
      "Epoch: 091, Loss: 0.7729, Train: 0.4458, Test: 0.5703\n",
      "Epoch: 092, Loss: 0.7701, Train: 0.4490, Test: 0.5762\n",
      "Epoch: 093, Loss: 0.7674, Train: 0.4517, Test: 0.5812\n",
      "Epoch: 094, Loss: 0.7646, Train: 0.4538, Test: 0.5855\n",
      "Epoch: 095, Loss: 0.7618, Train: 0.4555, Test: 0.5890\n",
      "Epoch: 096, Loss: 0.7591, Train: 0.4568, Test: 0.5919\n",
      "Epoch: 097, Loss: 0.7563, Train: 0.4578, Test: 0.5943\n",
      "Epoch: 098, Loss: 0.7536, Train: 0.4585, Test: 0.5963\n",
      "Epoch: 099, Loss: 0.7509, Train: 0.4589, Test: 0.5977\n",
      "Epoch: 100, Loss: 0.7483, Train: 0.4590, Test: 0.5984\n",
      "Epoch: 101, Loss: 0.7457, Train: 0.4591, Test: 0.5983\n",
      "Epoch: 102, Loss: 0.7432, Train: 0.4597, Test: 0.5979\n",
      "Epoch: 103, Loss: 0.7407, Train: 0.4609, Test: 0.5976\n",
      "Epoch: 104, Loss: 0.7383, Train: 0.4623, Test: 0.5976\n",
      "Epoch: 105, Loss: 0.7361, Train: 0.4639, Test: 0.5975\n",
      "Epoch: 106, Loss: 0.7339, Train: 0.4655, Test: 0.5974\n",
      "Epoch: 107, Loss: 0.7318, Train: 0.4674, Test: 0.5971\n",
      "Epoch: 108, Loss: 0.7298, Train: 0.4695, Test: 0.5968\n",
      "Epoch: 109, Loss: 0.7279, Train: 0.4718, Test: 0.5965\n",
      "Epoch: 110, Loss: 0.7261, Train: 0.4744, Test: 0.5961\n",
      "Epoch: 111, Loss: 0.7245, Train: 0.4772, Test: 0.5955\n",
      "Epoch: 112, Loss: 0.7229, Train: 0.4803, Test: 0.5947\n",
      "Epoch: 113, Loss: 0.7213, Train: 0.4835, Test: 0.5937\n",
      "Epoch: 114, Loss: 0.7199, Train: 0.4868, Test: 0.5926\n",
      "Epoch: 115, Loss: 0.7186, Train: 0.4901, Test: 0.5913\n",
      "Epoch: 116, Loss: 0.7173, Train: 0.4935, Test: 0.5899\n",
      "Epoch: 117, Loss: 0.7162, Train: 0.4968, Test: 0.5885\n",
      "Epoch: 118, Loss: 0.7150, Train: 0.5000, Test: 0.5871\n",
      "Epoch: 119, Loss: 0.7140, Train: 0.5029, Test: 0.5856\n",
      "Epoch: 120, Loss: 0.7130, Train: 0.5057, Test: 0.5842\n",
      "Epoch: 121, Loss: 0.7121, Train: 0.5081, Test: 0.5828\n",
      "Epoch: 122, Loss: 0.7112, Train: 0.5102, Test: 0.5815\n",
      "Epoch: 123, Loss: 0.7104, Train: 0.5121, Test: 0.5802\n",
      "Epoch: 124, Loss: 0.7096, Train: 0.5136, Test: 0.5790\n",
      "Epoch: 125, Loss: 0.7088, Train: 0.5150, Test: 0.5780\n",
      "Epoch: 126, Loss: 0.7081, Train: 0.5160, Test: 0.5771\n",
      "Epoch: 127, Loss: 0.7075, Train: 0.5169, Test: 0.5763\n",
      "Epoch: 128, Loss: 0.7069, Train: 0.5176, Test: 0.5756\n",
      "Epoch: 129, Loss: 0.7063, Train: 0.5182, Test: 0.5752\n",
      "Epoch: 130, Loss: 0.7057, Train: 0.5186, Test: 0.5748\n",
      "Epoch: 131, Loss: 0.7052, Train: 0.5188, Test: 0.5746\n",
      "Epoch: 132, Loss: 0.7047, Train: 0.5190, Test: 0.5744\n",
      "Epoch: 133, Loss: 0.7042, Train: 0.5190, Test: 0.5744\n",
      "Epoch: 134, Loss: 0.7038, Train: 0.5189, Test: 0.5745\n",
      "Epoch: 135, Loss: 0.7034, Train: 0.5188, Test: 0.5747\n",
      "Epoch: 136, Loss: 0.7030, Train: 0.5186, Test: 0.5749\n",
      "Epoch: 137, Loss: 0.7027, Train: 0.5183, Test: 0.5752\n",
      "Epoch: 138, Loss: 0.7023, Train: 0.5180, Test: 0.5755\n",
      "Epoch: 139, Loss: 0.7020, Train: 0.5176, Test: 0.5758\n",
      "Epoch: 140, Loss: 0.7018, Train: 0.5172, Test: 0.5762\n",
      "Epoch: 141, Loss: 0.7015, Train: 0.5167, Test: 0.5765\n",
      "Epoch: 142, Loss: 0.7013, Train: 0.5163, Test: 0.5769\n",
      "Epoch: 143, Loss: 0.7010, Train: 0.5158, Test: 0.5773\n",
      "Epoch: 144, Loss: 0.7009, Train: 0.5154, Test: 0.5776\n",
      "Epoch: 145, Loss: 0.7007, Train: 0.5149, Test: 0.5779\n",
      "Epoch: 146, Loss: 0.7005, Train: 0.5144, Test: 0.5782\n",
      "Epoch: 147, Loss: 0.7004, Train: 0.5140, Test: 0.5785\n",
      "Epoch: 148, Loss: 0.7002, Train: 0.5136, Test: 0.5788\n",
      "Epoch: 149, Loss: 0.7001, Train: 0.5132, Test: 0.5790\n",
      "Epoch: 150, Loss: 0.7000, Train: 0.5128, Test: 0.5792\n",
      "Epoch: 151, Loss: 0.6999, Train: 0.5125, Test: 0.5794\n",
      "Epoch: 152, Loss: 0.6998, Train: 0.5122, Test: 0.5795\n",
      "Epoch: 153, Loss: 0.6998, Train: 0.5119, Test: 0.5797\n",
      "Epoch: 154, Loss: 0.6997, Train: 0.5116, Test: 0.5798\n",
      "Epoch: 155, Loss: 0.6996, Train: 0.5114, Test: 0.5799\n",
      "Epoch: 156, Loss: 0.6996, Train: 0.5112, Test: 0.5799\n",
      "Epoch: 157, Loss: 0.6996, Train: 0.5110, Test: 0.5800\n",
      "Epoch: 158, Loss: 0.6995, Train: 0.5109, Test: 0.5800\n",
      "Epoch: 159, Loss: 0.6995, Train: 0.5108, Test: 0.5800\n",
      "Epoch: 160, Loss: 0.6994, Train: 0.5107, Test: 0.5800\n",
      "Epoch: 161, Loss: 0.6994, Train: 0.5107, Test: 0.5799\n",
      "Epoch: 162, Loss: 0.6994, Train: 0.5106, Test: 0.5799\n",
      "Epoch: 163, Loss: 0.6993, Train: 0.5106, Test: 0.5799\n",
      "Epoch: 164, Loss: 0.6993, Train: 0.5106, Test: 0.5798\n",
      "Epoch: 165, Loss: 0.6993, Train: 0.5106, Test: 0.5797\n",
      "Epoch: 166, Loss: 0.6993, Train: 0.5106, Test: 0.5797\n",
      "Epoch: 167, Loss: 0.6993, Train: 0.5106, Test: 0.5796\n",
      "Epoch: 168, Loss: 0.6992, Train: 0.5107, Test: 0.5795\n",
      "Epoch: 169, Loss: 0.6992, Train: 0.5107, Test: 0.5795\n",
      "Epoch: 170, Loss: 0.6992, Train: 0.5107, Test: 0.5794\n",
      "Epoch: 171, Loss: 0.6992, Train: 0.5108, Test: 0.5793\n",
      "Epoch: 172, Loss: 0.6992, Train: 0.5108, Test: 0.5792\n",
      "Epoch: 173, Loss: 0.6991, Train: 0.5108, Test: 0.5792\n",
      "Epoch: 174, Loss: 0.6991, Train: 0.5109, Test: 0.5791\n",
      "Epoch: 175, Loss: 0.6991, Train: 0.5109, Test: 0.5791\n",
      "Epoch: 176, Loss: 0.6991, Train: 0.5109, Test: 0.5790\n",
      "Epoch: 177, Loss: 0.6991, Train: 0.5109, Test: 0.5790\n",
      "Epoch: 178, Loss: 0.6991, Train: 0.5109, Test: 0.5790\n",
      "Epoch: 179, Loss: 0.6991, Train: 0.5109, Test: 0.5789\n",
      "Epoch: 180, Loss: 0.6990, Train: 0.5109, Test: 0.5789\n",
      "Epoch: 181, Loss: 0.6990, Train: 0.5109, Test: 0.5789\n",
      "Epoch: 182, Loss: 0.6990, Train: 0.5108, Test: 0.5789\n",
      "Epoch: 183, Loss: 0.6990, Train: 0.5108, Test: 0.5789\n",
      "Epoch: 184, Loss: 0.6990, Train: 0.5107, Test: 0.5789\n",
      "Epoch: 185, Loss: 0.6989, Train: 0.5107, Test: 0.5789\n",
      "Epoch: 186, Loss: 0.6989, Train: 0.5106, Test: 0.5790\n",
      "Epoch: 187, Loss: 0.6989, Train: 0.5106, Test: 0.5790\n",
      "Epoch: 188, Loss: 0.6989, Train: 0.5105, Test: 0.5790\n",
      "Epoch: 189, Loss: 0.6989, Train: 0.5104, Test: 0.5791\n",
      "Epoch: 190, Loss: 0.6989, Train: 0.5103, Test: 0.5791\n",
      "Epoch: 191, Loss: 0.6988, Train: 0.5102, Test: 0.5792\n",
      "Epoch: 192, Loss: 0.6988, Train: 0.5101, Test: 0.5793\n",
      "Epoch: 193, Loss: 0.6988, Train: 0.5100, Test: 0.5793\n",
      "Epoch: 194, Loss: 0.6988, Train: 0.5099, Test: 0.5794\n",
      "Epoch: 195, Loss: 0.6988, Train: 0.5098, Test: 0.5794\n",
      "Epoch: 196, Loss: 0.6987, Train: 0.5097, Test: 0.5795\n",
      "Epoch: 197, Loss: 0.6987, Train: 0.5096, Test: 0.5796\n",
      "Epoch: 198, Loss: 0.6987, Train: 0.5094, Test: 0.5796\n",
      "Epoch: 199, Loss: 0.6987, Train: 0.5093, Test: 0.5797\n",
      "Epoch: 200, Loss: 0.6987, Train: 0.5092, Test: 0.5798\n",
      "Epoch: 201, Loss: 0.6987, Train: 0.5091, Test: 0.5798\n",
      "Epoch: 202, Loss: 0.6986, Train: 0.5090, Test: 0.5799\n",
      "Epoch: 203, Loss: 0.6986, Train: 0.5089, Test: 0.5800\n",
      "Epoch: 204, Loss: 0.6986, Train: 0.5088, Test: 0.5800\n",
      "Epoch: 205, Loss: 0.6986, Train: 0.5086, Test: 0.5801\n",
      "Epoch: 206, Loss: 0.6986, Train: 0.5085, Test: 0.5802\n",
      "Epoch: 207, Loss: 0.6985, Train: 0.5084, Test: 0.5802\n",
      "Epoch: 208, Loss: 0.6985, Train: 0.5083, Test: 0.5803\n",
      "Epoch: 209, Loss: 0.6985, Train: 0.5082, Test: 0.5803\n",
      "Epoch: 210, Loss: 0.6985, Train: 0.5081, Test: 0.5804\n",
      "Epoch: 211, Loss: 0.6985, Train: 0.5080, Test: 0.5804\n",
      "Epoch: 212, Loss: 0.6985, Train: 0.5079, Test: 0.5805\n",
      "Epoch: 213, Loss: 0.6984, Train: 0.5078, Test: 0.5805\n",
      "Epoch: 214, Loss: 0.6984, Train: 0.5077, Test: 0.5806\n",
      "Epoch: 215, Loss: 0.6984, Train: 0.5076, Test: 0.5806\n",
      "Epoch: 216, Loss: 0.6984, Train: 0.5076, Test: 0.5807\n",
      "Epoch: 217, Loss: 0.6984, Train: 0.5075, Test: 0.5807\n",
      "Epoch: 218, Loss: 0.6984, Train: 0.5074, Test: 0.5807\n",
      "Epoch: 219, Loss: 0.6984, Train: 0.5073, Test: 0.5808\n",
      "Epoch: 220, Loss: 0.6983, Train: 0.5072, Test: 0.5808\n",
      "Epoch: 221, Loss: 0.6983, Train: 0.5071, Test: 0.5809\n",
      "Epoch: 222, Loss: 0.6983, Train: 0.5070, Test: 0.5809\n",
      "Epoch: 223, Loss: 0.6983, Train: 0.5070, Test: 0.5809\n",
      "Epoch: 224, Loss: 0.6983, Train: 0.5069, Test: 0.5810\n",
      "Epoch: 225, Loss: 0.6983, Train: 0.5068, Test: 0.5810\n",
      "Epoch: 226, Loss: 0.6983, Train: 0.5067, Test: 0.5810\n",
      "Epoch: 227, Loss: 0.6982, Train: 0.5067, Test: 0.5810\n",
      "Epoch: 228, Loss: 0.6982, Train: 0.5066, Test: 0.5811\n",
      "Epoch: 229, Loss: 0.6982, Train: 0.5065, Test: 0.5811\n",
      "Epoch: 230, Loss: 0.6982, Train: 0.5064, Test: 0.5811\n",
      "Epoch: 231, Loss: 0.6982, Train: 0.5064, Test: 0.5812\n",
      "Epoch: 232, Loss: 0.6982, Train: 0.5063, Test: 0.5812\n",
      "Epoch: 233, Loss: 0.6981, Train: 0.5062, Test: 0.5812\n",
      "Epoch: 234, Loss: 0.6981, Train: 0.5061, Test: 0.5813\n",
      "Epoch: 235, Loss: 0.6981, Train: 0.5060, Test: 0.5813\n",
      "Epoch: 236, Loss: 0.6981, Train: 0.5060, Test: 0.5813\n",
      "Epoch: 237, Loss: 0.6981, Train: 0.5059, Test: 0.5813\n",
      "Epoch: 238, Loss: 0.6981, Train: 0.5058, Test: 0.5814\n",
      "Epoch: 239, Loss: 0.6981, Train: 0.5057, Test: 0.5814\n",
      "Epoch: 240, Loss: 0.6980, Train: 0.5057, Test: 0.5814\n",
      "Epoch: 241, Loss: 0.6980, Train: 0.5056, Test: 0.5815\n",
      "Epoch: 242, Loss: 0.6980, Train: 0.5055, Test: 0.5815\n",
      "Epoch: 243, Loss: 0.6980, Train: 0.5054, Test: 0.5815\n",
      "Epoch: 244, Loss: 0.6980, Train: 0.5054, Test: 0.5815\n",
      "Epoch: 245, Loss: 0.6980, Train: 0.5053, Test: 0.5816\n",
      "Epoch: 246, Loss: 0.6979, Train: 0.5052, Test: 0.5816\n",
      "Epoch: 247, Loss: 0.6979, Train: 0.5051, Test: 0.5816\n",
      "Epoch: 248, Loss: 0.6979, Train: 0.5051, Test: 0.5816\n",
      "Epoch: 249, Loss: 0.6979, Train: 0.5050, Test: 0.5817\n",
      "Epoch: 250, Loss: 0.6979, Train: 0.5049, Test: 0.5817\n",
      "Epoch: 251, Loss: 0.6979, Train: 0.5048, Test: 0.5817\n",
      "Epoch: 252, Loss: 0.6979, Train: 0.5048, Test: 0.5818\n",
      "Epoch: 253, Loss: 0.6978, Train: 0.5047, Test: 0.5818\n",
      "Epoch: 254, Loss: 0.6978, Train: 0.5046, Test: 0.5818\n",
      "Epoch: 255, Loss: 0.6978, Train: 0.5045, Test: 0.5818\n",
      "Epoch: 256, Loss: 0.6978, Train: 0.5045, Test: 0.5819\n",
      "Epoch: 257, Loss: 0.6978, Train: 0.5044, Test: 0.5819\n",
      "Epoch: 258, Loss: 0.6978, Train: 0.5043, Test: 0.5819\n",
      "Epoch: 259, Loss: 0.6978, Train: 0.5042, Test: 0.5819\n",
      "Epoch: 260, Loss: 0.6978, Train: 0.5042, Test: 0.5820\n",
      "Epoch: 261, Loss: 0.6978, Train: 0.5041, Test: 0.5820\n",
      "Epoch: 262, Loss: 0.6977, Train: 0.5040, Test: 0.5820\n",
      "Epoch: 263, Loss: 0.6977, Train: 0.5039, Test: 0.5820\n",
      "Epoch: 264, Loss: 0.6977, Train: 0.5039, Test: 0.5821\n",
      "Epoch: 265, Loss: 0.6977, Train: 0.5038, Test: 0.5821\n",
      "Epoch: 266, Loss: 0.6977, Train: 0.5037, Test: 0.5821\n",
      "Epoch: 267, Loss: 0.6977, Train: 0.5036, Test: 0.5821\n",
      "Epoch: 268, Loss: 0.6977, Train: 0.5036, Test: 0.5822\n",
      "Epoch: 269, Loss: 0.6976, Train: 0.5035, Test: 0.5822\n",
      "Epoch: 270, Loss: 0.6976, Train: 0.5034, Test: 0.5822\n",
      "Epoch: 271, Loss: 0.6976, Train: 0.5033, Test: 0.5823\n",
      "Epoch: 272, Loss: 0.6976, Train: 0.5033, Test: 0.5823\n",
      "Epoch: 273, Loss: 0.6976, Train: 0.5032, Test: 0.5823\n",
      "Epoch: 274, Loss: 0.6976, Train: 0.5031, Test: 0.5823\n",
      "Epoch: 275, Loss: 0.6976, Train: 0.5030, Test: 0.5824\n",
      "Epoch: 276, Loss: 0.6975, Train: 0.5030, Test: 0.5824\n",
      "Epoch: 277, Loss: 0.6975, Train: 0.5029, Test: 0.5824\n",
      "Epoch: 278, Loss: 0.6975, Train: 0.5028, Test: 0.5824\n",
      "Epoch: 279, Loss: 0.6975, Train: 0.5028, Test: 0.5825\n",
      "Epoch: 280, Loss: 0.6975, Train: 0.5027, Test: 0.5825\n",
      "Epoch: 281, Loss: 0.6975, Train: 0.5026, Test: 0.5825\n",
      "Epoch: 282, Loss: 0.6975, Train: 0.5025, Test: 0.5825\n",
      "Epoch: 283, Loss: 0.6975, Train: 0.5025, Test: 0.5826\n",
      "Epoch: 284, Loss: 0.6974, Train: 0.5024, Test: 0.5826\n",
      "Epoch: 285, Loss: 0.6974, Train: 0.5023, Test: 0.5826\n",
      "Epoch: 286, Loss: 0.6974, Train: 0.5023, Test: 0.5826\n",
      "Epoch: 287, Loss: 0.6974, Train: 0.5022, Test: 0.5827\n",
      "Epoch: 288, Loss: 0.6974, Train: 0.5021, Test: 0.5827\n",
      "Epoch: 289, Loss: 0.6974, Train: 0.5021, Test: 0.5827\n",
      "Epoch: 290, Loss: 0.6974, Train: 0.5020, Test: 0.5827\n",
      "Epoch: 291, Loss: 0.6974, Train: 0.5019, Test: 0.5828\n",
      "Epoch: 292, Loss: 0.6974, Train: 0.5019, Test: 0.5828\n",
      "Epoch: 293, Loss: 0.6973, Train: 0.5018, Test: 0.5828\n",
      "Epoch: 294, Loss: 0.6973, Train: 0.5017, Test: 0.5828\n",
      "Epoch: 295, Loss: 0.6973, Train: 0.5016, Test: 0.5829\n",
      "Epoch: 296, Loss: 0.6973, Train: 0.5016, Test: 0.5829\n",
      "Epoch: 297, Loss: 0.6973, Train: 0.5015, Test: 0.5829\n",
      "Epoch: 298, Loss: 0.6973, Train: 0.5014, Test: 0.5829\n",
      "Epoch: 299, Loss: 0.6973, Train: 0.5014, Test: 0.5830\n",
      "Epoch: 300, Loss: 0.6973, Train: 0.5013, Test: 0.5830\n",
      "Epoch: 301, Loss: 0.6972, Train: 0.5012, Test: 0.5830\n",
      "Epoch: 302, Loss: 0.6972, Train: 0.5012, Test: 0.5830\n",
      "Epoch: 303, Loss: 0.6972, Train: 0.5011, Test: 0.5831\n",
      "Epoch: 304, Loss: 0.6972, Train: 0.5010, Test: 0.5831\n",
      "Epoch: 305, Loss: 0.6972, Train: 0.5009, Test: 0.5831\n",
      "Epoch: 306, Loss: 0.6972, Train: 0.5009, Test: 0.5831\n",
      "Epoch: 307, Loss: 0.6972, Train: 0.5008, Test: 0.5832\n",
      "Epoch: 308, Loss: 0.6972, Train: 0.5007, Test: 0.5832\n",
      "Epoch: 309, Loss: 0.6971, Train: 0.5007, Test: 0.5832\n",
      "Epoch: 310, Loss: 0.6971, Train: 0.5006, Test: 0.5832\n",
      "Epoch: 311, Loss: 0.6971, Train: 0.5005, Test: 0.5833\n",
      "Epoch: 312, Loss: 0.6971, Train: 0.5005, Test: 0.5833\n",
      "Epoch: 313, Loss: 0.6971, Train: 0.5004, Test: 0.5833\n",
      "Epoch: 314, Loss: 0.6971, Train: 0.5003, Test: 0.5833\n",
      "Epoch: 315, Loss: 0.6971, Train: 0.5002, Test: 0.5834\n",
      "Epoch: 316, Loss: 0.6971, Train: 0.5002, Test: 0.5834\n",
      "Epoch: 317, Loss: 0.6971, Train: 0.5001, Test: 0.5834\n",
      "Epoch: 318, Loss: 0.6971, Train: 0.5000, Test: 0.5834\n",
      "Epoch: 319, Loss: 0.6970, Train: 0.5000, Test: 0.5835\n",
      "Epoch: 320, Loss: 0.6970, Train: 0.4999, Test: 0.5835\n",
      "Epoch: 321, Loss: 0.6970, Train: 0.4998, Test: 0.5835\n",
      "Epoch: 322, Loss: 0.6970, Train: 0.4998, Test: 0.5835\n",
      "Epoch: 323, Loss: 0.6970, Train: 0.4997, Test: 0.5836\n",
      "Epoch: 324, Loss: 0.6970, Train: 0.4996, Test: 0.5836\n",
      "Epoch: 325, Loss: 0.6970, Train: 0.4996, Test: 0.5836\n",
      "Epoch: 326, Loss: 0.6970, Train: 0.4995, Test: 0.5836\n",
      "Epoch: 327, Loss: 0.6969, Train: 0.4994, Test: 0.5836\n",
      "Epoch: 328, Loss: 0.6970, Train: 0.4994, Test: 0.5837\n",
      "Epoch: 329, Loss: 0.6969, Train: 0.4993, Test: 0.5837\n",
      "Epoch: 330, Loss: 0.6969, Train: 0.4992, Test: 0.5837\n",
      "Epoch: 331, Loss: 0.6969, Train: 0.4992, Test: 0.5837\n",
      "Epoch: 332, Loss: 0.6969, Train: 0.4991, Test: 0.5838\n",
      "Epoch: 333, Loss: 0.6969, Train: 0.4990, Test: 0.5838\n",
      "Epoch: 334, Loss: 0.6969, Train: 0.4990, Test: 0.5838\n",
      "Epoch: 335, Loss: 0.6969, Train: 0.4989, Test: 0.5838\n",
      "Epoch: 336, Loss: 0.6969, Train: 0.4989, Test: 0.5838\n",
      "Epoch: 337, Loss: 0.6969, Train: 0.4988, Test: 0.5839\n",
      "Epoch: 338, Loss: 0.6968, Train: 0.4987, Test: 0.5839\n",
      "Epoch: 339, Loss: 0.6968, Train: 0.4987, Test: 0.5839\n",
      "Epoch: 340, Loss: 0.6968, Train: 0.4986, Test: 0.5839\n",
      "Epoch: 341, Loss: 0.6968, Train: 0.4985, Test: 0.5839\n",
      "Epoch: 342, Loss: 0.6968, Train: 0.4985, Test: 0.5840\n",
      "Epoch: 343, Loss: 0.6968, Train: 0.4984, Test: 0.5840\n",
      "Epoch: 344, Loss: 0.6968, Train: 0.4983, Test: 0.5840\n",
      "Epoch: 345, Loss: 0.6968, Train: 0.4983, Test: 0.5840\n",
      "Epoch: 346, Loss: 0.6968, Train: 0.4982, Test: 0.5841\n",
      "Epoch: 347, Loss: 0.6968, Train: 0.4981, Test: 0.5841\n",
      "Epoch: 348, Loss: 0.6967, Train: 0.4981, Test: 0.5841\n",
      "Epoch: 349, Loss: 0.6967, Train: 0.4980, Test: 0.5841\n",
      "Epoch: 350, Loss: 0.6967, Train: 0.4979, Test: 0.5841\n",
      "Epoch: 351, Loss: 0.6967, Train: 0.4979, Test: 0.5842\n",
      "Epoch: 352, Loss: 0.6967, Train: 0.4978, Test: 0.5842\n",
      "Epoch: 353, Loss: 0.6967, Train: 0.4977, Test: 0.5842\n",
      "Epoch: 354, Loss: 0.6967, Train: 0.4977, Test: 0.5842\n",
      "Epoch: 355, Loss: 0.6967, Train: 0.4976, Test: 0.5842\n",
      "Epoch: 356, Loss: 0.6967, Train: 0.4975, Test: 0.5843\n",
      "Epoch: 357, Loss: 0.6967, Train: 0.4975, Test: 0.5843\n",
      "Epoch: 358, Loss: 0.6967, Train: 0.4974, Test: 0.5843\n",
      "Epoch: 359, Loss: 0.6966, Train: 0.4973, Test: 0.5843\n",
      "Epoch: 360, Loss: 0.6966, Train: 0.4973, Test: 0.5843\n",
      "Epoch: 361, Loss: 0.6966, Train: 0.4972, Test: 0.5844\n",
      "Epoch: 362, Loss: 0.6966, Train: 0.4972, Test: 0.5844\n",
      "Epoch: 363, Loss: 0.6966, Train: 0.4971, Test: 0.5844\n",
      "Epoch: 364, Loss: 0.6966, Train: 0.4970, Test: 0.5844\n",
      "Epoch: 365, Loss: 0.6966, Train: 0.4970, Test: 0.5844\n",
      "Epoch: 366, Loss: 0.6966, Train: 0.4969, Test: 0.5845\n",
      "Epoch: 367, Loss: 0.6966, Train: 0.4968, Test: 0.5845\n",
      "Epoch: 368, Loss: 0.6966, Train: 0.4968, Test: 0.5845\n",
      "Epoch: 369, Loss: 0.6966, Train: 0.4967, Test: 0.5845\n",
      "Epoch: 370, Loss: 0.6965, Train: 0.4967, Test: 0.5845\n",
      "Epoch: 371, Loss: 0.6965, Train: 0.4966, Test: 0.5845\n",
      "Epoch: 372, Loss: 0.6965, Train: 0.4965, Test: 0.5846\n",
      "Epoch: 373, Loss: 0.6965, Train: 0.4965, Test: 0.5846\n",
      "Epoch: 374, Loss: 0.6965, Train: 0.4964, Test: 0.5846\n",
      "Epoch: 375, Loss: 0.6965, Train: 0.4964, Test: 0.5846\n",
      "Epoch: 376, Loss: 0.6965, Train: 0.4963, Test: 0.5846\n",
      "Epoch: 377, Loss: 0.6965, Train: 0.4962, Test: 0.5847\n",
      "Epoch: 378, Loss: 0.6965, Train: 0.4962, Test: 0.5847\n",
      "Epoch: 379, Loss: 0.6965, Train: 0.4961, Test: 0.5847\n",
      "Epoch: 380, Loss: 0.6965, Train: 0.4960, Test: 0.5847\n",
      "Epoch: 381, Loss: 0.6964, Train: 0.4960, Test: 0.5847\n",
      "Epoch: 382, Loss: 0.6964, Train: 0.4959, Test: 0.5848\n",
      "Epoch: 383, Loss: 0.6964, Train: 0.4959, Test: 0.5848\n",
      "Epoch: 384, Loss: 0.6964, Train: 0.4958, Test: 0.5848\n",
      "Epoch: 385, Loss: 0.6964, Train: 0.4957, Test: 0.5848\n",
      "Epoch: 386, Loss: 0.6964, Train: 0.4957, Test: 0.5848\n",
      "Epoch: 387, Loss: 0.6964, Train: 0.4956, Test: 0.5849\n",
      "Epoch: 388, Loss: 0.6964, Train: 0.4955, Test: 0.5849\n",
      "Epoch: 389, Loss: 0.6964, Train: 0.4955, Test: 0.5849\n",
      "Epoch: 390, Loss: 0.6964, Train: 0.4954, Test: 0.5849\n",
      "Epoch: 391, Loss: 0.6964, Train: 0.4954, Test: 0.5849\n",
      "Epoch: 392, Loss: 0.6964, Train: 0.4953, Test: 0.5849\n",
      "Epoch: 393, Loss: 0.6964, Train: 0.4952, Test: 0.5850\n",
      "Epoch: 394, Loss: 0.6964, Train: 0.4952, Test: 0.5850\n",
      "Epoch: 395, Loss: 0.6963, Train: 0.4951, Test: 0.5850\n",
      "Epoch: 396, Loss: 0.6963, Train: 0.4951, Test: 0.5850\n",
      "Epoch: 397, Loss: 0.6963, Train: 0.4950, Test: 0.5850\n",
      "Epoch: 398, Loss: 0.6963, Train: 0.4950, Test: 0.5850\n",
      "Epoch: 399, Loss: 0.6963, Train: 0.4949, Test: 0.5851\n",
      "Epoch: 400, Loss: 0.6963, Train: 0.4948, Test: 0.5851\n",
      "Epoch: 401, Loss: 0.6963, Train: 0.4948, Test: 0.5851\n",
      "Epoch: 402, Loss: 0.6963, Train: 0.4947, Test: 0.5851\n",
      "Epoch: 403, Loss: 0.6963, Train: 0.4947, Test: 0.5851\n",
      "Epoch: 404, Loss: 0.6963, Train: 0.4946, Test: 0.5851\n",
      "Epoch: 405, Loss: 0.6963, Train: 0.4946, Test: 0.5852\n",
      "Epoch: 406, Loss: 0.6963, Train: 0.4945, Test: 0.5852\n",
      "Epoch: 407, Loss: 0.6962, Train: 0.4945, Test: 0.5852\n",
      "Epoch: 408, Loss: 0.6962, Train: 0.4944, Test: 0.5852\n",
      "Epoch: 409, Loss: 0.6962, Train: 0.4943, Test: 0.5852\n",
      "Epoch: 410, Loss: 0.6962, Train: 0.4943, Test: 0.5852\n",
      "Epoch: 411, Loss: 0.6962, Train: 0.4942, Test: 0.5853\n",
      "Epoch: 412, Loss: 0.6962, Train: 0.4942, Test: 0.5853\n",
      "Epoch: 413, Loss: 0.6962, Train: 0.4941, Test: 0.5853\n",
      "Epoch: 414, Loss: 0.6962, Train: 0.4941, Test: 0.5853\n",
      "Epoch: 415, Loss: 0.6962, Train: 0.4940, Test: 0.5853\n",
      "Epoch: 416, Loss: 0.6962, Train: 0.4940, Test: 0.5853\n",
      "Epoch: 417, Loss: 0.6962, Train: 0.4939, Test: 0.5853\n",
      "Epoch: 418, Loss: 0.6962, Train: 0.4939, Test: 0.5854\n",
      "Epoch: 419, Loss: 0.6961, Train: 0.4938, Test: 0.5854\n",
      "Epoch: 420, Loss: 0.6962, Train: 0.4938, Test: 0.5854\n",
      "Epoch: 421, Loss: 0.6961, Train: 0.4937, Test: 0.5854\n",
      "Epoch: 422, Loss: 0.6961, Train: 0.4937, Test: 0.5854\n",
      "Epoch: 423, Loss: 0.6961, Train: 0.4936, Test: 0.5854\n",
      "Epoch: 424, Loss: 0.6961, Train: 0.4936, Test: 0.5855\n",
      "Epoch: 425, Loss: 0.6961, Train: 0.4935, Test: 0.5855\n",
      "Epoch: 426, Loss: 0.6961, Train: 0.4934, Test: 0.5855\n",
      "Epoch: 427, Loss: 0.6961, Train: 0.4934, Test: 0.5855\n",
      "Epoch: 428, Loss: 0.6961, Train: 0.4933, Test: 0.5855\n",
      "Epoch: 429, Loss: 0.6961, Train: 0.4933, Test: 0.5855\n",
      "Epoch: 430, Loss: 0.6961, Train: 0.4932, Test: 0.5855\n",
      "Epoch: 431, Loss: 0.6961, Train: 0.4932, Test: 0.5856\n",
      "Epoch: 432, Loss: 0.6961, Train: 0.4931, Test: 0.5856\n",
      "Epoch: 433, Loss: 0.6960, Train: 0.4931, Test: 0.5856\n",
      "Epoch: 434, Loss: 0.6960, Train: 0.4930, Test: 0.5856\n",
      "Epoch: 435, Loss: 0.6960, Train: 0.4930, Test: 0.5856\n",
      "Epoch: 436, Loss: 0.6960, Train: 0.4929, Test: 0.5856\n",
      "Epoch: 437, Loss: 0.6960, Train: 0.4929, Test: 0.5856\n",
      "Epoch: 438, Loss: 0.6960, Train: 0.4928, Test: 0.5857\n",
      "Epoch: 439, Loss: 0.6960, Train: 0.4928, Test: 0.5857\n",
      "Epoch: 440, Loss: 0.6960, Train: 0.4927, Test: 0.5857\n",
      "Epoch: 441, Loss: 0.6960, Train: 0.4927, Test: 0.5857\n",
      "Epoch: 442, Loss: 0.6960, Train: 0.4927, Test: 0.5857\n",
      "Epoch: 443, Loss: 0.6960, Train: 0.4926, Test: 0.5857\n",
      "Epoch: 444, Loss: 0.6960, Train: 0.4926, Test: 0.5857\n",
      "Epoch: 445, Loss: 0.6960, Train: 0.4925, Test: 0.5857\n",
      "Epoch: 446, Loss: 0.6960, Train: 0.4925, Test: 0.5858\n",
      "Epoch: 447, Loss: 0.6960, Train: 0.4924, Test: 0.5858\n",
      "Epoch: 448, Loss: 0.6959, Train: 0.4924, Test: 0.5858\n",
      "Epoch: 449, Loss: 0.6959, Train: 0.4923, Test: 0.5858\n",
      "Epoch: 450, Loss: 0.6959, Train: 0.4923, Test: 0.5858\n",
      "Epoch: 451, Loss: 0.6959, Train: 0.4922, Test: 0.5858\n",
      "Epoch: 452, Loss: 0.6959, Train: 0.4922, Test: 0.5858\n",
      "Epoch: 453, Loss: 0.6959, Train: 0.4921, Test: 0.5858\n",
      "Epoch: 454, Loss: 0.6959, Train: 0.4921, Test: 0.5859\n",
      "Epoch: 455, Loss: 0.6959, Train: 0.4920, Test: 0.5859\n",
      "Epoch: 456, Loss: 0.6959, Train: 0.4920, Test: 0.5859\n",
      "Epoch: 457, Loss: 0.6959, Train: 0.4920, Test: 0.5859\n",
      "Epoch: 458, Loss: 0.6959, Train: 0.4919, Test: 0.5859\n",
      "Epoch: 459, Loss: 0.6959, Train: 0.4919, Test: 0.5859\n",
      "Epoch: 460, Loss: 0.6959, Train: 0.4918, Test: 0.5859\n",
      "Epoch: 461, Loss: 0.6959, Train: 0.4918, Test: 0.5859\n",
      "Epoch: 462, Loss: 0.6959, Train: 0.4917, Test: 0.5860\n",
      "Epoch: 463, Loss: 0.6959, Train: 0.4917, Test: 0.5860\n",
      "Epoch: 464, Loss: 0.6959, Train: 0.4916, Test: 0.5860\n",
      "Epoch: 465, Loss: 0.6958, Train: 0.4916, Test: 0.5860\n",
      "Epoch: 466, Loss: 0.6958, Train: 0.4915, Test: 0.5860\n",
      "Epoch: 467, Loss: 0.6958, Train: 0.4915, Test: 0.5860\n",
      "Epoch: 468, Loss: 0.6958, Train: 0.4914, Test: 0.5860\n",
      "Epoch: 469, Loss: 0.6958, Train: 0.4914, Test: 0.5860\n",
      "Epoch: 470, Loss: 0.6958, Train: 0.4914, Test: 0.5861\n",
      "Epoch: 471, Loss: 0.6958, Train: 0.4913, Test: 0.5861\n",
      "Epoch: 472, Loss: 0.6958, Train: 0.4913, Test: 0.5861\n",
      "Epoch: 473, Loss: 0.6958, Train: 0.4912, Test: 0.5861\n",
      "Epoch: 474, Loss: 0.6958, Train: 0.4912, Test: 0.5861\n",
      "Epoch: 475, Loss: 0.6958, Train: 0.4911, Test: 0.5861\n",
      "Epoch: 476, Loss: 0.6958, Train: 0.4911, Test: 0.5861\n",
      "Epoch: 477, Loss: 0.6958, Train: 0.4911, Test: 0.5861\n",
      "Epoch: 478, Loss: 0.6958, Train: 0.4910, Test: 0.5861\n",
      "Epoch: 479, Loss: 0.6958, Train: 0.4910, Test: 0.5861\n",
      "Epoch: 480, Loss: 0.6958, Train: 0.4909, Test: 0.5862\n",
      "Epoch: 481, Loss: 0.6957, Train: 0.4909, Test: 0.5862\n",
      "Epoch: 482, Loss: 0.6957, Train: 0.4909, Test: 0.5862\n",
      "Epoch: 483, Loss: 0.6957, Train: 0.4908, Test: 0.5862\n",
      "Epoch: 484, Loss: 0.6957, Train: 0.4908, Test: 0.5862\n",
      "Epoch: 485, Loss: 0.6957, Train: 0.4907, Test: 0.5862\n",
      "Epoch: 486, Loss: 0.6957, Train: 0.4907, Test: 0.5862\n",
      "Epoch: 487, Loss: 0.6957, Train: 0.4907, Test: 0.5862\n",
      "Epoch: 488, Loss: 0.6957, Train: 0.4906, Test: 0.5862\n",
      "Epoch: 489, Loss: 0.6957, Train: 0.4906, Test: 0.5862\n",
      "Epoch: 490, Loss: 0.6957, Train: 0.4905, Test: 0.5863\n",
      "Epoch: 491, Loss: 0.6957, Train: 0.4905, Test: 0.5863\n",
      "Epoch: 492, Loss: 0.6957, Train: 0.4904, Test: 0.5863\n",
      "Epoch: 493, Loss: 0.6957, Train: 0.4904, Test: 0.5863\n",
      "Epoch: 494, Loss: 0.6957, Train: 0.4904, Test: 0.5863\n",
      "Epoch: 495, Loss: 0.6957, Train: 0.4903, Test: 0.5863\n",
      "Epoch: 496, Loss: 0.6957, Train: 0.4903, Test: 0.5863\n",
      "Epoch: 497, Loss: 0.6957, Train: 0.4902, Test: 0.5863\n",
      "Epoch: 498, Loss: 0.6957, Train: 0.4902, Test: 0.5863\n",
      "Epoch: 499, Loss: 0.6956, Train: 0.4902, Test: 0.5863\n",
      "Epoch: 500, Loss: 0.6956, Train: 0.4901, Test: 0.5863\n",
      "Best Test: 0.5984\n"
     ]
    }
   ],
   "source": [
    "print(train_data.x.tolist()[:20])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data = train_data.to(device)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # heads = 8\n",
    "        # droupout?, concat?\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "num_features = 2\n",
    "hidden_features = 8\n",
    "model = Net(num_features, hidden_features, num_features).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=.005)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "losses = []\n",
    "best_test_auc = 0\n",
    "for epoch in range(1, 501):\n",
    "    loss = train()\n",
    "    #val_auc = test(val_data)\n",
    "    train_auc = test(train_data)\n",
    "    test_auc = test(test_data)\n",
    "    if test_auc > best_test_auc:\n",
    "        best_test_auc = test_auc\n",
    "    # print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "    #       f'Test: {test_auc:.4f}')\n",
    "    losses.append(loss)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_auc:.4f}, Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Best Test: {best_test_auc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64584fae-de30-418f-a7f8-f9b637fd724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10000000149011612, 0.0], [1.0, 0.0], [0.4346197247505188, 5.0], [0.11166881769895554, 9.0], [0.0, 0.0], [0.0, 0.0], [0.2554112672805786, 7.0], [0.0, 0.0], [0.20346319675445557, 0.0], [0.4346764385700226, 0.0], [0.0, 14.0], [0.0679602101445198, 21.0], [0.20705881714820862, 0.0], [0.08176100999116898, 4.0], [1.0, 0.0], [0.4000000059604645, 0.0], [0.1428571492433548, 0.0], [0.12946158647537231, 1.0], [0.0, 0.0], [0.07621333748102188, 1.0]]\n",
      "[[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0]]\n",
      "Epoch: 001, Loss: 189.0872, Train: 0.4621, Test: 0.5116\n",
      "Epoch: 002, Loss: 119.4740, Train: 0.4539, Test: 0.5161\n",
      "Epoch: 003, Loss: 108.7607, Train: 0.4503, Test: 0.5148\n",
      "Epoch: 004, Loss: 85.1425, Train: 0.4504, Test: 0.5128\n",
      "Epoch: 005, Loss: 58.5354, Train: 0.4452, Test: 0.5228\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cc/tl7fjcg9171drhpd8n9f7y700000gn/T/ipykernel_38194/1759173089.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m#val_auc = test(val_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mtrain_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mtest_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtest_auc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_test_auc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/cc/tl7fjcg9171drhpd8n9f7y700000gn/T/ipykernel_38194/1759173089.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_label_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/cc/tl7fjcg9171drhpd8n9f7y700000gn/T/ipykernel_38194/1759173089.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_label_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch_geometric/nn/conv/gat_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# propagate_type: (x: OptPairTensor, alpha: OptPairTensor, edge_attr: OptTensor)  # noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         out = self.propagate(edge_index, x=x, alpha=alpha, edge_attr=edge_attr,\n\u001b[0m\u001b[1;32m    219\u001b[0m                              size=size)\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                         \u001b[0mmsg_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmsg_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmsg_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch_geometric/nn/conv/gat_conv.py\u001b[0m in \u001b[0;36mmessage\u001b[0;34m(self, x_j, alpha_j, alpha_i, edge_attr, index, ptr, size_i)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m  \u001b[0;31m# Save for later use.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(train_data.x.tolist()[:20])\n",
    "print(train_data.edge_attr.tolist()[:20])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data = train_data.to(device)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels,edge_dim):\n",
    "        super().__init__()\n",
    "        # heads = 8\n",
    "        # droupout?, concat?\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels,edge_dim=edge_dim)\n",
    "        self.conv2 = GATConv(hidden_channels, out_channels,edge_dim=edge_dim)\n",
    "\n",
    "    def encode(self, x, edge_index, edge_attr):\n",
    "        x = self.conv1(x, edge_index, edge_attr).relu()\n",
    "        return self.conv2(x, edge_index, edge_attr)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "num_features = 2\n",
    "edge_dim = 2\n",
    "hidden_features = 8\n",
    "model = Net(num_features, hidden_features, num_features,edge_dim).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=.01, weight_decay=.1)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index, train_data.edge_attr)\n",
    "\n",
    "    # We perform a new round of negative sampling for every training epoch:\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n",
    "        num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n",
    "\n",
    "    edge_label_index = torch.cat(\n",
    "        [train_data.edge_label_index, neg_edge_index],\n",
    "        dim=-1,\n",
    "    )\n",
    "    edge_label = torch.cat([\n",
    "        train_data.edge_label,\n",
    "        train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "    ], dim=0)\n",
    "\n",
    "    out = model.decode(z, edge_label_index).view(-1)\n",
    "    loss = criterion(out, edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(data):\n",
    "    model.eval()\n",
    "    z = model.encode(data.x, data.edge_index, data.edge_attr)\n",
    "    out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    "\n",
    "losses = []\n",
    "best_test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    #val_auc = test(val_data)\n",
    "    train_auc = test(train_data)\n",
    "    test_auc = test(test_data)\n",
    "    if test_auc > best_test_auc:\n",
    "        best_test_auc = test_auc\n",
    "    # print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "    #       f'Test: {test_auc:.4f}')\n",
    "    losses.append(loss)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_auc:.4f}, Test: {test_auc:.4f}')\n",
    "\n",
    "print(f'Best Test: {best_test_auc:.4f}')\n",
    "\n",
    "z = model.encode(test_data.x, test_data.edge_index)\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adbaa03-8a13-4409-976f-296e10cf0213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa1b10-4ce9-42b2-b2c3-d283d2798c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
